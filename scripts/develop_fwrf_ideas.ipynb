{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "import string\n",
    "from theano import tensor as tnsr\n",
    "from theano import function, shared\n",
    "from hrf_fitting.src.feature_weighted_rf_models import receptive_fields\n",
    "from hrf_fitting.src.features import make_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class definitions + Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##convenience functions for getting/setting params by name\n",
    "\n",
    "def get_model_params_names(l_model):\n",
    "    p_names = [shared_var.name for shared_var in lasagne.layers.get_all_params(l_model)]\n",
    "    return p_names\n",
    "\n",
    "def set_named_model_params(l_model,**kwargs):\n",
    "    for shared_var in lasagne.layers.get_all_params(l_model):\n",
    "        if shared_var.name in kwargs.keys():\n",
    "            shared_var.set_value(kwargs[shared_var.name])\n",
    "\n",
    "def get_named_param_shapes(l_model, *args):\n",
    "    param_shape_dict = {}\n",
    "    for shared_var in lasagne.layers.get_all_params(l_model):\n",
    "        if shared_var.name in args:\n",
    "            try:\n",
    "                param_shape_dict[shared_var.name] = shared_var.get_value().shape\n",
    "                print 'parameter %s has shape %s' %(shared_var.name, param_shape_dict[shared_var.name])\n",
    "            except AttributeError:\n",
    "                print 'no shape attribute available for %s' %(shared_var.name)\n",
    "    return param_shape_dict    \n",
    "\n",
    "\n",
    "def get_named_params(l_model, *args):\n",
    "    param_value_dict = {}\n",
    "    for shared_var in lasagne.layers.get_all_params(l_model):\n",
    "        if shared_var.name in args:\n",
    "            param_value_dict[shared_var.name] = shared_var\n",
    "    return param_value_dict\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter hid_init has shape (1, 11)\n",
      "parameter input_to_hidden.W has shape (144, 11)\n",
      "parameter input_to_hidden.b has shape (11,)\n",
      "parameter hidden_to_hidden.W has shape (11, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestGetSetShape(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        ##build some random complicated layer\n",
    "        input_shape = (14, 13, 12, 12)\n",
    "        num_units = 11\n",
    "        self.test_layer = lasagne.layers.RecurrentLayer(input_shape,num_units)\n",
    "    \n",
    "    def test_param_set_get_shape(self):\n",
    "        params = lasagne.layers.get_all_params(self.test_layer)\n",
    "        for p in params:\n",
    "            val = get_named_params(self.test_layer, p.name)[p.name]\n",
    "            self.assertEqual(val.get_value().shape, get_named_param_shapes(self.test_layer, p.name)[p.name])\n",
    "            \n",
    "    def test_param_names(self):\n",
    "        pnames = get_model_params_names(self.test_layer)\n",
    "        self.assertEqual(pnames, [p.name for p in lasagne.layers.get_all_params(self.test_layer)])\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestGetSetShape )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class receptive_field_layer(lasagne.layers.Layer):\n",
    "    '''\n",
    "    receptive_field_layer(incoming, num_voxels, (X_grid, Y_grid), deg_per_stim, x0=None, y0=None, sig=None)\n",
    "    \n",
    "    --inputs\n",
    "    incoming ~ should be an input layer with (T,D,S,S) input_var tensor4\n",
    "    \n",
    "    num_voxels  ~ V\n",
    "    \n",
    "    (X_grid, Y_grid) ~ numpy matrices of spatial 2D grid specified in pixels. output \"make_space\" or np.meshgrid\n",
    "    \n",
    "    when x0,y0,sig=None initializes each rf with (x0,y0)=(0,0), stdev = 1. otherwise, can initialize with a theano shared variable or numpy array\n",
    "    \n",
    "    creates a stack of 2D gaussian rf blobs. The stack has dimensions (V,S,S). just call \"make_rf_stack()\"\n",
    "    \n",
    "    application gives a tensor of shape (T,D,V) \n",
    "    \n",
    "    --attributes\n",
    "    params are [x0, y0, sig], which are the (x,y) coors and standard dev., resp., of the gaussian rfs\n",
    "    for some set of voxels\n",
    "    \n",
    "    make_rf_stack() will construct a stack of visualizable rf's\n",
    "    '''    \n",
    "    def __init__(self, incoming, num_voxels, space_grid, deg_per_stim, x0=None, y0=None, sig=None, dtype='float32', **kwargs):\n",
    "        super(receptive_field_layer, self).__init__(incoming, **kwargs)  ##this will give us an \"input_shape\" attribute\n",
    "        self.S = self.input_shape[-1]\n",
    "        self.V = num_voxels\n",
    "        \n",
    "        self.pix_per_deg = self.S * (1./deg_per_stim)  ##this will convert into pixels\n",
    "        self.Xm = shared(space_grid[0].astype(dtype))  ##explicit casting. is there a better way to do this?\n",
    "        self.Ym = shared(space_grid[1].astype(dtype))        \n",
    "        \n",
    "        if x0 is not None:\n",
    "            self.x0 = self.add_param(x0,(self.V,), name='x0',rf_param=True,x0=True)\n",
    "        else:\n",
    "            self.x0 = self.add_param(np.zeros(self.V,dtype=dtype),(self.V,), name='x0',rf_param=True,x0=True)\n",
    "        \n",
    "        if y0 is not None:\n",
    "            self.y0 = self.add_param(y0,(self.V,), name='y0',rf_param=True,y0=True)\n",
    "        else:\n",
    "            self.y0 = self.add_param(np.zeros(self.V,dtype=dtype),(self.V,), name='y0',rf_param=True,y0=True)\n",
    "        \n",
    "        if sig is not None:\n",
    "            self.sig = self.add_param(sig,(self.V,), name='sig',rf_param=True,sig=True) \n",
    "        else:\n",
    "            self.sig = self.add_param(np.ones(self.V,dtype=dtype),(self.V,), name='sig',rf_param=True,sig=True) \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        ##we put the pix_per_deg conversion here becuase x0,y0,sig will be shared across feature spaces\n",
    "        #this expression has shape (V,S,S)\n",
    "        self.gauss_expr = ((1. / 2*np.pi*(self.sig[:,np.newaxis,np.newaxis]*self.pix_per_deg)**2)\n",
    "                           *tnsr.exp(-((self.Xm[np.newaxis,:,:]-self.x0[:,np.newaxis,np.newaxis]*self.pix_per_deg)**2\n",
    "                                       + (self.Ym[np.newaxis,:,:]-self.y0[:, np.newaxis,np.newaxis]*self.pix_per_deg)**2)\n",
    "                                     /(2*(self.sig[:,np.newaxis,np.newaxis]*self.pix_per_deg)**2)))\n",
    "        \n",
    "        self.make_rf_stack = function([],self.gauss_expr)\n",
    "        \n",
    "    \n",
    "    #T,D,S,S x V,S,S --> T,D,V\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return tnsr.tensordot(input, self.gauss_expr, axes=[[2,3],[1,2]])\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape): ##(T,D,V)\n",
    "        return (input_shape[0], input_shape[1], self.V)\n",
    "    \n",
    "    def set_value_for_voxel(self, voxel_idx=None, x0=None, y0=None, sig=None):\n",
    "        if x0 is not None: ##we assume the shared variable has already been created and registered\n",
    "            x0_temp = self.x0.get_value()\n",
    "            x0_temp[voxel_idx] = x0\n",
    "            self.x0.set_value(x0_temp)\n",
    "        if y0 is not None:\n",
    "            y0_temp = self.y0.get_value()\n",
    "            y0_temp[voxel_idx] = y0\n",
    "            self.y0.set_value(y0_temp)\n",
    "        if sig is not None:\n",
    "            sig_temp = self.sig.get_value()\n",
    "            sig_temp[voxel_idx] = sig\n",
    "            self.sig.set_value(sig_temp)\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 1.172s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=6 errors=0 failures=0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestRfLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.T = 900\n",
    "        self.D = 90\n",
    "        self.S = 9\n",
    "        self.input_shape = (self.T,self.D,self.S,self.S)\n",
    "        self.V = 10\n",
    "        self.deg_per_stim = 20\n",
    "        #this tensor stores feature map\n",
    "        self.f_map_0 = tnsr.tensor4('f_map_0',dtype='float32')\n",
    "        ##construct an input layer\n",
    "        self.input_layer = lasagne.layers.InputLayer(self.input_shape, input_var = self.f_map_0, name='input_layer')\n",
    "        ##create a real numpy input of all 1's\n",
    "        self.input_data = np.ones(self.input_shape,dtype='float32')\n",
    "        \n",
    "        try:\n",
    "            self.rf_layer = receptive_field_layer(self.input_layer, self.V, make_space(self.S), self.deg_per_stim, name='rf_layer')\n",
    "        except:\n",
    "            print 'could not init rf layer'\n",
    "    \n",
    "    ##rf stack should be V,S,S\n",
    "    def test_rf_layer_make_rf_stack(self):          \n",
    "        self.assertEqual(self.rf_layer.make_rf_stack().shape, (self.V,self.S,self.S))\n",
    "    \n",
    "    ##output should be T,D,V\n",
    "    def test_rf_layer_output_shape(self):\n",
    "        output_shape = self.rf_layer.get_output_shape_for(self.input_shape)\n",
    "        self.assertEqual(output_shape, (self.T,self.D,self.V))\n",
    "    \n",
    "    ##grid where rfs are evaluated\n",
    "    def test_rf_layer_spatial_grid(self):\n",
    "        self.assertEqual(self.rf_layer.Xm.get_value().shape, (self.S,self.S))\n",
    "        self.assertEqual(self.rf_layer.Ym.get_value().shape, (self.S,self.S))\n",
    "    \n",
    "    ##should all be V\n",
    "    def test_rf_layer_param_shapes(self):\n",
    "        self.assertEqual(self.rf_layer.x0.get_value().shape, (self.V,))\n",
    "        self.assertEqual(self.rf_layer.y0.get_value().shape, (self.V,))\n",
    "        self.assertEqual(self.rf_layer.sig.get_value().shape, (self.V,))\n",
    "    \n",
    "    ##should be T,D,V\n",
    "    def test_rf_layer_output_function_shape(self):\n",
    "        rf_layer_expr = lasagne.layers.get_output(self.rf_layer)\n",
    "        rf_layer_func = function([self.input_layer.input_var], rf_layer_expr)\n",
    "        self.assertEqual(rf_layer_func(self.input_data).shape,(self.T,self.D,self.V))\n",
    "        \n",
    "    ##should go from 0 to 10\n",
    "    def test_set_value_for_voxel(self):\n",
    "        self.rf_layer.set_value_for_voxel(voxel_idx=-1,x0=10,y0=10,sig=10)\n",
    "        self.assertEqual(self.rf_layer.x0.get_value()[-1],10)  ## this is same as next line\n",
    "        self.assertEqual(get_named_params(self.rf_layer,self.rf_layer.x0.name)[self.rf_layer.x0.name].get_value()[-1],10)\n",
    "        self.assertEqual(self.rf_layer.y0.get_value()[-1],10)\n",
    "        self.assertEqual(self.rf_layer.sig.get_value()[-1],10)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestRfLayer )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class compressive_nonlinearity_layer(lasagne.layers.Layer):\n",
    "    '''\n",
    "    a compressive function used in a previous publication. works well.\n",
    "    requires all inputs to be positive, BUT DOES NOT CHECK!\n",
    "    computes elementwise log(1+sqrt(input))\n",
    "    '''\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return tnsr.log(1+tnsr.sqrt(input))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestCompressiveNonlinearityLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.T = 1001\n",
    "        self.D = 101\n",
    "        self.V = 11\n",
    "        self.input_shape = (self.T,self.D,self.V)\n",
    "        self.test_layer = compressive_nonlinearity_layer(self.input_shape)\n",
    "    def test_output_shape(self):\n",
    "        self.assertEqual(self.input_shape, self.test_layer.get_output_shape_for(self.input_shape))\n",
    " \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestCompressiveNonlinearityLayer )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class normalization_layer(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, mean=lasagne.init.Constant([0]), stdev=lasagne.init.Constant([1]), mask = lasagne.init.Constant([1]), **kwargs):\n",
    "        super(normalization_layer,self).__init__(incoming, **kwargs)\n",
    "        self.mean = self.add_param(mean, self.input_shape[1:], name='mean', trainable=False)\n",
    "        self.stdev = self.add_param(stdev, self.input_shape[1:], name='stdev', trainable=False)\n",
    "        self.stability_mask = self.add_param(mask, self.input_shape[1:], name='stability_mask', trainable=False)\n",
    "    \n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return tnsr.switch(self.stability_mask>0, (input - self.mean[np.newaxis,:,:])/self.stdev[np.newaxis,:,:], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.057s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestNormalizationLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.T = 1001\n",
    "        self.D = 4\n",
    "        self.V = 2\n",
    "        self.x = tnsr.tensor3('x')\n",
    "        self.input_shape = (self.T,self.D,self.V)\n",
    "        self.input_layer = lasagne.layers.InputLayer(self.input_shape, input_var=self.x, name='input_layer')\n",
    "        self.test_input = np.random.random(size=self.input_shape).astype('float32')\n",
    "        self.test_input[:,0,0] = 1.\n",
    "        self.mean = np.mean(self.test_input, axis=0)\n",
    "        self.std = np.std(self.test_input, axis=0)\n",
    "        self.mask = np.ones((self.D,self.V))\n",
    "        self.mask[0,0] = 0\n",
    "        self.test_layer = normalization_layer(self.input_layer, mean=self.mean, stdev = self.std, mask = self.mask)\n",
    "        \n",
    "    def test_output_shape(self):\n",
    "        self.assertEqual(self.input_shape, self.test_layer.get_output_shape_for(self.input_shape))\n",
    "    \n",
    "    def test_out_values(self):\n",
    "        output_func = function([self.x], [lasagne.layers.get_output(self.test_layer)])\n",
    "        y = output_func(self.test_input)[0]\n",
    "        ##shape is right\n",
    "        self.assertEqual(self.input_shape, y.shape)\n",
    "        ##degenerate entry set to 0\n",
    "        self.assertTrue(np.all(y[:,0,0]==0))\n",
    "        ##mean is 0\n",
    "        np.testing.assert_almost_equal(np.mean(y,axis=0), 0, decimal=5)\n",
    "        ##stdev is 1 for all but degnerate entry\n",
    "        np.testing.assert_almost_equal(np.std(y,axis=0).ravel()[1:], 1, decimal=6)\n",
    "\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestNormalizationLayer )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class feature_weights_layer(lasagne.layers.Layer):\n",
    "    '''\n",
    "    feature_weights_layer(incoming, NU = lasagne.init.Constant([0]))\n",
    "       incoming ~ should be an rf space tensor (T,D,V)\n",
    "             NU ~ (D, V) matrix of feature weights.\n",
    "    output      ~ (T, V) matrix of predicted responses.\n",
    "    '''    \n",
    "    def __init__(self, incoming, NU = lasagne.init.Constant([0]), **kwargs):\n",
    "        ##this will give us an \"input_shape\" attribute\n",
    "        super(feature_weights_layer, self).__init__(incoming, **kwargs)  \n",
    "        self.D = self.input_shape[1]\n",
    "        self.V = self.input_shape[-1]\n",
    "        self.feature_dim = 0\n",
    "        self.voxel_dim = 1\n",
    "        ##creates the theano shared variable\n",
    "        self.NU = self.add_param(NU, (self.D, self.V), name='feature_weights', feature_weights=True) \n",
    "    \n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return (input*self.NU[np.newaxis,:,:]).sum(axis=1)\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape): ##input_shape = (T, D, V)\n",
    "        return (input_shape[0], self.V)\n",
    "    \n",
    "    ##if NU = None, this will do nothing and return None\n",
    "    def set_weight_for_voxel(self, voxel_idx=None, NU=None):\n",
    "        if NU is not None:\n",
    "            NU_temp = self.NU.get_value()\n",
    "            NU_temp[:,voxel_idx] = NU\n",
    "            self.NU.set_value(NU_temp)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.066s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestFeatureWeightsLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        T,D,V = 1001,4,20\n",
    "        self.input_shape = (T,D,V)\n",
    "        self.x = tnsr.tensor3('x')\n",
    "        self.input_layer = lasagne.layers.InputLayer(self.input_shape, input_var=self.x, name='input_layer')\n",
    "        self.test_input = np.random.random(size=self.input_shape).astype('float32')\n",
    "        self.test_layer = feature_weights_layer(self.input_layer)\n",
    "        self.layer_func = function([self.x], lasagne.layers.get_output(self.test_layer))\n",
    "        self.y = self.layer_func(self.test_input)\n",
    "    \n",
    "    def test_output(self):\n",
    "        self.assertEqual((self.input_shape[0], self.input_shape[-1]), self.test_layer.get_output_shape_for(self.input_shape))\n",
    "        self.assertEqual((self.input_shape[0], self.input_shape[-1]), self.y.shape)\n",
    "        \n",
    "    def test_set_weight_for_voxel(self):\n",
    "        self.assertIsNone(self.test_layer.set_weight_for_voxel(NU = None))\n",
    "        voxel_idx = [0,1,-2,-1]\n",
    "        NU = [1,1] ##this raise value error because of mismatch\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.test_layer.set_weight_for_voxel(voxel_idx = voxel_idx, NU = NU)     \n",
    "        NU = np.random.random((self.input_shape[1], len(voxel_idx))).astype('float32')\n",
    "        self.test_layer.set_weight_for_voxel(voxel_idx = voxel_idx, NU=NU)\n",
    "        np.testing.assert_array_equal(self.test_layer.NU.get_value()[:,voxel_idx], NU)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestFeatureWeightsLayer )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((10,4,2))\n",
    "print x[0]\n",
    "x[0] = np.ones((4,2))\n",
    "print x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class lasagne_model_learner(object):\n",
    "    \n",
    "    def __init__(self,l_model, model_input_tnsr_dict, trn_data_generator, val_data_generator, epochs = 1, check_every=10, num_iters=10, learning_rate = 1.0,learn_these_params = None, voxel_dims = None, print_stuff=False):\n",
    "\n",
    "        '''\n",
    "        lasagne_model_learner(l_model,\n",
    "                            model_input_tnsr_dict,\n",
    "                            trn_data_generator,\n",
    "                            val_data_generator,\n",
    "                            epochs=1,\n",
    "                            check_every=10,\n",
    "                            num_iters=10,\n",
    "                            learning_rate = 1.0,\n",
    "                            learn_these_params = None,\n",
    "                            voxel_dims = None)\n",
    "\n",
    "        a class for doing cross-validated (stochastic) gradient descent on params of a lasagne model with many independent\n",
    "        chanels (referred to as \"voxels\"). After each gradient step, checks each model independently, and updates it\n",
    "        if gradient step reduces error on validation set.\n",
    "\n",
    "        inputs:\n",
    "                            l_model ~ a lasagne model with output shape (T,V), where T = #trials, V=#voxels\n",
    "            model_input_tensor_dict ~ dict of theano tensors that are input to the l_model.\n",
    "                 trn_data_generator ~ a generator of (input, output) training data batches. this is function that you have write.\n",
    "                                      it should yield batches of data, i.e., trn_data_generator() will give you your training batches.\n",
    "                                      the format for each batch is (input, output), where\n",
    "                                      input = training input data dictionary. each key/value matches the names/dimensions of corresponding tensor in model_input_tensor_dict\n",
    "                                      output = np.array of shape (Ttrn, V)\n",
    "                 val_data_generator ~ generator for validation data\n",
    "                             epochs ~ number of times through all data in the generator\n",
    "                        check_every ~ int. how often validation loss is checked. default = 10\n",
    "                          num_iters ~ number of gradient steps per batch before stopping. default = 100\n",
    "                      learning_rate ~ size of gradient step. default = 1\n",
    "                 learn_these_params ~ list of parameters names to learn, in case you don't want to train them all.\n",
    "                                      default = None, meaning train all trainable params\n",
    "                        voxel_dims  ~ dictionary, keys=param names, values = ints.\n",
    "                                      for each learned parameter, this corresponds to the voxel dimension.\n",
    "                                      default = None, in which case we try to figure out what the dimension is.\n",
    "                                      if we can't, we complain, and you are forced to specify.\n",
    "        outputs:\n",
    "                     l_model ~ original l_model with params all trained up. this is a convenience, as params are learned in-place\n",
    "                 trn_history ~ array of length num_iters showing number of voxels with decreased training loss at each time step\n",
    "                 val_history ~ array of length num_iters showing number of voxels with decreased validation loss at each time step\n",
    "\n",
    "        notes:\n",
    "            we call each independent model a \"voxel\". but it could be anything. \n",
    "\n",
    "            the data_generator approach is very general/flexible. in the case where you can simply load up the whole training/validation\n",
    "            data sets at once, it is just a slight encumberance, forcing you to define trn_data_generator() = yield trn_data.\n",
    "            in many cases though, it will not be possible to load all the data at once, so this approach will be helpful.\n",
    "\n",
    "            if trn_data_generator() contains only one large batch, epochs=1, and num_iters = BIG, we get standard grad. descent.\n",
    "            if trn_data_generator() contains many mini-batches, epochs > 1, num_iters = 1, we get *stochastic* grad. descent.\n",
    "        '''\n",
    "        \n",
    "        ##record all the inputs\n",
    "        self.l_model = l_model\n",
    "        \n",
    "        self.model_input_tnsr_dict = model_input_tnsr_dict\n",
    "        self.trn_data_generator = trn_data_generator\n",
    "        self.val_data_generator = val_data_generator\n",
    "        self.epochs = epochs\n",
    "        self.check_every=check_every\n",
    "        self.num_iters = num_iters\n",
    "        self.learn_these_params = learn_these_params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.print_stuff = print_stuff\n",
    "        \n",
    "        ##record/check data dimensions\n",
    "        ##read first batches to get some dimensions \n",
    "        trn_in, trn_out = next(trn_data_generator()) \n",
    "        val_in, val_out = next(val_data_generator())\n",
    "        trn_batch_size, num_trn_voxels = trn_out.shape  \n",
    "        val_batch_size, num_val_voxels = val_out.shape    \n",
    "        assert num_trn_voxels == num_val_voxels, \"number of trn/val voxels don't match\"\n",
    "        self.num_voxels = num_trn_voxels\n",
    "\n",
    "        ##check to make sure that all feature map stacks in the input dictionary have the same number of timepoints.\n",
    "        ##this is not a complete check, since it's just the first batch, but if shit is not fucked up for this batch, \n",
    "        ##shit will hopefully not be fucked up for subsequent batches\n",
    "        for k,v in trn_in.iteritems():\n",
    "            assert v.shape[0] == trn_batch_size, \"number of input/output trn trials don't match for feature %s\" %(k)\n",
    "\n",
    "        for k,v in val_in.iteritems():\n",
    "            assert v.shape[0] == val_batch_size, \"number of input/output val trials don't match for feature %s\" %(k)\n",
    "\n",
    "\n",
    "        ##get params (theano shared variables) to learn\n",
    "        if learn_these_params is not None:            \n",
    "            self.params = [v for v in get_named_params(l_model, *learn_these_params).values()] ##<<unpack the dict.\n",
    "        else:\n",
    "            self.params = lasagne.layers.get_all_params(l_model,trainable=True)\n",
    "        print 'will solve for: %s' %(self.params)\n",
    "\n",
    "\n",
    "        ##try to determine which dimension of each parameter is the voxel dimension\n",
    "        if voxel_dims is None:\n",
    "            self.voxel_dims = {}\n",
    "            for p in self.params:\n",
    "                ##the voxel dimension should be the one that matches \"num_voxels\"\n",
    "                vdim = [ii for ii,pdim in enumerate(p.shape.eval()) if pdim==self.num_voxels]\n",
    " \n",
    "                ##if we happen to have multiple dimensions that = \"num_voxels\", we need to use to disambiguate for us\n",
    "                assert len(vdim)==1, \"sorry, we can't determine voxel dimension for param %s. please supply explicit 'voxel_dims' argument\" %p.name\n",
    "                self.voxel_dims[p.name] = vdim[0]\n",
    "        else:\n",
    "            self.voxel_dims = voxel_dims\n",
    "\n",
    "\n",
    "        ##create symbolic voxel activity tensor\n",
    "        voxel_data_tnsr = tnsr.matrix('voxel_data_tnsr')  ##voxel data tensor: (T x V)\n",
    "\n",
    "        ##get symbolic prediction expression\n",
    "        pred_expr = lasagne.layers.get_output(l_model)  ##voxel prediction tensor: (T x V)\n",
    "\n",
    "        ##generate symbolic loss expression\n",
    "        trn_diff = voxel_data_tnsr-pred_expr  ##difference tensor: (T x V)\n",
    "        loss_expr = (trn_diff*trn_diff).sum(axis=0) ##sum-sqaured-diffs tensor (V)\n",
    "\n",
    "        ##construct update rule: note we sum the loss expression. \n",
    "        fwrf_update = lasagne.updates.sgd(loss_expr.sum(),self.params,learning_rate=learning_rate)\n",
    "\n",
    "        ##compile loss and training functions\n",
    "        print 'compiling...'\n",
    "        self.loss = function([voxel_data_tnsr]+model_input_tnsr_dict.values(), loss_expr)\n",
    "        self.trn_kernel = function([voxel_data_tnsr]+model_input_tnsr_dict.values(), loss_expr, updates=fwrf_update)        \n",
    "\n",
    "    def learn(self):\n",
    "        ##initialize parameter/loss\n",
    "        best_param_values = [np.copy(p.get_value()) for p in self.params]\n",
    "        val_loss_was = np.inf*np.ones(self.num_voxels)  ##start with infinite loss\n",
    "        trn_loss_was = np.inf*np.ones(self.num_voxels)  ##start with infinite loss\n",
    "        val_history = []\n",
    "        trn_history = []\n",
    "        \n",
    "        ##descend and validate\n",
    "        epoch_count = 0\n",
    "        while epoch_count < self.epochs:\n",
    "            print '=======epoch: %d' %(epoch_count) \n",
    "            for trn_in, trn_out in self.trn_data_generator():\n",
    "                step_count = 0\n",
    "                while step_count < self.num_iters:\n",
    "                    trn_loss_is = self.trn_kernel(trn_out, *trn_in.values()) \n",
    "                    \n",
    "                    if step_count % self.check_every == 0:\n",
    "\n",
    "                        ##check for improvements\n",
    "                        val_loss_is = 0\n",
    "                        for val_in, val_out in self.val_data_generator():\n",
    "                            val_loss_is += self.loss(val_out, *val_in.values())\n",
    "                        improved = np.where(val_loss_is < val_loss_was)[0]\n",
    "\n",
    "                        ##update val loss\n",
    "                        val_loss_was[improved] = val_loss_is[improved]\n",
    "\n",
    "                        ##replace old params with better params\n",
    "                        for ii,p in enumerate(self.params):\n",
    "                            vdim = self.voxel_dims[p.name]   ##the voxel dimension\n",
    "                            s = [slice(None),]*p.ndim   ##create a slicing object with right number of dims\n",
    "                            s[vdim] = improved          ##assign improved voxel indices to the correct dim of the slice object\n",
    "                            best_param_values[ii][s] = np.copy(p.get_value()[s])   ##keep a record of the best params.\n",
    "\n",
    "                        ##report on loss history\n",
    "                        trn_improved = trn_loss_is < trn_loss_was\n",
    "                        val_history.append(len(improved))\n",
    "                        trn_history.append(np.sum(trn_improved))\n",
    "                        if self.print_stuff:\n",
    "                            print '====iter: %d' %(step_count)\n",
    "                            print 'improvements val: %d' %(val_history[-1])\n",
    "                            print 'improvements trn: %d' %(trn_history[-1])\n",
    "                    \n",
    "                    step_count += 1\n",
    "\n",
    "            epoch_count += 1\n",
    "\n",
    "        ##restore best values of learned params\n",
    "        set_named_model_params(self.l_model, **{k.name:v for k,v in zip(self.params, best_param_values)})\n",
    "       \n",
    "\n",
    "        return self.l_model, val_history, trn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will solve for: [W, b]\n",
      "compiling...\n",
      "will solve for: [b]\n",
      "compiling...\n",
      "will solve for: [W]\n",
      "compiling...\n",
      "will solve for: [W, b]\n",
      "compiling...\n",
      "=======epoch: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "will solve for: [W, b]\n",
      "compiling...\n",
      "will solve for: [b]\n",
      "compiling...\n",
      "will solve for: [W]\n",
      "compiling...\n",
      "will solve for: [W, b]\n",
      "compiling...\n",
      "=======epoch: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "will solve for: [W, b]\n",
      "compiling...\n",
      "will solve for: [b]\n",
      "compiling...\n",
      "will solve for: [W]\n",
      "compiling...\n",
      "will solve for: [W, b]\n",
      "compiling...\n",
      "=======epoch: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "will solve for: [W, b]\n",
      "compiling...\n",
      "will solve for: [b]\n",
      "compiling...\n",
      "will solve for: [W]\n",
      "compiling...\n",
      "will solve for: [W, b]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "compiling...\n",
      "will solve for: [W, b]\n",
      "compiling...\n",
      "will solve for: [b]\n",
      "compiling...\n",
      "will solve for: [W]\n",
      "compiling...\n",
      "will solve for: [W, b]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "compiling...\n",
      "will solve for: [W, b]\n",
      "compiling...\n",
      "will solve for: [b]\n",
      "compiling...\n",
      "will solve for: [W]\n",
      "compiling...\n",
      "will solve for: [W, b]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "compiling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 101.152s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=6 errors=0 failures=0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestLasagneModelLearner(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        Ttrn,Tval,D,S,V = 5000,200,5,2,3\n",
    "        self.V = V\n",
    "        self.Ttrn = Ttrn\n",
    "        self.Tval = Tval\n",
    "        model_input_tnsr_dict = {}\n",
    "        model_input_tnsr_dict['fmap0'] = tnsr.tensor4('fmap0')\n",
    "        input_layer = lasagne.layers.InputLayer((None,D,S,S), input_var = model_input_tnsr_dict['fmap0'])\n",
    "        \n",
    "        self.true_W = np.random.random((D*S*S,V)).astype('float32')\n",
    "        true_model = lasagne.layers.DenseLayer(input_layer,num_units = V, W=self.true_W)\n",
    "        out_func = function([model_input_tnsr_dict['fmap0']], lasagne.layers.get_output(true_model))\n",
    "        \n",
    "        ##create training/val data using true model\n",
    "        trn_in = {}\n",
    "        trn_in['fmap0'] = np.random.random((Ttrn,D,S,S)).astype('float32')\n",
    "        trn_out = out_func(trn_in['fmap0'])\n",
    "        trn_data_gen = lambda: (yield trn_in, trn_out)\n",
    "        val_in = {}\n",
    "        val_in['fmap0'] = np.random.random((Tval,D,S,S)).astype('float32')\n",
    "        val_out = out_func(val_in['fmap0'])\n",
    "        val_data_gen = lambda: (yield val_in,val_out)\n",
    "        \n",
    "        ##test basic dimensions, parameter assignment\n",
    "        self.learn_nothing = lasagne_model_learner(true_model, model_input_tnsr_dict,trn_data_gen, val_data_gen)\n",
    "        \n",
    "        ##test learning of \"b\": start at b=1, see if it will learn b = 0\n",
    "        test_b_model = lasagne.layers.DenseLayer(input_layer,num_units = V, W=self.true_W, b=np.ones((self.V,)).astype('float32'))\n",
    "        self.learn_b = lasagne_model_learner(test_b_model, model_input_tnsr_dict,trn_data_gen, val_data_gen, learn_these_params=['b'], check_every=100,learning_rate= 0.00001, num_iters = 400,print_stuff=False)\n",
    "\n",
    "        ##test learning of \"W\": start at W = some other random values\n",
    "        test_W_model = lasagne.layers.DenseLayer(input_layer,num_units = V, W=np.random.random((D*S*S,V)).astype('float32'))\n",
    "        self.learn_W = lasagne_model_learner(test_b_model, model_input_tnsr_dict,trn_data_gen, val_data_gen, learn_these_params=['W'], check_every=100,learning_rate= 10e-15, num_iters = 90000, print_stuff=False)\n",
    "        \n",
    "        ##test learning of \"W\" and \"b\"\n",
    "        test_both_model = lasagne.layers.DenseLayer(input_layer,num_units = V, W=np.random.random((D*S*S,V)).astype('float32'),b=np.ones((self.V,)).astype('float32'))\n",
    "        self.learn_both = lasagne_model_learner(test_both_model, model_input_tnsr_dict,trn_data_gen, val_data_gen, check_every=100,learning_rate= 0.00001, num_iters = 15000, print_stuff=False)\n",
    " \n",
    "    def test_num_voxels(self):\n",
    "        self.assertEqual(self.learn_nothing.num_voxels, self.V)\n",
    "    \n",
    "    def test_voxel_dims(self):\n",
    "        self.assertDictEqual(self.learn_nothing.voxel_dims, {'W': 1, 'b':0})        \n",
    "        \n",
    "    def test_loss_func(self):\n",
    "        trn_in,trn_out = self.learn_nothing.trn_data_generator().next()\n",
    "        trn_loss = self.learn_nothing.loss(trn_out,*trn_in.values())\n",
    "        self.assertEqual(trn_out.shape, (self.Ttrn, self.V))\n",
    "        self.assertEqual(trn_loss.shape, (self.V,))\n",
    "        np.testing.assert_array_equal(trn_loss, np.zeros((self.learn_nothing.num_voxels,)))\n",
    "    \n",
    "    def test_learn_b(self):\n",
    "        new_model,val_hist,trn_hist = self.learn_b.learn()\n",
    "        np.testing.assert_array_almost_equal(new_model.b.get_value(), np.zeros((self.V)), decimal=5)\n",
    "        \n",
    "    def test_learn_W(self):\n",
    "        new_model,val_hist,trn_hist = self.learn_W.learn()\n",
    "        np.testing.assert_array_almost_equal(new_model.W.get_value(), self.true_W, decimal=5)\n",
    "    \n",
    "    def test_learn_both(self):\n",
    "        new_model,val_hist,trn_hist = self.learn_both.learn()\n",
    "        np.testing.assert_array_almost_equal(new_model.b.get_value(), np.zeros((self.V)), decimal=4)\n",
    "        np.testing.assert_array_almost_equal(new_model.W.get_value(), self.true_W, decimal=4)\n",
    "        \n",
    "        \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestLasagneModelLearner )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##merges multiple feature map stacks, applies receptive field layer, and whatever else you want.\n",
    "class rf_model_space(object):\n",
    "    '''  \n",
    "       \n",
    "    rf_model_space( feature_map_dict, deg_per_stim, num_voxels, rf_init={x0:array, y0:array, sig:array})\n",
    "    \n",
    "    The foundation for constructing a fwrf model. Creates receptive fields for a set of num_voxel voxels.\n",
    "    \n",
    "    inputs:\n",
    "    feature_map_dict ~ dictionary of (T,D_i,S_i,S_i) feature maps. T can be 1. These values are not stored, they\n",
    "                       are just used get basic name/dimension information for each kind of feature map.\n",
    "        deg_per_stim ~ for the rf_layer, so that we can interpret things if deg. of visual angle\n",
    "          num_voxels ~ the number of voxels\n",
    "             rf_init ~ optional initial receptive field params for each voxels. \n",
    "                       must be a dictionary like {x0:x0_array, y0:y0_array, sig:sig_array}, where len(??_array)=num_voxels\n",
    "    \n",
    "    attributes:\n",
    "        feature_depth\n",
    "        feature_indices\n",
    "        feature_resolutions\n",
    "        D\n",
    "        deg_per_stim\n",
    "        num_voxels\n",
    "        input_var_dict\n",
    "        rf_layer ~ lasagne layer. good for accessing rf parameters\n",
    "        rf_model_space ~ lasagne layer\n",
    "        construct_model_space_tensor(feature_map_dict) ~ method\n",
    "        normalize(calibration_data_generator, epsilon=10e-6, unstable=10e-10) ~ method\n",
    "    \n",
    "    \n",
    "    merges multiple feature map stacks, applies receptive field layer, creates a dictionary of input tensors\n",
    "    corresponding to each feature map stack, and creates an \"rf_layer\" attribute for convenient access to rf\n",
    "    parameters.\n",
    "    \n",
    "    normalization: call it once using training data, it computes the -mean/stdev, adds a normalization layer,\n",
    "                   and then sets the weights.\n",
    "    \n",
    "    does not apply the feature_weights layer. too many different branches after rf application for that to be useful.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, feature_map_dict, deg_per_stim, num_voxels, rf_init=None):\n",
    "        \n",
    "        ##record and store basic properties of the feature maps\n",
    "        self.feature_depth = {}\n",
    "        self.feature_indices = {}\n",
    "        self.feature_resolutions = {}\n",
    "        idx = 0\n",
    "        for f_key in feature_map_dict.keys():\n",
    "            self.feature_depth[f_key] = feature_map_dict[f_key].shape[1]\n",
    "            self.feature_indices[f_key] = np.arange(idx,idx + self.feature_depth[f_key],step=1)\n",
    "            idx += self.feature_depth[f_key]\n",
    "            self.feature_resolutions[f_key] = feature_map_dict[f_key].shape[2]\n",
    "        \n",
    "        ##total feature depth\n",
    "        self.D = np.sum(self.feature_depth.values())\n",
    "        \n",
    "        ##rf properties\n",
    "        self.deg_per_stim = deg_per_stim\n",
    "        self.num_voxels = num_voxels\n",
    "        if rf_init is None:\n",
    "            rf_init = {}\n",
    "            rf_init['x0'] = np.zeros(num_voxels,dtype='float32')\n",
    "            rf_init['y0'] = np.zeros(num_voxels,dtype='float32')\n",
    "            rf_init['sig'] = np.ones(num_voxels,dtype='float32')\n",
    "        \n",
    "            \n",
    "    \n",
    "        ##construct model space : applies rf's to feature maps\n",
    "        '''\n",
    "        each dict value is a (T,D_i,S_i,S_i) stack of feature maps, where i indexes each stack of feature maps\n",
    "        each model space in the list will have output shape (T,D_i,V)\n",
    "        will concatenate along axis 1 (= D)\n",
    "        will share x0,y0,sig (the rf params) across feature spaces\n",
    "        '''\n",
    "        rf_space_list = []\n",
    "        self.input_var_dict = {}\n",
    "        for f_map_name in feature_map_dict.keys():\n",
    "            \n",
    "            ##get S_i x S_i\n",
    "            input_shape = (None,)+feature_map_dict[f_map_name].shape[1:]\n",
    "\n",
    "            self.input_var_dict[f_map_name] = tnsr.tensor4(f_map_name)\n",
    "\n",
    "            ##the rf_layer for the first feature map\n",
    "            l1 = lasagne.layers.InputLayer(input_shape, input_var = self.input_var_dict[f_map_name], name='input_'+f_map_name)\n",
    "            \n",
    "            ##the points in 2D space where rf will be evaluated \n",
    "            Xm,Ym = make_space(input_shape[-1])\n",
    "            \n",
    "            ##only one feature map stack or the first map stack:\n",
    "            if not rf_space_list:  \n",
    "                self.rf_layer = receptive_field_layer(l1,num_voxels,(Xm,Ym),deg_per_stim,x0=rf_init['x0'], y0=rf_init['y0'], sig=rf_init['sig'],name='rf_'+f_map_name)\n",
    "                rf_space_list.append(self.rf_layer)\n",
    "            else: ##rf centers/sizes will be shared\n",
    "                rf_space_list.append(receptive_field_layer(l1,num_voxels,(Xm,Ym),deg_per_stim,\n",
    "                                                              x0=self.rf_layer.x0,\n",
    "                                                              y0=self.rf_layer.y0,\n",
    "                                                              sig=self.rf_layer.sig,\n",
    "                                                              name='rf_'+f_map_name))\n",
    "    \n",
    "        self.rf_model_space = lasagne.layers.ConcatLayer(rf_space_list,axis=1, name='rf_model_space')\n",
    "        \n",
    "        self.construct_model_space_tensor = function(self.input_var_dict.values(), lasagne.layers.get_output(self.rf_model_space))\n",
    "    \n",
    "    def normalize(self, calibration_data_generator, epsilon=0., unstable=10e-10):\n",
    "        mst = []\n",
    "        for cal_map in calibration_data_generator():\n",
    "            mst.append(self.construct_model_space_tensor(*cal_map.values()))\n",
    "        mst = np.concatenate(mst,axis=0)\n",
    "        mn = np.mean(mst,axis=0)\n",
    "        stdev = np.std(mst, axis=0)+epsilon\n",
    "        stability_mask = (stdev > unstable).astype('float32')\n",
    "        self.rf_model_space = normalization_layer(self.rf_model_space, mean=mn, stdev=stdev, mask=stability_mask)\n",
    "        self.epsilon=epsilon\n",
    "        self.unstable=unstable\n",
    "        self.construct_model_space_tensor = function(self.input_var_dict.values(), lasagne.layers.get_output(self.rf_model_space))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 14.116s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestRfModelSpace( unittest.TestCase ):\n",
    "    \n",
    "    def setUp(self):\n",
    "        T,D,V = 20,13,3\n",
    "        self.V = V\n",
    "        self.nmaps = 10\n",
    "        self.D = D\n",
    "        self.T = T\n",
    "        self.deg_per_stim = 10\n",
    "        self.feature_map_dict = {}\n",
    "        ##create 10 feature maps where resolution happens to be name\n",
    "        for ii in range(1,self.nmaps+1):\n",
    "            input_name = 'fmap_%0.2d' % (ii)\n",
    "            self.feature_map_dict[input_name] = np.random.random((T,D,ii,ii)).astype('float32')\n",
    "        \n",
    "        self.rfms = rf_model_space(self.feature_map_dict, self.deg_per_stim, self.V)\n",
    "        \n",
    "        self.cal_data_generator = lambda: (yield self.feature_map_dict)\n",
    "    \n",
    "    def test_basic_attributes(self):\n",
    "        self.assertEqual(self.rfms.D, self.D*self.nmaps)\n",
    "    \n",
    "    def test_construct_model_space_tensor(self):\n",
    "        mst = self.rfms.construct_model_space_tensor(*self.feature_map_dict.values())\n",
    "        self.assertEqual(mst.shape, (self.T, self.rfms.D, self.V))\n",
    "        \n",
    "    def test_normalize(self):\n",
    "        self.rfms.normalize(self.cal_data_generator)\n",
    "        mst = self.rfms.construct_model_space_tensor(*self.feature_map_dict.values())\n",
    "        np.testing.assert_array_almost_equal(np.mean(mst,axis=0), np.zeros((self.rfms.D,self.V)), decimal=6)\n",
    "        np.testing.assert_array_almost_equal(np.std(mst,axis=0), np.ones((self.rfms.D,self.V)), decimal=6)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestRfModelSpace )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##a fwrf model\n",
    "class fwrf(rf_model_space):\n",
    "    \n",
    "   '''\n",
    "   fwrf(feature_map_dict, deg_per_stim, num_voxels, rf_init=None, bonus_layers=None)\n",
    "   \n",
    "   Creates a fwrf model for multiple voxels.\n",
    "   \n",
    "    inputs:\n",
    "    feature_map_dict ~ dictionary of (T,D_i,S_i,S_i) feature maps. T can be 1. These values are not stored, they\n",
    "                       are just used get basic name/dimension information for each kind of feature map.\n",
    "        deg_per_stim ~ for the rf_layer, so that we can interpret things if deg. of visual angle\n",
    "          num_voxels ~ the number of voxels\n",
    "             rf_init ~ optional initial receptive field params for each voxels. \n",
    "                       must be a dictionary like {x0:x0_array, y0:y0_array, sig:sig_array}, where len(??_array)=num_voxels\n",
    "        bonus_layers ~ a callable stack of layers that accepts an incoming layer. if supplied, the rf_model_space\n",
    "                       lasagne model is treated as incoming to the bonus layer. a feature_weights_layer is then applied.\n",
    "                       if not supplied, a feature_weights_layer is added directly to rf_model_space.\n",
    "            \n",
    "       \n",
    "   '''\n",
    "    \n",
    "    def __init__(self, feature_map_dict, deg_per_stim, num_voxels, rf_init=None, bonus_layers=None):\n",
    "        super(fwrf, self).__init__(feature_map_dict, deg_per_stim, num_voxels, rf_init)\n",
    "        if bonus_layers:\n",
    "            self.stack_feature_layers = lambda x: feature_weights_layer(bonus_layers(x))\n",
    "        else:\n",
    "            self.stack_feature_layers = lambda x: feature_weights_layer(x)\n",
    "            \n",
    "        self.fwrf = self.stack_feature_layers(self.rf_model_space)\n",
    "        \n",
    "        \n",
    "        self.pred_expr = lasagne.layers.get_output(self.fwrf)\n",
    "        \n",
    "        ##this gives predicted output as (T,V) tensor\n",
    "        self.predicted_activity = function(self.input_var_dict.values(), self.pred_expr)\n",
    "        \n",
    "    \n",
    "    trn_data_generator, \n",
    "    val_data_generator, \n",
    "    epochs = 1, \n",
    "    check_every=10, \n",
    "    num_iters=10, \n",
    "    learning_rate = 1.0,\n",
    "    learn_these_params = None, \n",
    "    voxel_dims = None, \n",
    "    print_stuff=False\n",
    "    \n",
    "    def train_me(self, trn_data_generator, val_data_generator, rf_grid=None, **kwargs):\n",
    "        if 'coarse' in kwargs.keys() && kwargs['coarse']:\n",
    "            assert rf_grid is not None, 'if you are running coarse learning, you must supply an rf_grid'         \n",
    "            \n",
    "            ##a generator of model space tensors that iterates over T\n",
    "            trn_data_gen,input_shape = self._build_rf_model_space_tensor(trn_data_generator, rf_grid)\n",
    "            \n",
    "            ##same as a above, but for training data\n",
    "            val_data_gen = self._build_rf_model_space_tensor(val_data_generator, rf_grid)\n",
    "            \n",
    "            \n",
    "            ##a new network that treats everything up to the feature weights as input\n",
    "            proxy_net, proxy_input_var_dict = self._build_proxy_network(rf_grid)\n",
    "            \n",
    "            ##learn the best rf+feature_weights\n",
    "            proxy_net,val_history,trn_history = lasagne_model_learner(proxy_net, proxy_input_var_dict, trn_data_gen, val_data_gen, **kwargs).learn()\n",
    "            params = self._get_best_proxy_net_params(proxy_net, rf_grid)\n",
    "            \n",
    "            ##assign best params to network\n",
    "            lasagne.layers.set_all_params(self.fwrf, params)\n",
    "            \n",
    "            ##free up memory allocated to proxy network?\n",
    "            \n",
    "        elif ('fine' not in kwargs.keys()) or kwargs['fine']:\n",
    "            assert rf_grid is None, \"don't pass an rf grid if you are fine-tuning\"\n",
    "            self.fwrf,val_history, trn_history = lasagne_model_learner(self.fwrf, self.input_var_dict, trn_data_generator, val_data_generator, **kwargs).learn()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _build_proxy_network(self, input_shape):\n",
    "        #check fwrf, get shape\n",
    "        #make a new network with an input layer and feature layer only\n",
    "        proxy_input_var_dict = {}\n",
    "        proxy_input_var_dict['mst'] = tnsr.tensor4('mst')\n",
    "        proxy_net = lasagne.layers.InputLayer((None,)+input_shape[1:]), input_var = proxy_input_var_dict['mst'])\n",
    "        proxy_net = feature_weights_layer(proxy_net)\n",
    "        return proxy_net, proxy_input_var_dict\n",
    "        \n",
    "    \n",
    "    def _build_rf_model_space_tensor(self, data_gen, rf_grid):\n",
    "        data_example = data_gen().next\n",
    "        proxy_rf_model_space = rf_model_space(data_example, self.deg_per_stim, self.num_voxels,rf_init=rf_grid)\n",
    "        mst = []\n",
    "        for fmap in data_gen():\n",
    "            mst.append(proxy_rf_model_space.construct_model_space_tensor(*fmap.values()))\n",
    "        mst = np.concatenate(mst,axis=0)\n",
    "        mst_shape = mst.shape\n",
    "        mst = lambda: (yield mst)\n",
    "        #options:\n",
    "        # 1) use rf_model_space layer to create a new data generator \n",
    "        # 2) loop over all items in the data_generator, make one big brick, package as degenerate generator (heh)\n",
    "        return mst, mst_shape ##a generator\n",
    "\n",
    "    \n",
    "    def _get_best_proxy_net_params(proxy_net, val_data_gen, rf_grid):\n",
    "        ##final_loss = SOME_LOSS_FUNCTION(proxy_net,val_data_gen)\n",
    "        ##min_loss_arg = arg_min(final_loss, voxel_dim)\n",
    "        ##params= {}\n",
    "        ##params[feature_weights] = proxy_net.NU.get_value()[min_loss_arg]\n",
    "        ##params[rf blah] = rf_grid[min_loss_arg] <<something like this\n",
    "        ##return params\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def normalize(self, *args, **kwargs):\n",
    "        ##call the model_space method\n",
    "        super(fwrf, self).normalize(*args, **kwargs)\n",
    "        ##then overwrite the feature layer\n",
    "        self.fwrf = self.stack_feature_layers(self.rf_model_space)\n",
    "               \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1, 2, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##TODO: INPUTS AS SHARED VARIABLES?\n",
    "def decode_feature_maps(trn_fwrf_model, val_fwrf_model, trn_activity, val_activity, init_feature_map_dict, error_diff_thresh):\n",
    "    ##build loss\n",
    "    trn_activity_tensor = blarg\n",
    "    val_activity_tensor = blarg\n",
    "    trn_diff = trn_activity_tensor-trn_fwrf_model.pred_expr  ##difference tensor: (T x V)\n",
    "    trn_loss_expr = (trn_diff*trn_diff).sum(axis=1) ##sum-sqaured-diffs tensor\n",
    "\n",
    "    val_diff = val_activity_tensor-val_fwrf_model.pred_expr  ##difference tensor: (T x V)\n",
    "    val_loss_expr = (val_diff*val_diff).sum(axis=1) ##sum-sqaured-diffs tensor\n",
    "\n",
    "    ##build gradient w.r.t. input vars\n",
    "    grad_expr = tnsr.gradient(trn_loss_expr, wrt=trn_fwrf_model.input_var_dict.values())\n",
    "    \n",
    "    grad_func = function([trn_activity_tensor]+trn_fwrf_model.input_var_dict.values(), grad_expr)\n",
    "    \n",
    "    val_loss_is = np.inf\n",
    "    val_loss_was = np.inf\n",
    "    \n",
    "    err_diff = np.abs(val_loss_is - val_loss_was)\n",
    "    \n",
    "    new_map = np.copy(init_feature_map_dict)\n",
    "    \n",
    "    while err_diff < err_diff_thresh:\n",
    "        \n",
    "        new_map += learning_rate*grad_func(trn_activiy, **new_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####===================ORPHANAGE========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = 256 ##trials\n",
    "D = [61, 62, 63] ##features\n",
    "S = [11, 12, 13]\n",
    "nmaps = len(D)\n",
    "map_names = string.uppercase[:nmaps]\n",
    "V = 100 ##voxels\n",
    "deg_per_stim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MakeSpaceTest(unittest.TestCase):\n",
    "    def test_make_space(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class fwrfTest(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        \n",
    "        #testing of individual layers done with first feature map\n",
    "        #testing of rf_model_space and fwrf done with full feature map dict.\n",
    "        \n",
    "        #--feature maps\n",
    "        self.feature_map_dict = {key: val for key,val in zip(map_names, [np.random.rand(T,D[ii],S[ii],S[ii]) for ii in range(nmaps)])}\n",
    "        self.means = [np.mean(self.feature_map_dict[key],axis=0) for key in self.feature_map_dict.keys()]\n",
    "        self.stds = [np.std(self.feature_map_dict[key],axis=0) for key in self.feature_map_dict.keys()]\n",
    "        self.masks = [np.random.randint(0,2,size=(T,D[ii],S[ii],S[ii])) for ii in range(nmaps)]\n",
    "        \n",
    "        #--input\n",
    "        self.input_shape = (T,D,S[0],S[0])\n",
    "        #this tensor stores feature map\n",
    "        self.f_map_0 = tnsr.tensor4('f_map_0',dtype='float32')\n",
    "\n",
    "        #--layers\n",
    "        self.layers = {}\n",
    "        self.output_shapes = {}\n",
    "    \n",
    "    def test_input_layer_init(self):    \n",
    "        #the lasagne input layer\n",
    "        self.layers['layer_1'] = lasagne.layers.InputLayer(self.input_shape, input_var = self.f_map_0, name='layer_1')\n",
    "        #should not change output shape\n",
    "        self.output_shapes['layer_1'] = self.layers['layer_1'].get_output_shape_for(self.input_shape)\n",
    "        self.assertEqual(self.output_shapes['layer_1'], self.input_shape)\n",
    "    \n",
    "\n",
    "    def test_act_layer_init(self):\n",
    "        #--activation layber\n",
    "        self.layers['act_layer'] = compressive_nonlinearity_layer(self.layers['rf_layer'],name='act_layer')\n",
    "        self.output_shapes['act_layer'] = self.layers['act_layer'].get_output_shape_for(self.output_shapes['rf_layer'])\n",
    "        self.assertEqual(self.output_shapes['act_layer'], self.output_shapes['rf_layer']) ##should not change shape\n",
    "    \n",
    "    def test_norm_layer_init(self):\n",
    "        #--norm layer\n",
    "        self.layers['norm_layer'] = normalization_layer(self.layers['act_layer'], mean=self.means[0], stdev=self.stds[0], mask = self.masks[0], name='norm_layer')\n",
    "        self.output_shapes['norm_layer'] = self.layers['norm_layer'].get_output_shape_for(self.output_shapes['act_layer'])\n",
    "        self.assertEqual(self.output_shapes['norm_layer'], self.output_shapes['act_layer']) ##should not change shape\n",
    "    \n",
    "    def feature_layer_init(self):\n",
    "        #--feature layer\n",
    "        self.layers['feature_layer'] = feature_weights_layer(self.layers['norm_layer'])\n",
    "        self.output_shapes['feature_layer'] = self.layers['feature_layer'].get_output_shape_for(self.output_shapes['norm_layer'])\n",
    "        self.assertEqual(self.output_shapes['feature_layer'], (T,V))\n",
    "    \n",
    "    def tearDown(self):\n",
    "        for s in self.output_shapes:\n",
    "            print s\n",
    "    \n",
    "    \n",
    "    def test_rf_layer(self):\n",
    "        pass\n",
    "    \n",
    "    def test_norm_layer(self):\n",
    "        pass\n",
    "    \n",
    "    def test_modelspace(self):\n",
    "        pass\n",
    "    \n",
    "    def test_feature_weights_layer(self):\n",
    "        pass\n",
    "    \n",
    "    def test_fwrf_init(self):\n",
    "        pass\n",
    "    \n",
    "    def test_learn_lasagne_model(self):\n",
    "        pass\n",
    "    \n",
    "    def test_fwrf_train_me_coarse(self):\n",
    "        pass\n",
    "    \n",
    "    def test_fwrf_train_me_fine(self):\n",
    "        pass\n",
    "    \n",
    "    def test_fwrf_decode(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##rf layer unittest\n",
    "def setUp(self):\n",
    "        \n",
    "        #testing of individual layers done with first feature map\n",
    "        #testing of rf_model_space and fwrf done with full feature map dict.\n",
    "        \n",
    "        #--feature maps\n",
    "        self.feature_map_dict = {key: val for key,val in zip(map_names, [np.random.rand(T,D[ii],S[ii],S[ii]) for ii in range(nmaps)])}\n",
    "        self.means = [np.mean(self.feature_map_dict[key],axis=0) for key in self.feature_map_dict.keys()]\n",
    "        self.stds = [np.std(self.feature_map_dict[key],axis=0) for key in self.feature_map_dict.keys()]\n",
    "        self.masks = [np.random.randint(0,2,size=(T,D[ii],S[ii],S[ii])) for ii in range(nmaps)]\n",
    "        \n",
    "        #--input\n",
    "        self.input_shape = (T,D,S[0],S[0])\n",
    "        #this tensor stores feature map\n",
    "        self.f_map_0 = tnsr.tensor4('f_map_0',dtype='float32')\n",
    "\n",
    "        #--layers\n",
    "        self.layers = {}\n",
    "        self.output_shapes = {}\n",
    "    \n",
    "    def test_input_layer_init(self):    \n",
    "        #the lasagne input layer\n",
    "        self.layers['layer_1'] = lasagne.layers.InputLayer(self.input_shape, input_var = self.f_map_0, name='layer_1')\n",
    "        #should not change output shape\n",
    "        self.output_shapes['layer_1'] = self.layers['layer_1'].get_output_shape_for(self.input_shape)\n",
    "        self.assertEqual(self.output_shapes['layer_1'], self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-49159a4988b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msuite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munittest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTestLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadTestsFromTestCase\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfwrfTest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMakeSpaceTest\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0munittest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextTestRunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msuite\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tnaselar/anaconda/lib/python2.7/unittest/loader.pyc\u001b[0m in \u001b[0;36mloadTestsFromTestCase\u001b[1;34m(self, testCaseClass)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloadTestsFromTestCase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestCaseClass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;34m\"\"\"Return a suite of all tests cases contained in testCaseClass\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestCaseClass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTestSuite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             raise TypeError(\"Test cases should not be derived from TestSuite.\" \\\n\u001b[0;32m     52\u001b[0m                                 \" Maybe you meant to derive from TestCase?\")\n",
      "\u001b[1;31mTypeError\u001b[0m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        \n",
    "#     def _build_trn_pred(self):\n",
    "#         self.trn_pred_expr = lasagne.layers.get_output(self.fwrf)\n",
    "#         #self.trn_pred_func = function(self.input_var_dict.values(), self.trn_pred_expr)\n",
    "    \n",
    "#     def _build_val_pred(self):\n",
    "#         ##this will make predictions using the current normalization params (i.e., since the last call of trn_pred_func)\n",
    "#         self.val_pred_expr = lasagne.layers.get_output(self.fwrf, deterministic='True')\n",
    "#         ##FUNCTION\n",
    "#         self.predicted_activity = function(self.input_var_dict.values(), self.val_pred_expr)\n",
    "        \n",
    "#     def _build_trn_loss(self):\n",
    "#         ##training loss expression: same as above except for the last summing step. so:\n",
    "#         trn_diff = self.voxel_data_tnsr.T[:,:,np.newaxis]-self.trn_pred_expr  ##difference tensor: (V x T x 1) - (V x T x 1) = (V x T x 1)\n",
    "#         self.trn_loss_expr = (trn_diff*trn_diff).sum(axis=1).sum() ##sum-sqaured-diffs tensor: V x 1\n",
    "    \n",
    "#     def _build_val_loss(self): \n",
    "#         ##validation loss\n",
    "#         val_diff = self.voxel_data_tnsr.T[:,:,np.newaxis]-self.val_pred_expr  ##difference tensor: (V x T x 1) - (V x T x 1) = (V x T x 1)\n",
    "#         self.val_loss_expr = (val_diff*val_diff).sum(axis=1) ##sum-sqaured-diffs tensor: V x 1\n",
    "#         ##FUNCTION\n",
    "#         self.loss = function([self.voxel_data_tnsr]+self.input_var_dict.values(), self.val_loss_expr)\n",
    "\n",
    "        \n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to build an input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####input shape\n",
    "input_shape = (T,D,S,S)\n",
    "####this tensor stores feature map\n",
    "f_map_0 = tnsr.tensor4('f_map_0',dtype='float32')  ##(T,D,S,S)\n",
    "\n",
    "####the lasagne input layer\n",
    "layer_1 = lasagne.layers.InputLayer(input_shape, input_var = f_map_0, name='layer_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the rf layer\n",
    "Output will have model-space tensor dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = 48\n",
    "deg_per_stim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "rf_layer = receptive_field_layer(layer_1, G, make_space(S), deg_per_stim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### see initial stack of rfs -- they are all the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f77cf4f4510>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAEACAYAAABLUDivAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77ce389250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extent=[-deg_per_stim/2.,deg_per_stim/2.,deg_per_stim/2.,-deg_per_stim/2.]\n",
    "plt.imshow(rf_layer.make_rf_stack()[-1,:,:],cmap=plt.cm.Reds, interpolation='none', extent=extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### but now change them, and re-view them\n",
    "NOTE: everytyhing is specified in degrees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f77cf6cb550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAEACAYAAABLUDivAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77d05f6c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_layer.x0.set_value(-5*np.ones(G).astype('float32'))\n",
    "rf_layer.y0.set_value(5*np.ones(G).astype('float32'))\n",
    "rf_layer.sig.set_value(3*np.ones(G).astype('float32'))\n",
    "plt.imshow(rf_layer.make_rf_stack()[3,:,:],cmap=plt.cm.Reds, interpolation='none', extent=extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### compile output function, test shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output(rf_layer)\n",
    "pred_func = function([layer_1.input_var], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "(48, 256, 64)\n"
     ]
    }
   ],
   "source": [
    "outp = pred_func(np.zeros(input_shape).astype('float32'))\n",
    "print outp.dtype\n",
    "print outp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check out the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x0, y0, sig]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasagne.layers.get_all_params(rf_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the activation layer\n",
    "For the gabor model, we sometimes use the log(1+sqrt(x)) transforms. Here's that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_layer = compressive_nonlinearity_layer(rf_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x0, y0, sig]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasagne.layers.get_all_params(act_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_3 = lasagne.layers.get_output(act_layer)\n",
    "l_3_func = function([layer_1.input_var], l_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 256, 64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_3_func(np.zeros(input_shape).astype('float32')).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization layer\n",
    "    pass deterministic=False for training\n",
    "    pass deterministic=True for validation\n",
    "    in our case output shape = (G,T,D), so normalization must integrate over axis=1\n",
    "    this will give mean/stdev/gamma/beta params of shape (G,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_layer = lasagne.layers.BatchNormLayer(rf_layer, axes=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 64)\n",
      "(48, 64)\n",
      "(48, 64)\n",
      "(48, 64)\n"
     ]
    }
   ],
   "source": [
    "print norm_layer.mean.get_value().shape\n",
    "print norm_layer.inv_std.get_value().shape\n",
    "print norm_layer.gamma.get_value().shape\n",
    "print norm_layer.beta.get_value().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### so everytime you call the layer_4 output function the mean/stdev params are updated.\n",
    "but if you call with deterministic='True', the mean/stdev are not updated, they are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_pred_expr = lasagne.layers.get_output(norm_layer)\n",
    "norm_pred_func = function([layer_1.input_var], norm_pred_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.random(size=input_shape).astype('float32')\n",
    "_=norm_pred_func(x)\n",
    "before= norm_layer.mean.get_value()\n",
    "for _ in range(10):\n",
    "    x = np.random.random(size=input_shape).astype('float32')\n",
    "    um=norm_pred_func(x)\n",
    "after = norm_layer.mean.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f778f3a3c90>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCS1+v+nMkTzVrhJxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f778f13f690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(before,after)\n",
    "# lims = [np.min(after), np.max(after)]\n",
    "# plt.xlim(lims)\n",
    "# plt.ylim(lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...call layer_4 output function with deterministic='Trure' in a loop, and means are updated stay fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_pred_expr_fixed = lasagne.layers.get_output(norm_layer, deterministic = 'True')\n",
    "norm_pred_func_fixed = function([layer_1.input_var], norm_pred_expr_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f77cc20d910>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE3LwjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77cc32d290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "before= norm_layer.mean.get_value()\n",
    "for _ in range(10):\n",
    "    x = np.random.random(size=input_shape).astype('float32')\n",
    "    _=norm_pred_func_fixed(x)\n",
    "after = norm_layer.mean.get_value()\n",
    "plt.scatter(before,after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature weights layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_5 = feature_weights_layer(norm_layer,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_5_pred_expr = lasagne.layers.get_output(layer_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_5_pred_func = function([layer_1.input_var],layer_5_pred_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 256, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "layer_5_pred_func(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "Usually we'll have multiple features spaces, where each feature space is a stack of feature maps with the same \n",
    "resolution.  \n",
    "Because resolution can vary, we'll need construct separate rf layers for each feature space.  \n",
    "Then, we can merge the feature spaces, and apply whatever normalization, nonlinearity, etc. we want before apply the feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### construct feature space dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_map_names = ['a','b','c']\n",
    "feature_map_dict = {}\n",
    "feature_depths = [12, 45, 78]\n",
    "for fmap,fdep in zip(feature_map_names,feature_depths):\n",
    "    feature_map_dict[fmap] = np.random.random(size=(T,fdep,S,S)).astype('float32')\n",
    "\n",
    "num_rfs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##construct model space\n",
    "input_var_dict, model_space = make_fwrf_model_space(feature_map_dict, num_rfs, deg_per_stim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the model space part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 64, 125, 125)\n"
     ]
    }
   ],
   "source": [
    "print model_space.get_output_shape_for([input_shape])\n",
    "\n",
    "model_space_pred_expr = lasagne.layers.get_output(model_space)\n",
    "model_space_pred_func = function(input_var_dict.values(), model_space_pred_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[rf_a.x0, rf_a.y0, rf_a.sig]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasagne.layers.get_all_params(model_space,rf_param=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "important: note how we need to unpack the argument list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 135)\n",
      "135\n"
     ]
    }
   ],
   "source": [
    "print model_space_pred_func(*feature_map_dict.values()).shape\n",
    "print sum(feature_depths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two ways to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1**: Grid search over many rfs for each voxel. In this case we run the model space+act+norm network to construct a model-space tensor for the training and testing data. Then we construct a simple linear network where we just apply the feature weights to the mst. We do this to avoid having to re-apply the rf models to the feature maps each time we want to make a prediction. The gradient of loss is now w.r.t. to the feature weights only, but the training procedure needs to include a mechanism for selecting the best rf for each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##define the model space network\n",
    "model_space = compressive_nonlinearity_layer(model_space)\n",
    "model_space = lasagne.layers.BatchNormLayer(model_space, axes=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##define the prediction expression / function\n",
    "model_space_pred_expr = lasagne.layers.get_output(model_space, deterministic='False')\n",
    "model_space_pred_func = function(input_var_dict.values(), model_space_pred_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 135)\n"
     ]
    }
   ],
   "source": [
    "##construct the mst\n",
    "mst = model_space_pred_func(*feature_map_dict.values())\n",
    "print mst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##now construct full model: the input is the model space tensor\n",
    "mst_tnsr = tnsr.tensor3('mst')\n",
    "fwrf = lasagne.layers.InputLayer((mst.shape[0],None,mst.shape[-1]), input_var=mst_tnsr,name='fwrf')\n",
    "fwrf = feature_weights_layer(fwrf,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##construct prediction expr: (G,T,V)\n",
    "fwrf_pred_expr = lasagne.layers.get_output(fwrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##loss expression: we already have all this stuff from before\n",
    "voxel_data_tnsr = tnsr.matrix('voxel_data_tnsr')  ##voxel data tensor: T x V\n",
    "diff = voxel_data_tnsr-fwrf_pred_expr  ##difference tensor: (T x V) - (G x T x V) = (G x T x V)\n",
    "sq_diff = (diff*diff).sum(axis=1) ##sum-sqaured-diffs tensor: G x V\n",
    "\n",
    "\n",
    "fwrf_loss_expr = sq_diff.sum()  ##<<this sum is critical. theano knows not to differentiate w.r.t params if deriv. is always 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##update rule\n",
    "learning_rate = 0.1\n",
    "params = lasagne.layers.get_all_params(fwrf)\n",
    "fwrf_update = lasagne.updates.sgd(fwrf_loss_expr, params, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##training kernel: returns loss, updates params\n",
    "trn_kernel = function([mst_tnsr,voxel_data_tnsr], fwrf_loss_expr, updates=fwrf_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##validation kernel: returns predictions and loss, leaves params alone\n",
    "val_kernel = function([mst_tnsr, voxel_data_tnsr], [fwrf_pred_expr, fwrf_loss_expr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.03935089508e+14\n"
     ]
    }
   ],
   "source": [
    "neural_data = np.random.random(size=(T,V)).astype('float32')\n",
    "print trn_kernel(mst, neural_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 100)\n",
      "3.93386847789e+24\n"
     ]
    }
   ],
   "source": [
    "pred,loss = val_kernel(mst, neural_data)\n",
    "print pred.shape\n",
    "print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##training proc: pseudo-code\n",
    "# val_in, val_out = get_val_data(mst)\n",
    "# for trn_in, trn_out in batch_generator(mst):\n",
    "#     trn_loss = trn_kernel(trn_in, trn_out)\n",
    "#     _,val_loss = val_kernel(val_in,val_out) ##don't really care about val_pred here\n",
    "#     if val_loss is less_than_it_was:\n",
    "#         rf_idx = select_best_rf(fwrf)\n",
    "#         NU = select_best_weights(fwrf)\n",
    "#     if val_loss is too_big:\n",
    "#         break\n",
    "# new_fwrf = make_best_model(rf_idx, NU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2**: Gradient descent on rf parameters. In this case we optimize one rf per voxel using gradient descent. We build the full network and take gradients w.r.t. both the mean/size of each voxel's rf and the feature weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Construct the full model network\n",
    "##add activation\n",
    "fwrf = compressive_nonlinearity_layer(model_space)\n",
    "\n",
    "##add normalization\n",
    "fwrf = lasagne.layers.BatchNormLayer(fwrf, axes=(1,))\n",
    "\n",
    "##add feature layer\n",
    "num_outputs=1\n",
    "fwrf = feature_weights_layer(fwrf,num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##prediction expression with update of normalization params: ##V x T x 1\n",
    "trn_pred_expr = lasagne.layers.get_output(fwrf)\n",
    "# trn_pred_func = function(input_var_dict.values(), trn_pred_expr)\n",
    "\n",
    "##this will make predictions using the current normalization params (i.e., since the last call of trn_pred_func)\n",
    "val_pred_expr = lasagne.layers.get_output(fwrf, deterministic='True')\n",
    "# val_pred_func = function(input_var_dict.values(), val_pred_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##voxel data needed for loss\n",
    "voxel_data_tnsr = tnsr.matrix('voxel_data_tnsr')  ##voxel data tensor: (T x V)\n",
    "\n",
    "##training loss expression: same as above except for the last summing step. so:\n",
    "trn_diff = voxel_data_tnsr.T[:,:,np.newaxis]-trn_pred_expr  ##difference tensor: (V x T x 1) - (V x T x 1) = (V x T x 1)\n",
    "trn_loss = (trn_diff*trn_diff).sum(axis=1) ##sum-sqaured-diffs tensor: V x 1\n",
    "\n",
    "##validation loss\n",
    "val_diff = voxel_data_tnsr.T[:,:,np.newaxis]-val_pred_expr  ##difference tensor: (V x T x 1) - (V x T x 1) = (V x T x 1)\n",
    "val_loss = (val_diff*val_diff).sum(axis=1) ##sum-sqaured-diffs tensor: V x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[rf_a.x0, rf_a.y0, rf_a.sig, beta, gamma, feature_weights]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasagne.layers.get_all_params(fwrf,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##-----check the gradients\n",
    "fw_grad = tnsr.grad(trn_loss.sum(),fwrf.NU)\n",
    "rf_grad = tnsr.grad(trn_loss.sum(),lasagne.layers.get_all_params(fwrf,rf_param=True))\n",
    "\n",
    "\n",
    "##functionalize\n",
    "fw_grad_func = function([voxel_data_tnsr]+input_var_dict.values(), fw_grad)\n",
    "rf_grad_func = function([voxel_data_tnsr]+input_var_dict.values(), rf_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding_grad = tnsr.grad(trn_loss.sum(),lasagne.layers.get_all_params(fwrf,trainable=True))\n",
    "encoding_grad_func = function([voxel_data_tnsr]+input_var_dict.values(), encoding_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 135, 1)\n"
     ]
    }
   ],
   "source": [
    "print fw_grad_func(neural_data,*feature_map_dict.values()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neural_data = np.random.random(size=(T,num_rfs)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads= encoding_grad_func(neural_data,*feature_map_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 135, 1)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grad_func(neural_data,*feature_map_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##update rule\n",
    "learning_rate = 0.1\n",
    "params = lasagne.layers.get_all_params(fwrf,trainable=True)\n",
    "fwrf_update = lasagne.updates.sgd(trn_loss.sum(),params,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##training kernel\n",
    "trn_kernel = function(input_var_dict.values()+[voxel_data_tnsr], trn_loss, updates=fwrf_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##validation kernel\n",
    "val_kernel = function(input_var_dict.values()+[voxel_data_tnsr], [val_pred_expr,val_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##training proc: pseudo-code\n",
    "# val_in, val_out = get_val_data(mst)\n",
    "# for trn_in, trn_out in batch_generator(mst):\n",
    "#     trn_loss = trn_kernel(trn_in, trn_out)\n",
    "#     _,val_loss = val_kernel(val_in,val_out) ##don't really care about val_pred here\n",
    "#     if val_loss is less_than_it_was:\n",
    "#         NU = lasagne.layers.get_param_values(fwrf)[-1]  ##<<something like this\n",
    "#     if val_loss is too_big:\n",
    "#         break\n",
    "# new_fwrf = make_best_model(NU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "uses \"new_fwrf\" resulting from one of the two training methods.\n",
    "How to treat input layer?\n",
    "Link input to a shared variable? Probably a bad idea.\n",
    "Create a feature map layer that has a shared variable param? When building the \"new_fwrf\", make a decoding version with this feature map layer at the front?\n",
    "Or: just dont' use shared variable here...probably the best idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoding_pred_expr = lasagne.layers.get_output(fwrf, deterministic='True') ##V x T x 1\n",
    "target_activity = tnsr.matrix('target_activity')  #T x V\n",
    "decoding_loss = val_loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##apparently, the input_var can't be share variable, so we'll have to do updates explicitly\n",
    "decoding_grad = tnsr.grad(decoding_loss,wrt=input_var_dict.values())\n",
    "decoding_grad_func = function([voxel_data_tnsr]+input_var_dict.values(), decoding_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decodeable_pattern = np.random.random(size=(T,num_rfs)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_grad = decoding_grad_func(decodeable_pattern, *feature_map_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foo = shared(np.ones((2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fwrf_model_space'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_space.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = tnsr.tensor4('inp')\n",
    "rfs = tnsr.tensor3('rfs')\n",
    "outp = tnsr.tensordot(inp, rfs, axes= [[2,3],[1,2]])\n",
    "florg = function([inp, rfs], outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 64, 100)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((T,D,S,S),dtype='float32')\n",
    "r = np.zeros((V, S, S),dtype='float32')\n",
    "blarg = florg(x,r)\n",
    "print blarg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (256,64,125,125) (1,100,125,125) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1076-645ca8f7f1e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (256,64,125,125) (1,100,125,125) "
     ]
    }
   ],
   "source": [
    "x*r[np.newaxis,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featw = tnsr.matrix('featw')\n",
    "out2 = (outp*featw[np.newaxis,:,:]).sum(axis=1)\n",
    "final_out = function([outp,featw], out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 100)"
      ]
     },
     "execution_count": 1094,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.zeros((D,V), dtype='float32')\n",
    "final_out(blarg, w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
