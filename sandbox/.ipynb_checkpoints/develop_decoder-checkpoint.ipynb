{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lasagne\n",
    "import matplotlib.pyplot as plt\n",
    "from fwrf.models import *\n",
    "from fwrf.utils import *\n",
    "from fwrf.utils import make_rf_table\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_feature_maps(trn_fwrf_model, val_fwrf_model, trn_activity, val_activity, feature_map_dict, error_diff_thresh):\n",
    "    ##build loss\n",
    "    trn_activity_tensor = tnsr.matrix('trn_activity')\n",
    "    val_activity_tensor = tnsr.matrix('val_activity')\n",
    "\n",
    "    trn_diff = trn_activity_tensor-trn_fwrf_model.pred_expr  ##difference tensor: (T x V)\n",
    "    trn_loss_expr = (trn_diff*trn_diff).sum(axis=1) ##sum-sqaured-diffs tensor:: the sum is over VOXELS.\n",
    "    \n",
    "    ##Outputs are scalar\n",
    "    trn_loss_func = function([[trn_activity_tensor]+trn_fwrf_model.input_var_dict.values()], trn_loss_expr.sum())\n",
    "\n",
    "    val_diff = val_activity_tensor-val_fwrf_model.pred_expr  ##difference tensor: (T x V)\n",
    "    val_loss_expr = (val_diff*val_diff).sum(axis=1) ##sum-sqaured-diffs tensor: SUM OVER VOXELS\n",
    "    \n",
    "    ##Outputs should just be T.\n",
    "    val_loss_func = BLARF\n",
    "\n",
    "    ##build gradient w.r.t. input vars\n",
    "    grad_expr = tnsr.gradient(trn_loss_expr, wrt=trn_fwrf_model.input_var_dict.values())\n",
    "    \n",
    "    ##a list of feature map gradients.\n",
    "    ##each gradient in the list should be like T,D,S,S tensor4\n",
    "    grad_func = function([trn_activity_tensor]+trn_fwrf_model.input_var_dict.values(), grad_expr)\n",
    "    \n",
    "    val_loss_is = np.inf\n",
    "    val_loss_was = np.inf\n",
    "    \n",
    "    err_diff = np.abs(val_loss_is - val_loss_was)\n",
    "    \n",
    "    new_feature_map_values = SOME KIND OF COPY OF THE THE INIT. FEATURE MAPS. A LIST OF T,D,S,S tensors\n",
    "    \n",
    "    for step in range(number_of_steps):\n",
    "        \n",
    "        map_grads = grad_func(BLARF, new_feature_map_values) ##list of T,D,S,S tensors\n",
    "        ##update each feature map using \n",
    "        for fm in len(map_grads):\n",
    "            new_feature_map_values[fm] -= learning_rate*map_grads[fm]\n",
    "        \n",
    "        ##test against val set\n",
    "        val_loss = val_loss_fun(new_feature_map_values)\n",
    "        \n",
    "        ##save if improved. We'll stay with the batch model where we iterate for fixed number for all batches\n",
    "        \n",
    "    ##On exit, give the feature map, the val_loss, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class batch_feature_map_decoder(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 trn_model, val_model,\n",
    "                 trn_model_input_tnsr_dict, trn_model_input_tnsr_dict,\n",
    "                 trn_data_generator, val_data_generator,\n",
    "                 epochs = 1,\n",
    "                 check_every=10,\n",
    "                 num_iters=10,\n",
    "                 learning_rate = 1.0,\n",
    "                 learn_these_maps = None,\n",
    "                 check_dims = True,\n",
    "                 print_stuff=False):\n",
    "\n",
    "        '''\n",
    "    batch_feature_map_decoder(   init_feature_maps,\n",
    "                                 trn_model, val_model,\n",
    "                                 trn_model_input_tnsr_dict, trn_model_input_tnsr_dict,\n",
    "                                 trn_data_generator, val_data_generator,\n",
    "                                 epochs = 1,\n",
    "                                 check_every=10,\n",
    "                                 num_iters=10,\n",
    "                                 learning_rate = 1.0,\n",
    "                                 learn_these_feature_maps = None,\n",
    "                                 check_dims = True,\n",
    "                                 print_stuff=False)\n",
    "\n",
    "        a class for gradient descent on independent feature maps, given encoding models and activity patterns.\n",
    "\n",
    "        inputs:\n",
    "                init_feature_maps   ~ list of feature_map_dicts. length of list = num_stimuli\n",
    "                trn_model,val_model ~ lasagne models\n",
    "   trn_/val_model_input_tensor_dict ~ dicts of theano tensors that are inputs to trn/val_model, resp.\n",
    "                 trn_data_generator ~ a generator of (num_stimuli, num_voxels) voxel activity matrices. you must write it.\n",
    "                                      it should yield batches of data, i.e., trn_data_generator() returns your training batches.\n",
    "                                      iteration is over voxels, so each batch is (num_stimuli, batch_size) matrix. \n",
    "                 val_data_generator ~ generator for validation data\n",
    "                             epochs ~ number of times through all batches in the generator\n",
    "                        check_every ~ int. how many gradients steps until validation loss is checked. default = 10\n",
    "                          num_iters ~ number of gradient steps per batch before tranistioning to next batch. default = 100\n",
    "                      learning_rate ~ size of gradient step. default = 1\n",
    "           learn_these_feature_maps ~ list of names of feature maps to learn. \n",
    "                                      default = None, meaning learn all feature maps\n",
    "                         check_dims ~ if True, run potentially slow sanity check on dimensions of inputs\n",
    "        outputs:\n",
    "        decoded_feature_maps ~ original l_model with params all trained up. this is a convenience, as params are learned in-place\n",
    "              final_val_loss ~ the final validation loss for each of the models\n",
    "                 trn_history ~ array showing training err history\n",
    "                 val_history ~ history showing number of voxels with decreased validat'n loss at each iteration\n",
    "\n",
    "        '''\n",
    "        \n",
    "        ##record all the inputs\n",
    "        if type(init_feature_maps) not list:\n",
    "            init_feature_maps = [init_feature_maps]\n",
    "        self.init_feature_maps = init_feature_maps\n",
    "        self.num_stimuli = len(init_feature_maps)\n",
    "        self.trn_model = trn_model\n",
    "        self.val_model = val_model\n",
    "        self.learn_these_feature_maps = learn_these_feature_maps\n",
    "        self.trn_model_input_tnsr_dict = trn_model_input_tnsr_dict\n",
    "        self.val_model_input_tnsr_dict = val_model_input_tnsr_dict\n",
    "        self.trn_data_generator = trn_data_generator\n",
    "        self.val_data_generator = val_data_generator\n",
    "        self.epochs = epochs\n",
    "        self.check_every=check_every\n",
    "        self.num_iters = num_iters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.print_stuff = print_stuff\n",
    "        self.check_dims = check_dims\n",
    "\n",
    "        ##inspect one batch of input to sanity check dimensions\n",
    "        if self.check_dims:\n",
    "            self.grab_example_batch()\n",
    "            ##check data dimensions\n",
    "            self.check_consistent()\n",
    "    \n",
    "        ##get learned feature maps: stores as a list of shared variables just like output of lasage.layers.get_all_params\n",
    "        self.get_learned_params()\n",
    "\n",
    "\n",
    "        ##try to determine which dimension of each parameter is the voxel dimension\n",
    "        self.find_batch_dimension()\n",
    "\n",
    "        ##construct a gradient update and loss functions to be called iteratively by the learn method\n",
    "        self.construct_training_kernel()\n",
    "    \n",
    "    def grab_example_batch( self ):\n",
    "        self.trn_in, self.trn_out = next(self.trn_data_generator()) \n",
    "        self.val_in, self.val_out = next(self.val_data_generator())\n",
    "\n",
    "\n",
    "    def check_consistent( self ):\n",
    "        ##read first batches to get some dimensions. assumes first dimension of input is time, last dimension is voxel \n",
    "        num_trn_images,trn_batch_size = self.trn_out.shape[0],self.trn_out.shape[-1]\n",
    "        num_val_images,val_batch_size = self.val_out.shape[0],self.val_out.shape[-1]   \n",
    "        assert num_trn_images == num_val_images, \"number of trn/val images don't match\"\n",
    "        assert num_trn_voxels == self.num_stimuli,\"number stimuli in trn/val does not match specified number of stimuli\"\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_learned_params( self ):\n",
    "        if self.learn_these_params is not None:            \n",
    "            self.params = [fev for v in trn_.values()] ##<<unpack the dict.\n",
    "        else:\n",
    "            self.params = lasagne.layers.get_all_params(self.l_model,trainable=True)\n",
    "        print 'will solve for: %s' %(self.params)\n",
    "\n",
    "    def find_batch_dimension( self ):\n",
    "        if self.voxel_dims is None:\n",
    "            self.voxel_dims = {}\n",
    "            for p in self.params:\n",
    "                ##the voxel dimension should be the one that matches \"num_voxels\"\n",
    "                vdim = [ii for ii,pdim in enumerate(p.shape.eval()) if pdim==self.num_voxels]\n",
    "\n",
    "                ##if we happen to have multiple dimensions that = \"num_voxels\", user must disambiguate\n",
    "                assert len(vdim)==1, \"can't determine voxel dimension for param %s. supply 'voxel_dims' argument\" %p.name\n",
    "                self.voxel_dims[p.name] = vdim[0]\n",
    "\n",
    "                \n",
    "    ##construct a gradient update and loss functions to be called iteratively by the learn method\n",
    "    def construct_training_kernel( self ):\n",
    "        voxel_data_tnsr = tnsr.matrix('voxel_data_tnsr')  ##voxel data tensor: (T x V)\n",
    "\n",
    "        ##get symbolic prediction expression\n",
    "        pred_expr = lasagne.layers.get_output(self.l_model)  ##voxel prediction tensor: (T x V)\n",
    "\n",
    "        ##generate symbolic loss expression\n",
    "        trn_diff = voxel_data_tnsr-pred_expr        ##difference tensor: shape = (T, V)\n",
    "        loss_expr = (trn_diff*trn_diff).sum(axis=0) ##sum squared diffs over time: shape = (V,)\n",
    "\n",
    "        ##for *training* error we compute of errors along voxel dimension.\n",
    "        ##we have to do this because auto-diff requires. a scalar loss function.\n",
    "        ##BUT: this is fine because gradient w.r.t. one voxel's weights is not affected by loss for any other voxel.\n",
    "        trn_loss_expr = loss_expr.sum()\n",
    "\n",
    "        #construct update rule using *training* loss.\n",
    "        fwrf_update = lasagne.updates.sgd(trn_loss_expr,self.params,learning_rate=self.learning_rate)\n",
    "        self.trn_kernel = function([voxel_data_tnsr]+self.model_input_tnsr_dict.values(),\n",
    "                                   trn_loss_expr,\n",
    "                                   updates=fwrf_update)           \n",
    "        print 'will update wrt: %s' % (self.params,)\n",
    "        \n",
    "        ##compile loss and training functions\n",
    "        ##NOTE: this is *validation* loss, not summed over voxels, so it has len = num_voxels\n",
    "        print 'compiling...'\n",
    "        self.loss = function([voxel_data_tnsr]+self.model_input_tnsr_dict.values(), loss_expr)\n",
    "                \n",
    "    \n",
    "    ##if the last gradient step made things better for some voxels, update their parameters\n",
    "    def update_best_param_values(self, best_param_values, improved_voxels):\n",
    "        for ii,p in enumerate(self.params):\n",
    "            vdim = self.voxel_dims[p.name] ##the voxel dimension\n",
    "            s = [slice(None),]*p.ndim      ##create a slicing object with right number of dims\n",
    "            s[vdim] = improved_voxels      ##assign improved voxel indices to the correct dim of the slice object\n",
    "            best_param_values[ii][s] = np.copy(p.get_value()[s])   ##keep a record of the best params.\n",
    "        return best_param_values        \n",
    "    \n",
    "    ##iteratively perform gradient descent\n",
    "    def learn(self):\n",
    "        \n",
    "        ##initialize best parameters to whatever they are\n",
    "        best_param_values = [np.copy(p.get_value()) for p in self.params]\n",
    "        \n",
    "        ##initalize validation loss to whatever you get from initial weigths\n",
    "        val_loss_was = 0\n",
    "        for val_in, val_out in self.val_data_generator():\n",
    "            val_loss_was += self.loss(val_out, *val_in.values())\n",
    "\n",
    "        ##initialize train loss to whatever, we only report the difference    \n",
    "        trn_loss_was = 0.0 ##we keep track of total across voxels as sanity check, since it *must* decrease\n",
    "  \n",
    "        val_history = []\n",
    "        trn_history = []\n",
    "        \n",
    "        ##descend and validate\n",
    "        epoch_count = 0\n",
    "        while epoch_count < self.epochs:\n",
    "            print '=======epoch: %d' %(epoch_count) \n",
    "            for trn_in, trn_out in self.trn_data_generator():\n",
    "                step_count = 0\n",
    "                while step_count < self.num_iters:\n",
    "                    \n",
    "                    ##update params, output training loss\n",
    "                    trn_loss_is = self.trn_kernel(trn_out, *trn_in.values())                    \n",
    "                    if step_count % self.check_every == 0:\n",
    "\n",
    "                        ##check for improvements\n",
    "                        val_loss_is = 0\n",
    "                        for val_in, val_out in self.val_data_generator():\n",
    "                            val_loss_is += self.loss(val_out, *val_in.values())\n",
    "                        improved = (val_loss_is < val_loss_was)\n",
    "\n",
    "                        ##update val loss\n",
    "                        val_loss_was[improved] = val_loss_is[improved]\n",
    "\n",
    "                        ##replace old params with better params\n",
    "                        best_param_values = self.update_best_param_values(best_param_values, improved)\n",
    "\n",
    "                        ##report on loss history\n",
    "                        val_history.append(improved.sum())\n",
    "                        trn_history.append(trn_loss_is)\n",
    "                        if self.print_stuff:\n",
    "                            print '====iter: %d' %(step_count)\n",
    "                            print 'number of improved models: %d' %(val_history[-1])\n",
    "                            print 'trn error: %0.6f' %(trn_history[-1])\n",
    "                    \n",
    "                    step_count += 1\n",
    "\n",
    "            epoch_count += 1\n",
    "\n",
    "        ##restore best values of learned params\n",
    "        set_named_model_params(self.l_model, **{k.name:v for k,v in zip(self.params, best_param_values)})\n",
    "       \n",
    "\n",
    "        return self.l_model, val_loss_was, val_history, trn_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
