{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original class definitions + unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "import string\n",
    "from types import GeneratorType\n",
    "from theano import tensor as tnsr\n",
    "from theano import function, shared\n",
    "from hrf_fitting.src.feature_weighted_rf_models import receptive_fields, make_rf_table\n",
    "from hrf_fitting.src.features import make_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##convenience functions for getting/setting params by name\n",
    "\n",
    "def get_model_params_names(l_model):\n",
    "    '''\n",
    "    Return list of names of all parameters (shared variables)\n",
    "    '''\n",
    "    p_names = [shared_var.name for shared_var in lasagne.layers.get_all_params(l_model)]\n",
    "    return p_names\n",
    "\n",
    "def set_named_model_params(l_model,**kwargs):\n",
    "    '''\n",
    "    set_named_model_params(lasagne_model, param1=param1_value, ...)\n",
    "    sets the values of all named params. uses theano shared variable method \"set_value()\"\n",
    "    returns nothing, modifies the l_model in-place.\n",
    "    '''\n",
    "    for shared_var in lasagne.layers.get_all_params(l_model):\n",
    "        if shared_var.name in kwargs.keys():\n",
    "            shared_var.set_value(kwargs[shared_var.name])\n",
    "\n",
    "def get_named_param_shapes(l_model, *args):\n",
    "    \"\"\"\n",
    "    get_named_param_shapes(l_model, 'param1',...)\n",
    "    \n",
    "    returns dictionary of name/shape-tuple pairs\n",
    "    \n",
    "    \"\"\"\n",
    "    param_shape_dict = {}\n",
    "    for shared_var in lasagne.layers.get_all_params(l_model):\n",
    "        if shared_var.name in args:\n",
    "            try:\n",
    "                param_shape_dict[shared_var.name] = shared_var.get_value().shape\n",
    "                print 'parameter %s has shape %s' %(shared_var.name, param_shape_dict[shared_var.name])\n",
    "            except AttributeError:\n",
    "                print 'no shape attribute available for %s' %(shared_var.name)\n",
    "    return param_shape_dict    \n",
    "\n",
    "\n",
    "def get_named_params(l_model, *args):\n",
    "    \"\"\"\n",
    "    get_named_params(l_model, 'param1',...)\n",
    "    \n",
    "    return dictionary of name/theano-shared-variable pairs.\n",
    "    \n",
    "    if you modify values of the returned shared variables, you will modify the values\n",
    "    of the corresponding parameters attached to l_model.\n",
    "    \"\"\"\n",
    "    param_value_dict = {}\n",
    "    for shared_var in lasagne.layers.get_all_params(l_model):\n",
    "        if shared_var.name in args:\n",
    "            param_value_dict[shared_var.name] = shared_var\n",
    "    return param_value_dict\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##TODO: GENERALIZE SO NOT COMMITTED TO GAUSSIAN RF MODEL\n",
    "##TODO: CREATE AN \"RF GRID\" CLASS TO PASS TO THIS.\n",
    "\n",
    "class receptive_field_layer(lasagne.layers.Layer):\n",
    "    '''\n",
    "    receptive_field_layer(incoming, num_voxels, (X_grid, Y_grid), deg_per_stim, x0=None, y0=None, sig=None)\n",
    "    \n",
    "    --inputs\n",
    "    incoming ~ should be an input layer with (T,D,S,S) input_var tensor4\n",
    "    \n",
    "    num_voxels  ~ V\n",
    "    \n",
    "    (X_grid, Y_grid) ~ numpy matrices of spatial 2D grid specified in pixels. output \"make_space\" or np.meshgrid\n",
    "    \n",
    "    when x0,y0,sig=None initializes each rf with (x0,y0)=(0,0), stdev = 1. otherwise, can initialize with a theano shared variable or numpy array\n",
    "    \n",
    "    creates a stack of 2D gaussian rf blobs. The stack has dimensions (V,S,S). just call \"make_rf_stack()\"\n",
    "    \n",
    "    application gives a tensor of shape (T,D,V) \n",
    "    \n",
    "    --attributes\n",
    "    params are [x0, y0, sig], which are the (x,y) coors and standard dev., resp., of the gaussian rfs\n",
    "    for some set of voxels\n",
    "    \n",
    "    make_rf_stack() will construct a stack of visualizable rf's\n",
    "    '''    \n",
    "    def __init__(self, incoming, num_voxels, space_grid, deg_per_stim, x0=None, y0=None, sig=None, dtype='float32', **kwargs):\n",
    "        super(receptive_field_layer, self).__init__(incoming, **kwargs)  ##this will give us an \"input_shape\" attribute\n",
    "        self.S = self.input_shape[-1]\n",
    "        self.V = num_voxels\n",
    "        \n",
    "        self.pix_per_deg = self.S * (1./deg_per_stim)  ##this will convert into pixels\n",
    "        self.Xm = shared(space_grid[0].astype(dtype))  ##explicit casting. is there a better way to do this?\n",
    "        self.Ym = shared(space_grid[1].astype(dtype))        \n",
    "        \n",
    "        if x0 is not None:\n",
    "            self.x0 = self.add_param(x0,(self.V,), name='x0',rf_param=True,x0=True)\n",
    "        else:\n",
    "            self.x0 = self.add_param(np.zeros(self.V,dtype=dtype),(self.V,), name='x0',rf_param=True,x0=True)\n",
    "        \n",
    "        if y0 is not None:\n",
    "            self.y0 = self.add_param(y0,(self.V,), name='y0',rf_param=True,y0=True)\n",
    "        else:\n",
    "            self.y0 = self.add_param(np.zeros(self.V,dtype=dtype),(self.V,), name='y0',rf_param=True,y0=True)\n",
    "        \n",
    "        if sig is not None:\n",
    "            self.sig = self.add_param(sig,(self.V,), name='sig',rf_param=True,sig=True) \n",
    "        else:\n",
    "            self.sig = self.add_param(np.ones(self.V,dtype=dtype),(self.V,), name='sig',rf_param=True,sig=True) \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        ##we put the pix_per_deg conversion here becuase x0,y0,sig will be shared across feature spaces\n",
    "        #this expression has shape (V,S,S)\n",
    "        self.gauss_expr = ((1. / 2*np.pi*(self.sig[:,np.newaxis,np.newaxis]*self.pix_per_deg)**2)\n",
    "                           *tnsr.exp(-((self.Xm[np.newaxis,:,:]-self.x0[:,np.newaxis,np.newaxis]*self.pix_per_deg)**2\n",
    "                                       + (self.Ym[np.newaxis,:,:]-self.y0[:, np.newaxis,np.newaxis]*self.pix_per_deg)**2)\n",
    "                                     /(2*(self.sig[:,np.newaxis,np.newaxis]*self.pix_per_deg)**2)))\n",
    "        \n",
    "        self.make_rf_stack = function([],self.gauss_expr)\n",
    "        \n",
    "    \n",
    "    #T,D,S,S x V,S,S --> T,D,V\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return tnsr.tensordot(input, self.gauss_expr, axes=[[2,3],[1,2]])\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape): ##(T,D,V)\n",
    "        return (input_shape[0], input_shape[1], self.V)\n",
    "    \n",
    "    def set_value_for_voxel(self, voxel_idx=None, x0=None, y0=None, sig=None):\n",
    "        if x0 is not None: ##we assume the shared variable has already been created and registered\n",
    "            x0_temp = self.x0.get_value()\n",
    "            x0_temp[voxel_idx] = x0\n",
    "            self.x0.set_value(x0_temp)\n",
    "        if y0 is not None:\n",
    "            y0_temp = self.y0.get_value()\n",
    "            y0_temp[voxel_idx] = y0\n",
    "            self.y0.set_value(y0_temp)\n",
    "        if sig is not None:\n",
    "            sig_temp = self.sig.get_value()\n",
    "            sig_temp[voxel_idx] = sig\n",
    "            self.sig.set_value(sig_temp)\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "class compressive_nonlinearity_layer(lasagne.layers.Layer):\n",
    "    '''\n",
    "    a compressive function used in a previous publication. works well.\n",
    "    requires all inputs to be positive, BUT DOES NOT CHECK!\n",
    "    computes elementwise log(1+sqrt(input))\n",
    "    '''\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return tnsr.log(1+tnsr.sqrt(input))\n",
    "    \n",
    "\n",
    "class normalization_layer(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, mean=lasagne.init.Constant([0]), stdev=lasagne.init.Constant([1]), mask = lasagne.init.Constant([1]), **kwargs):\n",
    "        super(normalization_layer,self).__init__(incoming, **kwargs)\n",
    "        self.mean = self.add_param(mean, self.input_shape[1:], name='mean', trainable=False)\n",
    "        self.stdev = self.add_param(stdev, self.input_shape[1:], name='stdev', trainable=False)\n",
    "        self.stability_mask = self.add_param(mask, self.input_shape[1:], name='stability_mask', trainable=False)\n",
    "    \n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return tnsr.switch(self.stability_mask>0, (input - self.mean[np.newaxis,:,:])/self.stdev[np.newaxis,:,:], 0)\n",
    "\n",
    "class feature_weights_layer(lasagne.layers.Layer):\n",
    "    '''\n",
    "    feature_weights_layer(incoming, NU = lasagne.init.Constant([0]))\n",
    "       incoming ~ should be an rf space tensor (T,D,V)\n",
    "             NU ~ (D, V) matrix of feature weights.\n",
    "    output      ~ (T, V) matrix of predicted responses.\n",
    "    '''    \n",
    "    def __init__(self, incoming, NU = lasagne.init.Constant([0]), **kwargs):\n",
    "        ##this will give us an \"input_shape\" attribute\n",
    "        super(feature_weights_layer, self).__init__(incoming, **kwargs)  \n",
    "        self.D = self.input_shape[1]\n",
    "        self.V = self.input_shape[-1]\n",
    "        self.feature_dim = 0\n",
    "        self.voxel_dim = 1\n",
    "        ##creates the theano shared variable\n",
    "        self.NU = self.add_param(NU, (self.D, self.V), name='feature_weights', feature_weights=True) \n",
    "    \n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return (input*self.NU[np.newaxis,:,:]).sum(axis=1)\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape): ##input_shape = (T, D, V)\n",
    "        return (input_shape[0], self.V)\n",
    "    \n",
    "    ##if NU = None, this will do nothing and return None\n",
    "    def set_weight_for_voxel(self, voxel_idx=None, NU=None):\n",
    "        if NU is not None:\n",
    "            NU_temp = self.NU.get_value()\n",
    "            NU_temp[:,voxel_idx] = NU\n",
    "            self.NU.set_value(NU_temp)\n",
    "            \n",
    "\n",
    "class prediction_menu_layer(lasagne.layers.Layer):\n",
    "    '''\n",
    "    feature_weights_layer(incoming, num_voxels, NU = lasagne.init.Constant([0]))\n",
    "         incoming ~ should be an rf space tensor (T,D,G), where G is the grid of potential receptive fields\n",
    "       num_voxels ~ number of voxels to make predictions for. num_voxels = V\n",
    "               NU ~ (D, G, V) matrix of feature weights. \n",
    "    outputs a (T, G, V) menu of predicted responses. V predictions for each of the G potential rf models.\n",
    "    '''    \n",
    "    def __init__(self, incoming, num_voxels, NU = lasagne.init.Constant([0]), **kwargs):\n",
    "        ##this will give us an \"input_shape\" attribute\n",
    "        super(prediction_menu_layer, self).__init__(incoming, **kwargs)  \n",
    "        self.D = self.input_shape[1]\n",
    "        self.G = self.input_shape[-1]\n",
    "        self.feature_dim = 0\n",
    "        self.voxel_dim = 2\n",
    "        self.V = num_voxels\n",
    "        ##creates the theano shared variable\n",
    "        self.NU = self.add_param(NU, (self.D, self.G, self.V), name='feature_weights', feature_weights=True) \n",
    "    \n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return (input[:,:,:,np.newaxis]*self.NU[np.newaxis,:,:, :]).sum(axis=1)\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape): ##should be (T, G, V)\n",
    "        return (input_shape[0], self.G, self.V)\n",
    "        \n",
    "    ##if NU = None, this will do nothing and return None\n",
    "    def set_weight_for_voxel(self, voxel_idx=None, NU=None):\n",
    "        if NU is not None:\n",
    "            NU_temp = self.NU.get_value()\n",
    "            NU_temp[:,:,voxel_idx] = NU\n",
    "            self.NU.set_value(NU_temp)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class batch_model_learner(object):\n",
    "    \n",
    "    def __init__(self,l_model, model_input_tnsr_dict, trn_data_generator, val_data_generator, num_voxels, epochs = 1, check_every=10, num_iters=10, learning_rate = 1.0,learn_these_params = None, voxel_dims = None, check_dims = True, print_stuff=False):\n",
    "\n",
    "        '''\n",
    "        batch_model_learner(l_model,\n",
    "                            model_input_tnsr_dict,\n",
    "                            trn_data_generator,\n",
    "                            val_data_generator,\n",
    "                            num_voxels,\n",
    "                            epochs=1,\n",
    "                            check_every=10,\n",
    "                            num_iters=10,\n",
    "                            learning_rate = 1.0,\n",
    "                            learn_these_params = None,\n",
    "                            voxel_dims = None\n",
    "                            check_dims = True)\n",
    "\n",
    "        a class for doing cross-validated (stochastic) gradient descent on params of many independent encoding models at the same time.\n",
    "        After each gradient step, checks each model independently, and updates it if gradient step reduces error on validation set.\n",
    "\n",
    "        inputs:\n",
    "                            l_model ~ a lasagne model with output shape (T,V), where T = #trials, V=#voxels\n",
    "            model_input_tensor_dict ~ dict of theano tensors that are input to the l_model.\n",
    "                 trn_data_generator ~ a generator of (input, output) training data batches. this is function that you have write.\n",
    "                                      it should yield batches of data, i.e., trn_data_generator() will give you your training batches.\n",
    "                                      the format for each batch is (input, output), where\n",
    "                                      input = training input data dictionary. each key/value matches the names/dimensions of corresponding tensor in model_input_tensor_dict\n",
    "                                      output = np.array of shape (Ttrn, V)\n",
    "                 val_data_generator ~ generator for validation data\n",
    "                             epochs ~ number of times through all data in the generator\n",
    "                        check_every ~ int. how often validation loss is checked. default = 10\n",
    "                          num_iters ~ number of gradient steps per batch before stopping. default = 100\n",
    "                      learning_rate ~ size of gradient step. default = 1\n",
    "                 learn_these_params ~ list of parameters names to learn, in case you don't want to train them all.\n",
    "                                      default = None, meaning train all trainable params\n",
    "                        voxel_dims  ~ dictionary, keys=param names, values = ints.\n",
    "                                      for each learned parameter, this corresponds to the voxel dimension.\n",
    "                                      default = None, in which case we try to figure out what the dimension is.\n",
    "                                      if we can't, we complain, and you are forced to specify.\n",
    "                         check_dims ~ if True, run potentially slow sanity check on dimensions of inputs\n",
    "        outputs:\n",
    "                     l_model ~ original l_model with params all trained up. this is a convenience, as params are learned in-place\n",
    "              final_val_loss ~ the final validation loss for each of the models\n",
    "                 trn_history ~ array of length num_iters showing number of voxels with decreased training loss at each time step\n",
    "                 val_history ~ array of length num_iters showing number of voxels with decreased validation loss at each time step\n",
    "\n",
    "        notes:\n",
    "            we call each independent model a \"voxel\". but it could be anything. \n",
    "\n",
    "            the data_generator approach is very general/flexible. in the case where you can simply load up the whole training/validation\n",
    "            data sets at once, it is just a slight encumberance, forcing you to define trn_data_generator() = yield trn_data.\n",
    "            in many cases though, it will not be possible to load all the data at once, so this approach will be helpful.\n",
    "\n",
    "            if trn_data_generator() contains only one large batch, epochs=1, and num_iters = BIG, we get standard grad. descent.\n",
    "            if trn_data_generator() contains many mini-batches, epochs > 1, num_iters = 1, we get *stochastic* grad. descent.\n",
    "        '''\n",
    "        \n",
    "        ##record all the inputs\n",
    "        self.l_model = l_model\n",
    "        self.learn_these_params = learn_these_params\n",
    "        self.model_input_tnsr_dict = model_input_tnsr_dict\n",
    "        self.trn_data_generator = trn_data_generator\n",
    "        self.val_data_generator = val_data_generator\n",
    "        self.num_voxels = num_voxels\n",
    "        self.epochs = epochs\n",
    "        self.check_every=check_every\n",
    "        self.num_iters = num_iters\n",
    "        self.learn_these_params = learn_these_params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.print_stuff = print_stuff\n",
    "        self.voxel_dims = voxel_dims\n",
    "        self.check_dims = check_dims\n",
    "\n",
    "        ##inspect one batch of input to sanity check dimensions\n",
    "        if self.check_dims:\n",
    "            self.grab_example_batch()\n",
    "            ##check data dimensions\n",
    "            self.check_consistent()\n",
    "    \n",
    "        ##get learned params: stores as a list of shared variables just like output of lasage.layers.get_all_params\n",
    "        self.get_learned_params()\n",
    "\n",
    "\n",
    "        ##try to determine which dimension of each parameter is the voxel dimension\n",
    "        self.find_batch_dimension()\n",
    "\n",
    "        ##construct a gradient update and loss functions to be called iteratively by the learn method\n",
    "        self.construct_training_kernel()\n",
    "    \n",
    "    def grab_example_batch( self ):\n",
    "        self.trn_in, self.trn_out = next(self.trn_data_generator()) \n",
    "        self.val_in, self.val_out = next(self.val_data_generator())\n",
    "\n",
    "\n",
    "    def check_consistent( self ):\n",
    "        ##read first batches to get some dimensions. assumes first dimension of input is time, last dimension is voxel \n",
    "        trn_batch_size, num_trn_voxels = self.trn_out.shape[0],self.trn_out.shape[-1]\n",
    "        val_batch_size, num_val_voxels = self.val_out.shape[0],self.val_out.shape[-1]   \n",
    "        assert num_trn_voxels == num_val_voxels, \"number of trn/val voxels don't match\"\n",
    "        assert num_trn_voxels == self.num_voxels,\"number voxels in trn/val does not match specific number of voxels\"\n",
    "        \n",
    "        \n",
    "        ##check to make sure that all feature map stacks in the input dictionary have the same number of timepoints.\n",
    "        ##this is not a complete check, since it's just the first batch, but if shit is not fucked up for this batch, \n",
    "        ##shit will hopefully not be fucked up for subsequent batches\n",
    "        for k,v in self.trn_in.iteritems():\n",
    "            assert v.shape[0] == trn_batch_size, \"number of input/output trn trials don't match for feature %s\" %(k)\n",
    "\n",
    "        for k,v in self.val_in.iteritems():\n",
    "            assert v.shape[0] == val_batch_size, \"number of input/output val trials don't match for feature %s\" %(k)\n",
    "\n",
    "\n",
    "    def get_learned_params( self ):\n",
    "        if self.learn_these_params is not None:            \n",
    "            self.params = [v for v in get_named_params(self.l_model, *self.learn_these_params).values()] ##<<unpack the dict.\n",
    "        else:\n",
    "            self.params = lasagne.layers.get_all_params(self.l_model,trainable=True)\n",
    "        print 'will solve for: %s' %(self.params)\n",
    "\n",
    "    def find_batch_dimension( self ):\n",
    "        if self.voxel_dims is None:\n",
    "            self.voxel_dims = {}\n",
    "            for p in self.params:\n",
    "                ##the voxel dimension should be the one that matches \"num_voxels\"\n",
    "                vdim = [ii for ii,pdim in enumerate(p.shape.eval()) if pdim==self.num_voxels]\n",
    "\n",
    "                ##if we happen to have multiple dimensions that = \"num_voxels\", user must disambiguate\n",
    "                assert len(vdim)==1, \"sorry, we can't determine voxel dimension for param %s. please supply explicit 'voxel_dims' argument\" %p.name\n",
    "                self.voxel_dims[p.name] = vdim[0]\n",
    "\n",
    "                \n",
    "    ##construct a gradient update and loss functions to be called iteratively by the learn method\n",
    "    def construct_training_kernel( self ):\n",
    "        voxel_data_tnsr = tnsr.matrix('voxel_data_tnsr')  ##voxel data tensor: (T x V)\n",
    "\n",
    "        ##get symbolic prediction expression\n",
    "        pred_expr = lasagne.layers.get_output(self.l_model)  ##voxel prediction tensor: (T x V)\n",
    "\n",
    "        ##generate symbolic loss expression\n",
    "        trn_diff = voxel_data_tnsr-pred_expr        ##difference tensor: shape = (T, V)\n",
    "        loss_expr = (trn_diff*trn_diff).sum(axis=0) ##sum squared diffs over time: shape = (V,)\n",
    "\n",
    "        ##for *training* error we compute of errors along voxel dimension.\n",
    "        ##we have to do this because auto-diff requires. a scalar loss function.\n",
    "        ##BUT: this is fine because gradient w.r.t. one voxel's weights is not affected by loss for any other voxel.\n",
    "        trn_loss_expr = loss_expr.sum()\n",
    "\n",
    "        #construct update rule using *training* loss.\n",
    "        fwrf_update = lasagne.updates.sgd(trn_loss_expr,self.params,learning_rate=self.learning_rate)\n",
    "        self.trn_kernel = function([voxel_data_tnsr]+self.model_input_tnsr_dict.values(), trn_loss_expr, updates=fwrf_update)           \n",
    "        print 'will update wrt: %s' % (self.params,)\n",
    "        \n",
    "        ##compile loss and training functions\n",
    "        ##NOTE: this is *validation* loss, not summed over voxels, so it has len = num_voxels\n",
    "        print 'compiling...'\n",
    "        self.loss = function([voxel_data_tnsr]+self.model_input_tnsr_dict.values(), loss_expr)\n",
    "                \n",
    "    \n",
    "    ##if the last gradient step made things better for some voxels, update their parameters\n",
    "    def update_best_param_values(self, best_param_values, improved_voxels):\n",
    "        for ii,p in enumerate(self.params):\n",
    "            vdim = self.voxel_dims[p.name] ##the voxel dimension\n",
    "            s = [slice(None),]*p.ndim      ##create a slicing object with right number of dims\n",
    "            s[vdim] = improved_voxels      ##assign improved voxel indices to the correct dim of the slice object\n",
    "            best_param_values[ii][s] = np.copy(p.get_value()[s])   ##keep a record of the best params.\n",
    "        return best_param_values        \n",
    "    \n",
    "    ##iteratively perform gradient descent\n",
    "    def learn(self):\n",
    "        \n",
    "        ##initialize best parameters to whatever they are\n",
    "        best_param_values = [np.copy(p.get_value()) for p in self.params]\n",
    "        \n",
    "        ##initalize validation loss to whatever you get from initial weigths\n",
    "        val_loss_was = 0\n",
    "        for val_in, val_out in self.val_data_generator():\n",
    "            val_loss_was += self.loss(val_out, *val_in.values())\n",
    "\n",
    "        ##initialize train loss to whatever, we only report the difference    \n",
    "        trn_loss_was = 0.0 ##we keep track of total across voxels as sanity check, since it *must* decrease\n",
    "  \n",
    "        val_history = []\n",
    "        trn_history = []\n",
    "        \n",
    "        ##descend and validate\n",
    "        epoch_count = 0\n",
    "        while epoch_count < self.epochs:\n",
    "            print '=======epoch: %d' %(epoch_count) \n",
    "            for trn_in, trn_out in self.trn_data_generator():\n",
    "                step_count = 0\n",
    "                while step_count < self.num_iters:\n",
    "                    \n",
    "                    ##update params, output training loss\n",
    "                    trn_loss_is = self.trn_kernel(trn_out, *trn_in.values())                    \n",
    "                    if step_count % self.check_every == 0:\n",
    "\n",
    "                        ##check for improvements\n",
    "                        val_loss_is = 0\n",
    "                        for val_in, val_out in self.val_data_generator():\n",
    "                            val_loss_is += self.loss(val_out, *val_in.values())\n",
    "                        improved = (val_loss_is < val_loss_was)\n",
    "\n",
    "                        ##update val loss\n",
    "                        val_loss_was[improved] = val_loss_is[improved]\n",
    "\n",
    "                        ##replace old params with better params\n",
    "                        best_param_values = self.update_best_param_values(best_param_values, improved)\n",
    "\n",
    "                        ##report on loss history\n",
    "                        val_history.append(improved.sum())\n",
    "                        trn_history.append(trn_loss_is)\n",
    "                        if self.print_stuff:\n",
    "                            print '====iter: %d' %(step_count)\n",
    "                            print 'number of improved models: %d' %(val_history[-1])\n",
    "                            print 'trn error: %0.6f' %(trn_history[-1])\n",
    "                    \n",
    "                    step_count += 1\n",
    "\n",
    "            epoch_count += 1\n",
    "\n",
    "        ##restore best values of learned params\n",
    "        set_named_model_params(self.l_model, **{k.name:v for k,v in zip(self.params, best_param_values)})\n",
    "       \n",
    "\n",
    "        return self.l_model, val_loss_was, val_history, trn_history\n",
    "\n",
    "class batch_model_learner_multi( batch_model_learner ):\n",
    "   \n",
    "    '''\n",
    "    batch_model_learner_multi(l_model, model_input_tnsr_dict,trn_data_gen, val_data_gen, num_voxels, num_models)\n",
    "    \n",
    "    learn multiple models for a batch of voxels. for each voxel, we have multiple possible models\n",
    "    to choose from. we use sgd to optimize parameters for each possible model. some methods for selecting\n",
    "    the best model are added here.\n",
    "    \n",
    "    T = number of time points\n",
    "    G = number of possible models\n",
    "    V = number of voxels.\n",
    "    \n",
    "    lasagne_model is assumed to have an output of dimensions (T, G, V)\n",
    "    \n",
    "    loss ~ (G,V)\n",
    "    \n",
    "    assumes extra argument \"num_models\"\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        model_dim_arg_dx = 5 ##we just happen to know this. shit will get fucked up if we get this wrong.\n",
    "        args = list(args)\n",
    "        self.num_models = args.pop(model_dim_arg_dx)\n",
    "        args = tuple(args)\n",
    "        self.model_dims = kwargs.pop('model_dims', None)\n",
    "        super(batch_model_learner_multi, self).__init__(*args,**kwargs)\n",
    " \n",
    "        \n",
    "        self.find_model_dimension()\n",
    "    \n",
    "    ##overwrite the training kernel:\n",
    "    ##construct a gradient update and loss functions to be called iteratively by the learn method\n",
    "    ##output loss is (G,V)\n",
    "    def construct_training_kernel( self ):\n",
    "        voxel_data_tnsr = tnsr.matrix('voxel_data_tnsr')  ##voxel data tensor: (T, V)\n",
    "\n",
    "        ##get symbolic prediction expression\n",
    "        pred_expr = lasagne.layers.get_output(self.l_model)  ##voxel prediction tensor: (T,G,V)\n",
    "\n",
    "        ##generate symbolic loss expression\n",
    "        trn_diff = voxel_data_tnsr[:,np.newaxis,:]-pred_expr ##difference tensor: shape = (T, V)\n",
    "        loss_expr = (trn_diff*trn_diff).sum(axis=0)          ##sum squared diffs over time: shape = (G,V)\n",
    "\n",
    "        ##for *training* error we compute sum of errors along G and V dimensions.\n",
    "        ##we have to do this because auto-diff requires. a scalar loss function.\n",
    "        ##BUT: this is fine because gradient w.r.t. one voxel's weights is not affected by loss for any other voxel.\n",
    "        trn_loss_expr = loss_expr.sum()\n",
    "\n",
    "        #construct update rule using *training* loss.\n",
    "        fwrf_update = lasagne.updates.sgd(trn_loss_expr,self.params,learning_rate=self.learning_rate)\n",
    "        self.trn_kernel = function([voxel_data_tnsr]+self.model_input_tnsr_dict.values(), trn_loss_expr, updates=fwrf_update)           \n",
    "        print 'will update wrt: %s' % (self.params,)\n",
    "        \n",
    "        ##compile loss and training functions\n",
    "        ##NOTE: this is *validation* loss, not summed over voxels, so it has len = num_voxels\n",
    "        print 'compiling...'\n",
    "        self.loss = function([voxel_data_tnsr]+self.model_input_tnsr_dict.values(), loss_expr)\n",
    "\n",
    "    ##get the dimension for the possible models (the \"G\" dimension) \n",
    "    def find_model_dimension( self ):\n",
    "        if self.model_dims is None:\n",
    "            self.model_dims = {}\n",
    "            for p in self.params:\n",
    "                ##the model dimension should be the one that matches \"num_models\"\n",
    "                mdim = [ii for ii,pdim in enumerate(p.shape.eval()) if pdim==self.num_models]\n",
    "                \n",
    "                ##if we happen to have multiple dimensions that = \"num_models\", user must disambiguate\n",
    "                assert len(mdim)==1, \"can't determine model dimension for param %s. supply explicit 'model_dims' argument\" %p.name\n",
    "                self.model_dims[p.name] = mdim[0]\n",
    "       \n",
    "    \n",
    "    ##overwrite to deal with 2D nature of 'improved_voxels'\n",
    "    ##if the last gradient step made things better for some voxels, update their parameters\n",
    "    def update_best_param_values(self, best_param_values, improved_voxels):\n",
    "        '''\n",
    "        update_best_param_values(self, best_param_values, improved_voxels)\n",
    "        improved_voxels ~ (G,V)\n",
    "        \n",
    "        '''\n",
    "        improvement_tuples = np.where(improved_voxels)\n",
    "        for ii,p in enumerate(self.params):\n",
    "            vdim = self.voxel_dims[p.name] ##the voxel dimension\n",
    "            mdim = self.model_dims[p.name]\n",
    "            s = [slice(None),]*p.ndim      ##create a slicing object with right number of dims\n",
    "            s[mdim] = improvement_tuples[0]\n",
    "            s[vdim] = improvement_tuples[1]\n",
    "            best_param_values[ii][s] = np.copy(p.get_value()[s])   ##keep a record of the best params.\n",
    "        return best_param_values\n",
    "    \n",
    "    def get_min_loss(self, loss):\n",
    "        min_loss = np.zeros((self.num_voxels))\n",
    "        argmin_loss = np.zeros((self.num_voxels),dtype='int')\n",
    "        for ii in range(self.num_voxels):\n",
    "            min_loss[ii] = np.min(loss[:,ii])\n",
    "            argmin_loss[ii] = np.argmin(loss[:,ii])\n",
    "        return min_loss, argmin_loss\n",
    "    \n",
    "    \n",
    "    def select_best_model(self, loss):\n",
    "        '''\n",
    "        after learning, submit final val. loss to get a dict. of best model params.\n",
    "        best param arrays don't have the \"model\" dimension\n",
    "        '''\n",
    "        _,best_model_dx = self.get_min_loss(loss)\n",
    "        improvement_tuples = [tuple(best_model_dx), tuple(range(self.num_voxels))]\n",
    "        best_model_params = {}\n",
    "        for ii,p in enumerate(self.params):\n",
    "            vdim = self.voxel_dims[p.name] ##the voxel dimension\n",
    "            mdim = self.model_dims[p.name]\n",
    "            s = [slice(None),]*p.ndim      ##create a slicing object with right number of dims\n",
    "            s[mdim] = improvement_tuples[0]\n",
    "            s[vdim] = improvement_tuples[1]\n",
    "            best_model_params[p.name] = np.copy(p.get_value()[s])  ##select the best params.\n",
    "            assert self.num_voxels in best_model_params[p.name].shape, 'voxel dim. not right for best param %s' %p.name\n",
    "        return best_model_params\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##TODO: GENERALIZE SO THAT NOT COMMITTED TO GAUSSIAN RF MODEL\n",
    "##merges multiple feature map stacks, applies receptive field layer, and whatever else you want.\n",
    "class rf_model_space(object):\n",
    "    '''  \n",
    "       \n",
    "    rf_model_space( feature_map_dict, deg_per_stim, num_voxels, rf_init={x0:array, y0:array, sig:array})\n",
    "    \n",
    "    The foundation for constructing a fwrf model. Creates receptive fields for a set of num_voxel voxels.\n",
    "    \n",
    "    inputs:\n",
    "    feature_map_dict ~ dictionary of (T,D_i,S_i,S_i) feature maps. T can be 1. These values are not stored, they\n",
    "                       are just used get basic name/dimension information for each kind of feature map.\n",
    "        deg_per_stim ~ for the rf_layer, so that we can interpret things if deg. of visual angle\n",
    "          num_voxels ~ the number of voxels\n",
    "             rf_init ~ optional initial receptive field params for each voxels. \n",
    "                       must be a dictionary like {x0:x0_array, y0:y0_array, sig:sig_array}, where len(??_array)=num_voxels\n",
    "    \n",
    "    attributes:\n",
    "        feature_depth\n",
    "        feature_indices\n",
    "        feature_resolutions\n",
    "        D\n",
    "        deg_per_stim\n",
    "        num_voxels\n",
    "        input_var_dict\n",
    "        rf_layer ~ lasagne layer. good for accessing rf parameters\n",
    "        rf_model_space ~ lasagne layer\n",
    "        construct_model_space_tensor(feature_map_dict) ~ method\n",
    "        normalize(calibration_data_generator, epsilon=10e-6, unstable=10e-10) ~ method\n",
    "    \n",
    "    \n",
    "    merges multiple feature map stacks, applies receptive field layer, creates a dictionary of input tensors\n",
    "    corresponding to each feature map stack, and creates an \"rf_layer\" attribute for convenient access to rf\n",
    "    parameters.\n",
    "    \n",
    "    normalization: call it once using training data, it computes the -mean/stdev, adds a normalization layer,\n",
    "                   and then sets the weights.\n",
    "    \n",
    "    does not apply the feature_weights layer. too many different branches after rf application for that to be useful.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, feature_map_dict, deg_per_stim, num_voxels, rf_init=None):\n",
    "        \n",
    "        ##record and store basic properties of the feature maps\n",
    "        self.feature_depth = {}\n",
    "        self.feature_indices = {}\n",
    "        self.feature_resolutions = {}\n",
    "        idx = 0\n",
    "        for f_key in feature_map_dict.keys():\n",
    "            self.feature_depth[f_key] = feature_map_dict[f_key].shape[1]\n",
    "            self.feature_indices[f_key] = np.arange(idx,idx + self.feature_depth[f_key],step=1)\n",
    "            idx += self.feature_depth[f_key]\n",
    "            self.feature_resolutions[f_key] = feature_map_dict[f_key].shape[2]\n",
    "        \n",
    "        ##total feature depth\n",
    "        self.D = np.sum(self.feature_depth.values())\n",
    "        \n",
    "        ##rf properties\n",
    "        self.deg_per_stim = deg_per_stim\n",
    "        self.num_voxels = num_voxels\n",
    "        if rf_init is None:\n",
    "            rf_init = {}\n",
    "            rf_init['x0'] = np.zeros(num_voxels,dtype='float32')\n",
    "            rf_init['y0'] = np.zeros(num_voxels,dtype='float32')\n",
    "            rf_init['sig'] = np.ones(num_voxels,dtype='float32')\n",
    "        \n",
    "            \n",
    "    \n",
    "        ##construct model space : applies rf's to feature maps\n",
    "        '''\n",
    "        each dict value is a (T,D_i,S_i,S_i) stack of feature maps, where i indexes each stack of feature maps\n",
    "        each model space in the list will have output shape (T,D_i,V)\n",
    "        will concatenate along axis 1 (= D)\n",
    "        will share x0,y0,sig (the rf params) across feature spaces\n",
    "        '''\n",
    "        rf_space_list = []\n",
    "        self.input_var_dict = {}\n",
    "        for f_map_name in feature_map_dict.keys():\n",
    "            \n",
    "            ##get S_i x S_i\n",
    "            input_shape = (None,)+feature_map_dict[f_map_name].shape[1:]\n",
    "\n",
    "            self.input_var_dict[f_map_name] = tnsr.tensor4(f_map_name)\n",
    "\n",
    "            ##the rf_layer for the first feature map\n",
    "            l1 = lasagne.layers.InputLayer(input_shape, input_var = self.input_var_dict[f_map_name], name='input_'+f_map_name)\n",
    "            \n",
    "            ##the points in 2D space where rf will be evaluated \n",
    "            Xm,Ym = make_space(input_shape[-1])\n",
    "            \n",
    "            ##only one feature map stack or the first map stack:\n",
    "            if not rf_space_list:  \n",
    "                self.rf_layer = receptive_field_layer(l1,num_voxels,(Xm,Ym),deg_per_stim,x0=rf_init['x0'], y0=rf_init['y0'], sig=rf_init['sig'],name='rf_'+f_map_name)\n",
    "                rf_space_list.append(self.rf_layer)\n",
    "            else: ##rf centers/sizes will be shared\n",
    "                rf_space_list.append(receptive_field_layer(l1,num_voxels,(Xm,Ym),deg_per_stim,\n",
    "                                                              x0=self.rf_layer.x0,\n",
    "                                                              y0=self.rf_layer.y0,\n",
    "                                                              sig=self.rf_layer.sig,\n",
    "                                                              name='rf_'+f_map_name))\n",
    "    \n",
    "        self.rf_model_space = lasagne.layers.ConcatLayer(rf_space_list,axis=1, name='rf_model_space')\n",
    "        \n",
    "        self.construct_model_space_tensor = function(self.input_var_dict.values(), lasagne.layers.get_output(self.rf_model_space))\n",
    "    \n",
    "    def normalize(self, calibration_data_generator, epsilon=0., unstable=10e-10):\n",
    "        mst = []\n",
    "        for cal_map in calibration_data_generator():\n",
    "            mst.append(self.construct_model_space_tensor(*cal_map.values()))\n",
    "        mst = np.concatenate(mst,axis=0)\n",
    "        mn = np.mean(mst,axis=0)\n",
    "        stdev = np.std(mst, axis=0)+epsilon\n",
    "        stability_mask = (stdev > unstable).astype('float32')\n",
    "        self.rf_model_space = normalization_layer(self.rf_model_space, mean=mn, stdev=stdev, mask=stability_mask)\n",
    "        self.epsilon=epsilon\n",
    "        self.unstable=unstable\n",
    "        self.construct_model_space_tensor = function(self.input_var_dict.values(), lasagne.layers.get_output(self.rf_model_space))\n",
    "        \n",
    "\n",
    "##a fwrf model\n",
    "##TODO: A \"merge_with\" method to merge one fwrf with another so it will be easy to batch up voxels.\n",
    "\n",
    "class fwrf(rf_model_space):\n",
    "    \n",
    "    '''\n",
    "    fwrf(feature_map_dict, deg_per_stim, num_voxels, rf_init=None, bonus_layers=None)\n",
    "\n",
    "    Creates a fwrf model for multiple voxels.\n",
    "\n",
    "    inputs:\n",
    "    feature_map_dict ~ dictionary of (T,D_i,S_i,S_i) feature maps. T can be 1. These values are not stored, they\n",
    "                       are just used get basic name/dimension information for each kind of feature map.\n",
    "        deg_per_stim ~ for the rf_layer, so that we can interpret things if deg. of visual angle\n",
    "          num_voxels ~ the number of voxels\n",
    "             rf_init ~ optional initial receptive field params for each voxels. \n",
    "                       must be a dictionary like {x0:x0_array, y0:y0_array, sig:sig_array}, where len(??_array)=num_voxels\n",
    "        bonus_layers ~ a callable stack of layers that accepts an incoming layer. if supplied, the rf_model_space\n",
    "                       lasagne model is treated as incoming to the bonus layer. a feature_weights_layer is then applied.\n",
    "                       if not supplied, a feature_weights_layer is added directly to rf_model_space.\n",
    "    \n",
    "    attributes:\n",
    "    predicted_activity(*feature_map_dict.values())\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.bonus_layers = kwargs.pop('bonus_layers', None)\n",
    "        NU = kwargs.pop('NU', lasagne.init.Constant([0]))\n",
    " \n",
    "        super(fwrf, self).__init__(*args, **kwargs)  ##this creates the rf_model_space\n",
    "\n",
    "        if self.bonus_layers is not None:\n",
    "            self.stack_feature_layers = lambda x: feature_weights_layer(self.bonus_layers(x),NU=NU)\n",
    "        else:\n",
    "            self.stack_feature_layers = lambda x: feature_weights_layer(x,NU=NU)\n",
    "            \n",
    "        self.fwrf = self.stack_feature_layers(self.rf_model_space)\n",
    "        \n",
    "        self.pred_expr = lasagne.layers.get_output(self.fwrf)\n",
    "        \n",
    "        ##this gives predicted output as (T,V) tensor\n",
    "        self.pred_func = function(self.input_var_dict.values(), self.pred_expr)\n",
    "    \n",
    "    def predict( self, feature_map_dict):\n",
    "        return self.pred_func(*feature_map_dict.values())\n",
    "\n",
    "    \n",
    "    def train_me(self, *args, **kwargs):\n",
    "        '''\n",
    "        you can call:\n",
    "        train_me(trn_data_generator, val_data_generator, fine=True, **training_keywords,)\n",
    "        or:\n",
    "        train_me(trn_data_generator, val_data_generator, coarse=True, rf_grid=required_dict_of_rf_params, rf_batch_size=optional, **training_keywords)\n",
    "        \n",
    "        note that trn/val_data_generator should be functions that return generataors of batches of data.\n",
    "        \n",
    "        so the first batch of training data would be like: first_trn_batch = trn_data_generator().next\n",
    "        first_trn_batch = (inp, outp)\n",
    "        inp = feature_map_dict\n",
    "        outp = matrix of voxel activities with shape (first_batch_size, num_voxels)\n",
    "        \n",
    "        optional training_keywords are (with their default values):\n",
    "            epochs = 1, \n",
    "            check_every=10,\n",
    "            num_iters=10,\n",
    "            learning_rate = 1.0,\n",
    "            learn_these_params = None,\n",
    "            voxel_dims = None,\n",
    "            model_dims = None,\n",
    "            check_dims = True,\n",
    "            print_stuff=False\n",
    "        \n",
    "        modifies the feature_weights and the rf parameters in place.\n",
    "        returns:\n",
    "        \n",
    "        fine:    (final_val_loss, improvement_history, trn_loss_history)\n",
    "        coarse: final_val_loss (sorry, batching the rf's makes it a pita to return full history)\n",
    "        '''\n",
    "        \n",
    "        coarse = kwargs.pop('coarse', False)\n",
    "        \n",
    "        if coarse:\n",
    "            assert kwargs.pop('fine', False) == False, 'you have to decide on coarse or fine fitting. fine is default'\n",
    "            assert 'rf_grid' in kwargs.keys(), 'coarse traning requires an rf_grid'\n",
    "            assert type(kwargs['rf_grid']) is dict, 'rf_grid should be a dictionary with keys x0,y0,sig' \n",
    "            return self._train_coarse(*args, **kwargs)\n",
    "                        \n",
    "        else:\n",
    "            assert 'rf_grid' not in kwargs.keys(), \"don't pass an rf grid if you are fine-tuning. you will just confuse yourself.\"\n",
    "            fine = kwargs.pop('fine',True)\n",
    "            return self._train_fine(*args,**kwargs)\n",
    "            \n",
    "    \n",
    "    def _train_fine(self, trn_data_generator, val_data_generator, **kwargs):\n",
    "        self.fwrf,val_loss,val_history,trn_history = batch_model_learner(self.fwrf, self.input_var_dict, trn_data_generator, val_data_generator, self.num_voxels, **kwargs).learn()\n",
    "        return val_loss, val_history, trn_history\n",
    "\n",
    "    def _train_coarse(self, trn_data_generator, val_data_generator, **kwargs):\n",
    "        \n",
    "        rf_grid = kwargs.pop('rf_grid','None')\n",
    "        assert rf_grid is not None, 'you somehow magically managed to call _train_coarse without passing rf_grid as keyword. are you a wizard?'\n",
    "        rf_batch_size = kwargs.pop('rf_batch_size',None)\n",
    "        consolidate = kwargs.pop('consolidate', False)\n",
    "        normalize = kwargs.pop('normalize', False)\n",
    "        \n",
    "  \n",
    "        if rf_batch_size is None:\n",
    "            rf_grid_batches = (rf_grid,)\n",
    "        else: ##break the rf_grid up into batches\n",
    "            rf_grid_batches = self._batch_rf_grid(rf_grid, rf_batch_size)  ##give us a generator\n",
    "\n",
    "        ##initialize containers to collect training results across batches of rf's\n",
    "        best_loss = []     ##collects 1 x V arrays\n",
    "        best_params = []   ##collects 1 x D x V arrays\n",
    "        best_rfs = []      ##collects 1 x 3 x V arrays, (x0,y0,sig)\n",
    "        \n",
    "        for rfb in rf_grid_batches:\n",
    "                  \n",
    "            ##a generator of model space tensors that iterates over T\n",
    "            trn_gen_multi,val_gen_multi = self._build_rf_model_space_tensor(trn_data_generator, val_data_generator, rfb, normalize=normalize, consolidate=consolidate)\n",
    "            assert type(trn_gen_multi()) is GeneratorType, 'you somehow managed to not produce a generator'\n",
    "            assert type(val_gen_multi()) is GeneratorType, 'you somehow managed to not produce a generator'\n",
    "            \n",
    "\n",
    "            ##a new network that treats everything up to the feature weights as input\n",
    "            ##NOTE: IT IS REALLY BAD TO BE REFERENCING A DICT. KEY HERE. SUPER NOT GENERAL!\n",
    "            G = len(rfb['x0']) ##grab the number of candidate rf models\n",
    "            proxy_net, proxy_input_var_dict = self._build_proxy_network((None,self.fwrf.D,G))\n",
    "\n",
    "            ##learn the best feature weights for the rf models in rfb\n",
    "            learn_best_rf = batch_model_learner_multi(proxy_net, proxy_input_var_dict, trn_gen_multi, val_gen_multi,self.num_voxels,G, **kwargs)\n",
    "            _,val_loss,val_history,trn_history = learn_best_rf.learn()\n",
    "\n",
    "            ##get the index of the best set of weights\n",
    "            cur_best_loss,best_dx = learn_best_rf.get_min_loss(val_loss)\n",
    "            best_loss.append(cur_best_loss)\n",
    "                        \n",
    "            ##get the best feature weights\n",
    "            best_params.append(learn_best_rf.select_best_model(val_loss)['feature_weights'])\n",
    "                       \n",
    "            ##expose the best rf params for each voxel\n",
    "            best_rfs.append(self._index_rf_grid_dict(rfb, best_dx))\n",
    "\n",
    "        best_loss = np.vstack(best_loss)  ##batch x voxels\n",
    "        best_params = np.stack(best_params, axis=0)   ##batch x D x voxels\n",
    "        best_rfs = np.stack(best_rfs, axis=0)         ##batch x 3 x voxels\n",
    "        \n",
    "        best_batches = tuple(np.argmin(best_loss, axis=0))\n",
    "        vox_range = tuple(range(self.num_voxels))\n",
    "        best_params = best_params[best_batches,:,vox_range].T  ##(D,voxels). transpose to put voxels in last dim.\n",
    "        best_rfs = best_rfs[best_batches,:,vox_range].T ##(3,voxels)\n",
    "#         best_loss = np.min(best_loss,axis=0)\n",
    "        \n",
    "        ##return stuff\n",
    "#         return best_params, best_rfs, best_loss\n",
    "         \n",
    "        ##assign this back to fwrf\n",
    "        ##BAD: THIS IS SICKENINGLY NOT GENERAL.\n",
    "        set_named_model_params(self.fwrf, feature_weights=best_params)\n",
    "        set_named_model_params(self.fwrf, x0=best_rfs[0])\n",
    "        set_named_model_params(self.fwrf, y0=best_rfs[1])\n",
    "        set_named_model_params(self.fwrf, sig=best_rfs[2])\n",
    "        \n",
    "        ##return training history\n",
    "        return best_loss\n",
    "        \n",
    "        \n",
    "    def _batch_rf_grid(self, rf_grid,rf_batch_size):\n",
    "        '''\n",
    "        returns a proper generator of batches of rf_grid\n",
    "        '''\n",
    "        x0 = rf_grid['x0']\n",
    "        y0 = rf_grid['y0']\n",
    "        sig = rf_grid['sig']\n",
    "        n = rf_batch_size\n",
    "        for ii in xrange(0, len(x0), rf_batch_size):\n",
    "            yield {'x0':x0[ii:ii+n],'y0':y0[ii:ii+n], 'sig':sig[ii:ii+n]}\n",
    "    \n",
    "    def _index_rf_grid_dict(self, rf_grid_dict, dx):\n",
    "        rf = np.zeros((3,len(dx)))\n",
    "        rf[0,:] = rf_grid_dict['x0'][dx]\n",
    "        rf[1,:] = rf_grid_dict['y0'][dx]\n",
    "        rf[2,:] = rf_grid_dict['sig'][dx]\n",
    "        return rf\n",
    "    \n",
    "    def _build_proxy_network(self, input_shape):\n",
    "        #check fwrf, get shape\n",
    "        #make a new network with an input layer and prediction_menu layer only\n",
    "        proxy_input_var_dict = {}\n",
    "        proxy_input_var_dict['mst'] = tnsr.tensor3('mst')\n",
    "        proxy_net = lasagne.layers.InputLayer((None,)+input_shape[1:], input_var = proxy_input_var_dict['mst'])\n",
    "        proxy_net = prediction_menu_layer(proxy_net, self.num_voxels)\n",
    "        return proxy_net, proxy_input_var_dict\n",
    "        \n",
    "    \n",
    "    def _build_rf_model_space_tensor(self, trn_data_gen, val_data_gen, rf_grid, normalize=False, consolidate=False):\n",
    "        '''\n",
    "        generating_function = _build_rf_model_space_tensor(trn_data_gen, val_data_gen, rf_grid, consolidate=False)\n",
    "        the goal is to convert a data generator of (feature_map_dict,voxel_activity) tuples\n",
    "        into a generator of (model_space_tensor, voxel_activity) tuples.\n",
    "        \n",
    "        the model_space_tensors will have shape (batch_size,D,G)\n",
    "        \n",
    "        returns  *functions* that, when called with no arguments, are each a generator of such tuples.\n",
    "        so\n",
    "        \n",
    "        (first_mst_batch, first_voxel_act_batch) = generating_function().next()\n",
    "        \n",
    "        note that, to comply with idea that network inputs should be dictionaries,\n",
    "        \n",
    "        first_mst_batch = {'mst':mst}, where mst is a model space tensoro with shape (batch_size,D,G)\n",
    "        \n",
    "        '''\n",
    "        example_inp,_ = trn_data_gen().next()\n",
    "        G = len(rf_grid['x0']) ##grab the number of candidate rf models\n",
    "        proxy_rf_model_space = rf_model_space(example_inp, self.deg_per_stim, G,rf_init=rf_grid)\n",
    "        if normalize:\n",
    "            ##need a new generator that only generates inputs\n",
    "            def only_input_trn_gen():\n",
    "                for inp,outp in trn_data_gen():\n",
    "                    yield inp\n",
    "            ##get normalization params       \n",
    "            proxy_rf_model_space.normalize(only_input_trn_gen)\n",
    "            \n",
    "        ##okay, this is ugly but,\n",
    "        if not consolidate:\n",
    "            ##generating function for trn data\n",
    "            def make_trn_gen_func():\n",
    "                for inp,outp in trn_data_gen():\n",
    "                    yield ({'mst':proxy_rf_model_space.construct_model_space_tensor(*inp.values())}, outp)\n",
    "            ##generating function for val data\n",
    "            def make_val_gen_func():\n",
    "                for inp,outp in val_data_gen():\n",
    "                    yield ({'mst':proxy_rf_model_space.construct_model_space_tensor(*inp.values())}, outp)\n",
    "                    \n",
    "        else:\n",
    "            ##we don't want batches, so we smoosh them all together\n",
    "            def make_trn_gen_func():\n",
    "                mst = []\n",
    "                r = []\n",
    "                for inp,outp in trn_data_gen():\n",
    "                    r.append(outp)\n",
    "                    mst.append(proxy_rf_model_space.construct_model_space_tensor(*inp.values()))\n",
    "                mst = np.concatenate(mst, axis=0)\n",
    "                r = np.concatenate(r, axis=0)\n",
    "                yield ({'mst':mst},r)\n",
    "\n",
    "            def make_val_gen_func():\n",
    "                mst = []\n",
    "                r = []\n",
    "                for inp,outp in val_data_gen():\n",
    "                    r.append(outp)\n",
    "                    mst.append(proxy_rf_model_space.construct_model_space_tensor(*inp.values()))\n",
    "                mst = np.concatenate(mst, axis=0)\n",
    "                r = np.concatenate(r, axis=0)\n",
    "                yield ({'mst':mst},r)\n",
    "\n",
    "        return make_trn_gen_func, make_val_gen_func\n",
    "    \n",
    "    def normalize(self, *args, **kwargs):\n",
    "        ##TODO: Should check to make sure no normalization yet!\n",
    "        \n",
    "        ##call the model_space method\n",
    "        super(fwrf, self).normalize(*args, **kwargs)\n",
    "        \n",
    "        ##then overwrite the feature layer\n",
    "        self.fwrf = self.stack_feature_layers(self.rf_model_space)\n",
    "        \n",
    "        ##and update the prediction expressions/functions\n",
    "        self.pred_expr = lasagne.layers.get_output(self.fwrf)\n",
    "        self.pred_func = function(self.input_var_dict.values(), self.pred_expr)\n",
    "        \n",
    "               \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deg_per_radius</th>\n",
       "      <th>x_deg</th>\n",
       "      <th>y_deg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-7.777778</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-5.555556</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.333333</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.111111</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   deg_per_radius      x_deg  y_deg\n",
       "0               1 -10.000000    -10\n",
       "1               1  -7.777778    -10\n",
       "2               1  -5.555556    -10\n",
       "3               1  -3.333333    -10\n",
       "4               1  -1.111111    -10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##params\n",
    "Ttrn,Tval,D,V,nmaps = 2003,301,17,4,8\n",
    "deg_per_stim = 20\n",
    "sf = 10\n",
    "\n",
    "##feature maps\n",
    "trn_feature_map_dict = {}\n",
    "for ii in range(1,nmaps+1):\n",
    "    input_name = 'fmap_%0.2d' % (ii)\n",
    "    trn_feature_map_dict[input_name] = np.random.random((Ttrn,D,sf*ii,sf*ii)).astype('float32')\n",
    "\n",
    "val_feature_map_dict = {}\n",
    "for ii in range(1,nmaps+1):\n",
    "    input_name = 'fmap_%0.2d' % (ii)\n",
    "    val_feature_map_dict[input_name] = np.random.random((Tval,D,sf*ii,sf*ii)).astype('float32')\n",
    "    \n",
    "    \n",
    "##true rfs\n",
    "bound = int((deg_per_stim-3)/2.)\n",
    "true_rf_params = {k:np.random.randint(-bound,high=bound, size=V).astype('float32') for k in ['x0','y0']}\n",
    "true_rf_params['sig'] = np.random.randint(1,high=bound,size=V).astype('float32')\n",
    "##true feature weights\n",
    "true_NU = np.random.random((D*nmaps,V)).astype('float32')\n",
    "\n",
    "##fwrf model\n",
    "true_model = fwrf(val_feature_map_dict,deg_per_stim,V,rf_init=true_rf_params, NU=true_NU)\n",
    "true_model.normalize(lambda: (yield trn_feature_map_dict))\n",
    "\n",
    "##true outputs, trn/val\n",
    "trn_voxel_activity = true_model.predict(trn_feature_map_dict).astype('float32')\n",
    "val_voxel_activity = true_model.predict(val_feature_map_dict).astype('float32')\n",
    "\n",
    "##data generator: note these are functions that *return* generators, so we can reboot the generator whenever.\n",
    "chunk_size = 100\n",
    "trn_data_gen = lambda: (({k:v[ii:ii+chunk_size,:,:,:] for k,v in trn_feature_map_dict.iteritems()}, trn_voxel_activity[ii:ii+chunk_size,:]) for ii in range(0,Ttrn,chunk_size))\n",
    "val_data_gen = lambda: (({k:v[ii:ii+chunk_size,:,:,:] for k,v in val_feature_map_dict.iteritems()}, val_voxel_activity[ii:ii+chunk_size,:]) for ii in range(0,Tval,chunk_size))       \n",
    "\n",
    "##rf grid for coarse training\n",
    "deg_per_radius = (1,deg_per_stim,12)\n",
    "spacing = 2\n",
    "rf_grid_df = make_rf_table(deg_per_stim,deg_per_radius,spacing,pix_per_stim = None)\n",
    "G = rf_grid_df.shape[0]\n",
    "rf_grid = {}\n",
    "rf_grid['x0'] = rf_grid_df.x_deg.values.astype('float32')\n",
    "rf_grid['y0'] = rf_grid_df.y_deg.values.astype('float32')\n",
    "rf_grid['sig'] = rf_grid_df.deg_per_radius.values.astype('float32')\n",
    "rf_grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f78f3399c50>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKdxFRBJI4S4ikkAKdxGRBFK4i4gk0P8DeT0xIRqDWqwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78f33bf310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(true_model.fwrf.NU.get_value().ravel(), true_NU.ravel(),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will solve for: [feature_weights]\n",
      "will update wrt: [feature_weights]\n",
      "compiling...\n",
      "=======epoch: 0\n",
      "====iter: 0\n",
      "number of improved models: 4190\n",
      "trn error: 461179232.000000\n",
      "=======epoch: 1\n",
      "====iter: 0\n",
      "number of improved models: 4187\n",
      "trn error: 461039296.000000\n",
      "=======epoch: 2\n",
      "====iter: 0\n",
      "number of improved models: 4182\n",
      "trn error: 460900576.000000\n",
      "=======epoch: 3\n",
      "====iter: 0\n",
      "number of improved models: 4183\n",
      "trn error: 460760448.000000\n",
      "=======epoch: 4\n",
      "====iter: 0\n",
      "number of improved models: 4179\n",
      "trn error: 460619360.000000\n",
      "=======epoch: 5\n",
      "====iter: 0\n",
      "number of improved models: 4174\n",
      "trn error: 460479584.000000\n",
      "=======epoch: 6\n",
      "====iter: 0\n",
      "number of improved models: 4179\n",
      "trn error: 460339712.000000\n",
      "=======epoch: 7\n",
      "====iter: 0\n",
      "number of improved models: 4175\n",
      "trn error: 460200256.000000\n",
      "=======epoch: 8\n",
      "====iter: 0\n",
      "number of improved models: 4171\n",
      "trn error: 460060800.000000\n",
      "=======epoch: 9\n",
      "====iter: 0\n",
      "number of improved models: 4172\n",
      "trn error: 459921216.000000\n"
     ]
    }
   ],
   "source": [
    "_=true_model.train_me(trn_data_gen, val_data_gen,coarse=True,rf_grid=rf_grid, learning_rate=10e-8,epochs=10, num_iters = 1,check_every=1,print_stuff=True,check_dims=False, normalize=True,consolidate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f78ae3fcb50>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQkgbQaVPCCFtBJU+IYS0EVT6hBDSRvx/A7N0LvjrLe4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78ae4573d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(true_model.fwrf.NU.get_value().ravel(), true_NU.ravel(),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will solve for: [rf_fmap_08.x0, rf_fmap_08.y0, rf_fmap_08.sig, feature_weights]\n",
      "will update wrt: [rf_fmap_08.x0, rf_fmap_08.y0, rf_fmap_08.sig, feature_weights]\n",
      "compiling...\n",
      "=======epoch: 0\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 15979.267578\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18067.097656\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 21679.121094\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20802.160156\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 17009.835938\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18831.109375\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19643.904297\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19635.968750\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18696.960938\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 17143.531250\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 16112.613281\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18979.517578\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19132.960938\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18346.816406\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 20646.437500\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20705.427734\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18186.609375\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20337.863281\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 21314.109375\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19417.878906\n",
      "====iter: 0\n",
      "number of improved models: 1\n",
      "trn error: 341.967957\n",
      "=======epoch: 1\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 15964.748047\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18052.492188\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 21660.853516\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20783.882812\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 16994.777344\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18814.949219\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19628.042969\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19619.292969\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18680.099609\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 17128.832031\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 16098.342773\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18963.390625\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19115.871094\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18330.832031\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 20627.513672\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20687.259766\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18171.298828\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20320.328125\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 21295.683594\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19401.097656\n",
      "====iter: 0\n",
      "number of improved models: 1\n",
      "trn error: 341.758911\n",
      "=======epoch: 2\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 15950.111328\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18038.140625\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 21642.646484\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20765.585938\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 16979.687500\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18798.851562\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19612.417969\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19602.640625\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 18663.179688\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 17114.152344\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 16083.989258\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18947.316406\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19098.726562\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18314.875000\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 20608.402344\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 20669.078125\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18156.126953\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20302.849609\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 21277.277344\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19384.320312\n",
      "====iter: 0\n",
      "number of improved models: 1\n",
      "trn error: 341.565063\n",
      "=======epoch: 3\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 15935.381836\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18024.011719\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 21624.500000\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20747.281250\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 16964.574219\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18782.816406\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19596.994141\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19586.011719\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 18646.218750\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 17099.492188\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 16069.573242\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18931.285156\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 19081.546875\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18298.945312\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 20589.152344\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 20650.902344\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18141.068359\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20285.421875\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 21258.890625\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19367.552734\n",
      "====iter: 0\n",
      "number of improved models: 1\n",
      "trn error: 341.382629\n",
      "=======epoch: 4\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 15920.593750\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18010.054688\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 21606.417969\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20728.976562\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 16949.451172\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18766.832031\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19581.712891\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19569.398438\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 18629.248047\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 17084.861328\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 16055.128906\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18915.304688\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 19064.359375\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18283.048828\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 20569.839844\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 20632.738281\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18126.105469\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20268.031250\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 21240.523438\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19350.804688\n",
      "====iter: 0\n",
      "number of improved models: 1\n",
      "trn error: 341.207275\n",
      "=======epoch: 5\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 15905.782227\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 17996.210938\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 21588.406250\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20710.683594\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 16934.330078\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18750.880859\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 19566.509766\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19552.794922\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 18612.304688\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 17070.255859\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 16040.695312\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18899.363281\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 19047.191406\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18267.179688\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 20550.527344\n",
      "====iter: 0\n",
      "number of improved models: 2\n",
      "trn error: 20614.609375\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 18111.207031\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20250.681641\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 21222.171875\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19334.082031\n",
      "====iter: 0\n",
      "number of improved models: 1\n",
      "trn error: 341.033264\n",
      "=======epoch: 6\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 15890.992188\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 17982.410156\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 21570.458984\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20692.419922\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 16919.226562\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18734.949219\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 19551.310547\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19536.195312\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 18595.410156\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 17055.683594\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 16026.304688\n",
      "====iter: 0\n",
      "number of improved models: 2\n",
      "trn error: 18883.464844\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 19030.070312\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 18251.337891\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 20531.304688\n",
      "====iter: 0\n",
      "number of improved models: 2\n",
      "trn error: 20596.523438\n",
      "====iter: 0\n",
      "number of improved models: 3\n",
      "trn error: 18096.328125\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 20233.355469\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 21203.847656\n",
      "====iter: 0\n",
      "number of improved models: 4\n",
      "trn error: 19317.392578\n",
      "====iter: 0\n",
      "number of improved models: 1\n",
      "trn error: 340.855713\n"
     ]
    }
   ],
   "source": [
    "_=true_model.train_me(trn_data_gen, val_data_gen, fine=True, learning_rate=10e-8,epochs=7, num_iters = 1,check_every=1,print_stuff=True,check_dims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f78ae448c50>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naSGo9AkhpIX4/7veiYMMdPBhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78abadccd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(true_model.fwrf.NU.get_value().ravel(), true_NU.ravel(),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00152183  4.00058699  7.00012922  4.00034666]\n",
      "[ 1.  4.  7.  4.]\n"
     ]
    }
   ],
   "source": [
    "print true_model.rf_layer.sig.get_value().astype('float32')\n",
    "print true_rf_params['sig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter hid_init has shape (1, 11)\n",
      "parameter input_to_hidden.W has shape (144, 11)\n",
      "parameter input_to_hidden.b has shape (11,)\n",
      "parameter hidden_to_hidden.W has shape (11, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestGetSetShape(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        ##build some random complicated layer\n",
    "        input_shape = (14, 13, 12, 12)\n",
    "        num_units = 11\n",
    "        self.test_layer = lasagne.layers.RecurrentLayer(input_shape,num_units)\n",
    "    \n",
    "    def test_param_set_get_shape(self):\n",
    "        params = lasagne.layers.get_all_params(self.test_layer)\n",
    "        for p in params:\n",
    "            val = get_named_params(self.test_layer, p.name)[p.name]\n",
    "            self.assertEqual(val.get_value().shape, get_named_param_shapes(self.test_layer, p.name)[p.name])\n",
    "            \n",
    "    def test_param_names(self):\n",
    "        pnames = get_model_params_names(self.test_layer)\n",
    "        self.assertEqual(pnames, [p.name for p in lasagne.layers.get_all_params(self.test_layer)])\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestGetSetShape )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )\n",
    "\n",
    "class TestRfLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.T = 900\n",
    "        self.D = 90\n",
    "        self.S = 9\n",
    "        self.input_shape = (self.T,self.D,self.S,self.S)\n",
    "        self.V = 10\n",
    "        self.deg_per_stim = 20\n",
    "        #this tensor stores feature map\n",
    "        self.f_map_0 = tnsr.tensor4('f_map_0',dtype='float32')\n",
    "        ##construct an input layer\n",
    "        self.input_layer = lasagne.layers.InputLayer(self.input_shape, input_var = self.f_map_0, name='input_layer')\n",
    "        ##create a real numpy input of all 1's\n",
    "        self.input_data = np.ones(self.input_shape,dtype='float32')\n",
    "        \n",
    "        try:\n",
    "            self.rf_layer = receptive_field_layer(self.input_layer, self.V, make_space(self.S), self.deg_per_stim, name='rf_layer')\n",
    "        except:\n",
    "            print 'could not init rf layer'\n",
    "    \n",
    "    ##rf stack should be V,S,S\n",
    "    def test_rf_layer_make_rf_stack(self):          \n",
    "        self.assertEqual(self.rf_layer.make_rf_stack().shape, (self.V,self.S,self.S))\n",
    "    \n",
    "    ##output should be T,D,V\n",
    "    def test_rf_layer_output_shape(self):\n",
    "        output_shape = self.rf_layer.get_output_shape_for(self.input_shape)\n",
    "        self.assertEqual(output_shape, (self.T,self.D,self.V))\n",
    "    \n",
    "    ##grid where rfs are evaluated\n",
    "    def test_rf_layer_spatial_grid(self):\n",
    "        self.assertEqual(self.rf_layer.Xm.get_value().shape, (self.S,self.S))\n",
    "        self.assertEqual(self.rf_layer.Ym.get_value().shape, (self.S,self.S))\n",
    "    \n",
    "    ##should all be V\n",
    "    def test_rf_layer_param_shapes(self):\n",
    "        self.assertEqual(self.rf_layer.x0.get_value().shape, (self.V,))\n",
    "        self.assertEqual(self.rf_layer.y0.get_value().shape, (self.V,))\n",
    "        self.assertEqual(self.rf_layer.sig.get_value().shape, (self.V,))\n",
    "    \n",
    "    ##should be T,D,V\n",
    "    def test_rf_layer_output_function_shape(self):\n",
    "        rf_layer_expr = lasagne.layers.get_output(self.rf_layer)\n",
    "        rf_layer_func = function([self.input_layer.input_var], rf_layer_expr)\n",
    "        self.assertEqual(rf_layer_func(self.input_data).shape,(self.T,self.D,self.V))\n",
    "        \n",
    "    ##should go from 0 to 10\n",
    "    def test_set_value_for_voxel(self):\n",
    "        self.rf_layer.set_value_for_voxel(voxel_idx=-1,x0=10,y0=10,sig=10)\n",
    "        self.assertEqual(self.rf_layer.x0.get_value()[-1],10)  ## this is same as next line\n",
    "        self.assertEqual(get_named_params(self.rf_layer,self.rf_layer.x0.name)[self.rf_layer.x0.name].get_value()[-1],10)\n",
    "        self.assertEqual(self.rf_layer.y0.get_value()[-1],10)\n",
    "        self.assertEqual(self.rf_layer.sig.get_value()[-1],10)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestRfLayer )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )        \n",
    "\n",
    "class TestCompressiveNonlinearityLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.T = 1001\n",
    "        self.D = 101\n",
    "        self.V = 11\n",
    "        self.input_shape = (self.T,self.D,self.V)\n",
    "        self.test_layer = compressive_nonlinearity_layer(self.input_shape)\n",
    "    def test_output_shape(self):\n",
    "        self.assertEqual(self.input_shape, self.test_layer.get_output_shape_for(self.input_shape))\n",
    " \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestCompressiveNonlinearityLayer )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )   \n",
    "\n",
    "class TestNormalizationLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.T = 1001\n",
    "        self.D = 4\n",
    "        self.V = 2\n",
    "        self.x = tnsr.tensor3('x')\n",
    "        self.input_shape = (self.T,self.D,self.V)\n",
    "        self.input_layer = lasagne.layers.InputLayer(self.input_shape, input_var=self.x, name='input_layer')\n",
    "        self.test_input = np.random.random(size=self.input_shape).astype('float32')\n",
    "        self.test_input[:,0,0] = 1.\n",
    "        self.mean = np.mean(self.test_input, axis=0)\n",
    "        self.std = np.std(self.test_input, axis=0)\n",
    "        self.mask = np.ones((self.D,self.V))\n",
    "        self.mask[0,0] = 0\n",
    "        self.test_layer = normalization_layer(self.input_layer, mean=self.mean, stdev = self.std, mask = self.mask)\n",
    "        \n",
    "    def test_output_shape(self):\n",
    "        self.assertEqual(self.input_shape, self.test_layer.get_output_shape_for(self.input_shape))\n",
    "    \n",
    "    def test_out_values(self):\n",
    "        output_func = function([self.x], [lasagne.layers.get_output(self.test_layer)])\n",
    "        y = output_func(self.test_input)[0]\n",
    "        ##shape is right\n",
    "        self.assertEqual(self.input_shape, y.shape)\n",
    "        ##degenerate entry set to 0\n",
    "        self.assertTrue(np.all(y[:,0,0]==0))\n",
    "        ##mean is 0\n",
    "        np.testing.assert_almost_equal(np.mean(y,axis=0), 0, decimal=5)\n",
    "        ##stdev is 1 for all but degnerate entry\n",
    "        np.testing.assert_almost_equal(np.std(y,axis=0).ravel()[1:], 1, decimal=6)\n",
    "\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestNormalizationLayer )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )           \n",
    "\n",
    "class TestFeatureWeightsLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        T,D,V = 1001,4,20\n",
    "        self.input_shape = (T,D,V)\n",
    "        self.x = tnsr.tensor3('x')\n",
    "        self.input_layer = lasagne.layers.InputLayer(self.input_shape, input_var=self.x, name='input_layer')\n",
    "        self.test_input = np.random.random(size=self.input_shape).astype('float32')\n",
    "        self.test_layer = feature_weights_layer(self.input_layer)\n",
    "        self.layer_func = function([self.x], lasagne.layers.get_output(self.test_layer))\n",
    "        self.y = self.layer_func(self.test_input)\n",
    "    \n",
    "    def test_output(self):\n",
    "        self.assertEqual((self.input_shape[0], self.input_shape[-1]), self.test_layer.get_output_shape_for(self.input_shape))\n",
    "        self.assertEqual((self.input_shape[0], self.input_shape[-1]), self.y.shape)\n",
    "        \n",
    "    def test_set_weight_for_voxel(self):\n",
    "        self.assertIsNone(self.test_layer.set_weight_for_voxel(NU = None))\n",
    "        voxel_idx = [0,1,-2,-1]\n",
    "        NU = [1,1] ##this raise value error because of mismatch\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.test_layer.set_weight_for_voxel(voxel_idx = voxel_idx, NU = NU)     \n",
    "        NU = np.random.random((self.input_shape[1], len(voxel_idx))).astype('float32')\n",
    "        self.test_layer.set_weight_for_voxel(voxel_idx = voxel_idx, NU=NU)\n",
    "        np.testing.assert_array_equal(self.test_layer.NU.get_value()[:,voxel_idx], NU)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestFeatureWeightsLayer )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )         \n",
    "\n",
    "class TestPredictionMenuLayer( unittest.TestCase ):\n",
    "    def setUp(self):\n",
    "        T,D,G,V = 100, 11, 5, 3\n",
    "        x = tnsr.tensor3('x')\n",
    "        test_input = np.random.random((T,D,G)).astype('float32')\n",
    "        test_layer = lasagne.layers.InputLayer((T,D,G), input_var=x)\n",
    "        test_layer = prediction_menu_layer(test_layer,V)\n",
    "        test_layer_expr = lasagne.layers.get_output(test_layer) \n",
    "        test_layer_func = function([x], test_layer_expr)\n",
    "        \n",
    "        ##record\n",
    "        self.T,self.D,self.G,self.V = T,D,G,V\n",
    "        self.test_layer = test_layer\n",
    "        self.test_input = test_input\n",
    "        self.test_layer_func = test_layer_func\n",
    "    \n",
    "    def test_output_shape(self):\n",
    "        self.assertEqual((self.T,self.G,self.V), self.test_layer.get_output_shape_for(self.test_input.shape))\n",
    "        self.assertEqual((self.T,self.G,self.V), self.test_layer_func(self.test_input).shape)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestPredictionMenuLayer )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )         \n",
    "\n",
    "class TestBatchModelLearner(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        Ttrn,Tval,D,S,V = 5000,200,5,2,3\n",
    "        self.V = V\n",
    "        self.Ttrn = Ttrn\n",
    "        self.Tval = Tval\n",
    "        model_input_tnsr_dict = {}\n",
    "        model_input_tnsr_dict['fmap0'] = tnsr.tensor4('fmap0')\n",
    "        input_layer = lasagne.layers.InputLayer((None,D,S,S), input_var = model_input_tnsr_dict['fmap0'])\n",
    "        \n",
    "        self.true_W = np.random.random((D*S*S,V)).astype('float32')\n",
    "        true_model = lasagne.layers.DenseLayer(input_layer,num_units = V, W=self.true_W)\n",
    "        out_func = function([model_input_tnsr_dict['fmap0']], lasagne.layers.get_output(true_model))\n",
    "        \n",
    "        ##create training/val data using true model\n",
    "        trn_in = {}\n",
    "        trn_in['fmap0'] = np.random.random((Ttrn,D,S,S)).astype('float32')\n",
    "        trn_out = out_func(trn_in['fmap0'])\n",
    "        trn_data_gen = lambda: (yield trn_in, trn_out)\n",
    "        val_in = {}\n",
    "        val_in['fmap0'] = np.random.random((Tval,D,S,S)).astype('float32')\n",
    "        val_out = out_func(val_in['fmap0'])\n",
    "        val_data_gen = lambda: (yield val_in,val_out)\n",
    "        \n",
    "        ##test basic dimensions, parameter assignment\n",
    "        self.learn_nothing = batch_model_learner(true_model, model_input_tnsr_dict,trn_data_gen, val_data_gen, self.V)\n",
    "        \n",
    "        ##test learning of \"b\": start at b=1, see if it will learn b = 0\n",
    "        test_b_model = lasagne.layers.DenseLayer(input_layer,num_units = V, W=self.true_W, b=np.ones((self.V,)).astype('float32'))\n",
    "        self.learn_b = batch_model_learner(test_b_model, model_input_tnsr_dict,trn_data_gen, val_data_gen, self.V, learn_these_params=['b'], check_every=100,learning_rate= 0.00001, num_iters = 400,print_stuff=False)\n",
    "\n",
    "        ##test learning of \"W\": start at W = some other random values\n",
    "        test_W_model = lasagne.layers.DenseLayer(input_layer,num_units = V, W=np.random.random((D*S*S,V)).astype('float32'))\n",
    "        self.learn_W = batch_model_learner(test_b_model, model_input_tnsr_dict,trn_data_gen, val_data_gen, self.V, learn_these_params=['W'], check_every=100,learning_rate= 10e-15, num_iters = 90000, print_stuff=False)\n",
    "        \n",
    "        ##test learning of \"W\" and \"b\"\n",
    "        test_both_model = lasagne.layers.DenseLayer(input_layer,num_units = V, W=np.random.random((D*S*S,V)).astype('float32'),b=np.ones((self.V,)).astype('float32'))\n",
    "        self.learn_both = batch_model_learner(test_both_model, model_input_tnsr_dict,trn_data_gen, val_data_gen, self.V, check_every=100,learning_rate= 0.00001, num_iters = 15000, print_stuff=False)\n",
    "    \n",
    "    def test_num_voxels(self):\n",
    "        self.assertEqual(self.learn_nothing.num_voxels, self.V)\n",
    "    \n",
    "    def test_voxel_dims(self):\n",
    "        self.assertDictEqual(self.learn_nothing.voxel_dims, {'W': 1, 'b':0})        \n",
    "        \n",
    "    def test_loss_func(self):\n",
    "        trn_in,trn_out = self.learn_nothing.trn_data_generator().next()\n",
    "        trn_loss = self.learn_nothing.loss(trn_out,*trn_in.values())\n",
    "        self.assertEqual(trn_out.shape, (self.Ttrn, self.V))\n",
    "        self.assertEqual(trn_loss.shape, (self.V,))\n",
    "        np.testing.assert_array_equal(trn_loss, np.zeros((self.learn_nothing.num_voxels,)))\n",
    "    \n",
    "    def test_learn_b(self):\n",
    "        new_model,val_loss,val_hist,trn_hist = self.learn_b.learn()\n",
    "        np.testing.assert_array_almost_equal(new_model.b.get_value(), np.zeros((self.V)), decimal=5)\n",
    "        \n",
    "    def test_learn_W(self):\n",
    "        new_model,val_loss,val_hist,trn_hist = self.learn_W.learn()\n",
    "        np.testing.assert_array_almost_equal(new_model.W.get_value(), self.true_W, decimal=5)\n",
    "    \n",
    "    def test_learn_both(self):\n",
    "        new_model,val_loss,val_hist,trn_hist = self.learn_both.learn()\n",
    "        np.testing.assert_array_almost_equal(new_model.b.get_value(), np.zeros((self.V)), decimal=4)\n",
    "        np.testing.assert_array_almost_equal(new_model.W.get_value(), self.true_W, decimal=4)\n",
    "        \n",
    "        \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestBatchModelLearner )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )                \n",
    "\n",
    "class TestBatchModelLearnerMulti(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        Ttrn,Tval,D,G,V = 5000,200,5,2,3\n",
    "        model_input_tnsr_dict = {}\n",
    "        model_input_tnsr_dict['fmap0'] = tnsr.tensor3('fmap0')\n",
    "        input_layer = lasagne.layers.InputLayer((None,D,G), input_var = model_input_tnsr_dict['fmap0'])\n",
    "        \n",
    "        true_NU = np.random.random((D,G,V)).astype('float32')\n",
    "        true_rf = np.random.randint(0,high=G,size=V)\n",
    "        best_model_matrix = [slice(None), tuple(true_rf), tuple(range(V))]\n",
    "        true_model = prediction_menu_layer(input_layer,V, NU=true_NU)\n",
    "        \n",
    "        \n",
    "        tmp_out_func = function([model_input_tnsr_dict['fmap0']], lasagne.layers.get_output(true_model))\n",
    "        out_func = lambda x: tmp_out_func(x)[best_model_matrix]\n",
    "\n",
    "\n",
    "        trn_in = {}\n",
    "        trn_in['fmap0'] = np.random.random((Ttrn,D,G)).astype('float32')\n",
    "        trn_out = out_func(trn_in['fmap0'])\n",
    "        trn_data_gen = lambda: (yield trn_in, trn_out)\n",
    "        val_in = {}\n",
    "        val_in['fmap0'] = np.random.random((Tval,D,G)).astype('float32')\n",
    "        val_out = out_func(val_in['fmap0'])\n",
    "        val_data_gen = lambda: (yield val_in,val_out)   \n",
    "        \n",
    "        ##test basic dimensions, parameter assignment\n",
    "        learn_nothing = batch_model_learner_multi(true_model, model_input_tnsr_dict,trn_data_gen, val_data_gen, V, G, model_dims=None)\n",
    "        val_loss = learn_nothing.loss(val_out,*val_in.values())        \n",
    "\n",
    "        \n",
    "        ##test learning of NU\n",
    "        test_model = prediction_menu_layer(input_layer,V, NU=np.random.random((D,G,V)).astype('float32'))\n",
    "        learn_something = batch_model_learner_multi(test_model, model_input_tnsr_dict,trn_data_gen, val_data_gen, V, G, check_every=100,learning_rate= 0.0000001, num_iters = 150000, print_stuff=False)\n",
    "        \n",
    " \n",
    "        ##record everything needed in self\n",
    "        self.V = V\n",
    "        self.Ttrn = Ttrn\n",
    "        self.Tval = Tval\n",
    "        self.G = G\n",
    "        self.input_layer = input_layer\n",
    "        self.true_NU = true_NU\n",
    "        self.true_rf = true_rf\n",
    "        self.learn_nothing = learn_nothing\n",
    "        self.learn_something = learn_something\n",
    "        self.val_in = val_in\n",
    "        self.val_out = val_out\n",
    " \n",
    "    \n",
    "    def test_num_voxels(self):\n",
    "        self.assertEqual(self.learn_nothing.num_voxels, self.V)\n",
    "    \n",
    "    def test_voxel_dims(self):\n",
    "        self.assertDictEqual(self.learn_nothing.voxel_dims, {'feature_weights': 2})   \n",
    "        \n",
    "    def test_data_generator(self):\n",
    "        val_in, val_out = self.learn_nothing.val_data_generator().next()\n",
    "        self.assertEqual(val_out.shape, (self.Tval, self.V))\n",
    "        \n",
    "    def test_loss_func_and_best_model_selection(self):\n",
    "        \n",
    "        val_loss = self.learn_nothing.loss(self.val_out,*self.val_in.values())        \n",
    "        self.assertEqual(val_loss.shape, (self.G, self.V,))\n",
    "        \n",
    "        min_val_loss,argmin_val_loss = self.learn_nothing.get_min_loss(val_loss)\n",
    "\n",
    "        \n",
    "        np.testing.assert_array_equal(min_val_loss, np.zeros(self.learn_nothing.num_voxels))\n",
    "        np.testing.assert_array_equal(argmin_val_loss, self.true_rf)\n",
    "        \n",
    "        best_model_params = self.learn_nothing.select_best_model(val_loss)\n",
    "        for vv in range(self.V):\n",
    "            np.testing.assert_array_equal(best_model_params['feature_weights'][:,vv], self.true_NU[:,self.true_rf[vv],vv])\n",
    "            \n",
    "    def test_learn_something(self):\n",
    "        new_model,val_loss,val_hist,trn_hist = self.learn_something.learn()\n",
    "        best_model_params = self.learn_something.select_best_model(val_loss)\n",
    "        for vv in range(self.V):\n",
    "            np.testing.assert_array_almost_equal(best_model_params['feature_weights'][:,vv], self.true_NU[:,self.true_rf[vv],vv], decimal=2)\n",
    "    \n",
    "        \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestBatchModelLearnerMulti )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )  \n",
    "\n",
    "class TestRfModelSpace( unittest.TestCase ):\n",
    "    \n",
    "    def setUp(self):\n",
    "        T,D,V = 20,13,3\n",
    "        self.V = V\n",
    "        self.nmaps = 10\n",
    "        self.D = D\n",
    "        self.T = T\n",
    "        self.deg_per_stim = 10\n",
    "        self.feature_map_dict = {}\n",
    "        ##create 10 feature maps where resolution happens to be name\n",
    "        for ii in range(1,self.nmaps+1):\n",
    "            input_name = 'fmap_%0.2d' % (ii)\n",
    "            self.feature_map_dict[input_name] = np.random.random((T,D,ii,ii)).astype('float32')\n",
    "        \n",
    "        self.rfms = rf_model_space(self.feature_map_dict, self.deg_per_stim, self.V)\n",
    "        \n",
    "        self.cal_data_generator = lambda: (yield self.feature_map_dict)\n",
    "    \n",
    "    def test_basic_attributes(self):\n",
    "        self.assertEqual(self.rfms.D, self.D*self.nmaps)\n",
    "    \n",
    "    def test_construct_model_space_tensor(self):\n",
    "        mst = self.rfms.construct_model_space_tensor(*self.feature_map_dict.values())\n",
    "        self.assertEqual(mst.shape, (self.T, self.rfms.D, self.V))\n",
    "        \n",
    "    def test_normalize(self):\n",
    "        self.rfms.normalize(self.cal_data_generator)\n",
    "        mst = self.rfms.construct_model_space_tensor(*self.feature_map_dict.values())\n",
    "        np.testing.assert_array_almost_equal(np.mean(mst,axis=0), np.zeros((self.rfms.D,self.V)), decimal=6)\n",
    "        np.testing.assert_array_almost_equal(np.std(mst,axis=0), np.ones((self.rfms.D,self.V)), decimal=6)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestRfModelSpace )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )                        \n",
    "\n",
    "class TestFwrf( unittest.TestCase ):\n",
    "    \n",
    "    def setUp(self):\n",
    "        \n",
    "        ##params\n",
    "        Ttrn,Tval,D,V,nmaps = 2003,301,17,4,8\n",
    "        deg_per_stim = 20\n",
    "        sf = 10\n",
    "\n",
    "        ##feature maps\n",
    "        trn_feature_map_dict = {}\n",
    "        for ii in range(1,nmaps+1):\n",
    "            input_name = 'fmap_%0.2d' % (ii)\n",
    "            trn_feature_map_dict[input_name] = np.random.random((Ttrn,D,sf*ii,sf*ii)).astype('float32')\n",
    "\n",
    "        val_feature_map_dict = {}\n",
    "        for ii in range(1,nmaps+1):\n",
    "            input_name = 'fmap_%0.2d' % (ii)\n",
    "            val_feature_map_dict[input_name] = np.random.random((Tval,D,sf*ii,sf*ii)).astype('float32')\n",
    "\n",
    "\n",
    "        ##true rfs\n",
    "        bound = int((deg_per_stim-3)/2.)\n",
    "        true_rf_params = {k:np.random.randint(-bound,high=bound, size=V).astype('float32') for k in ['x0','y0']}\n",
    "        true_rf_params['sig'] = np.random.randint(1,high=bound,size=V).astype('float32')\n",
    "        ##true feature weights\n",
    "        true_NU = np.random.random((D*nmaps,V)).astype('float32')\n",
    "\n",
    "        ##fwrf model\n",
    "        true_model = fwrf(val_feature_map_dict,deg_per_stim,V,rf_init=true_rf_params, NU=true_NU)\n",
    "        true_model.normalize(lambda: (yield trn_feature_map_dict))\n",
    "\n",
    "        ##true outputs, trn/val\n",
    "        trn_voxel_activity = true_model.predict(trn_feature_map_dict).astype('float32')\n",
    "        val_voxel_activity = true_model.predict(val_feature_map_dict).astype('float32')\n",
    "\n",
    "        ##data generator: note these are functions that *return* generators, so we can reboot the generator whenever.\n",
    "        chunk_size = 100\n",
    "        trn_data_gen = lambda: (({k:v[ii:ii+chunk_size,:,:,:] for k,v in trn_feature_map_dict.iteritems()}, trn_voxel_activity[ii:ii+chunk_size,:]) for ii in range(0,Ttrn,chunk_size))\n",
    "        val_data_gen = lambda: (({k:v[ii:ii+chunk_size,:,:,:] for k,v in val_feature_map_dict.iteritems()}, val_voxel_activity[ii:ii+chunk_size,:]) for ii in range(0,Tval,chunk_size))       \n",
    "\n",
    "        ##rf grid for coarse training\n",
    "        deg_per_radius = (1,deg_per_stim,12)\n",
    "        spacing = 2\n",
    "        rf_grid_df = make_rf_table(deg_per_stim,deg_per_radius,spacing,pix_per_stim = None)\n",
    "        G = rf_grid_df.shape[0]\n",
    "        rf_grid = {}\n",
    "        rf_grid['x0'] = rf_grid_df.x_deg.values.astype('float32')\n",
    "        rf_grid['y0'] = rf_grid_df.y_deg.values.astype('float32')\n",
    "        rf_grid['sig'] = rf_grid_df.deg_per_radius.values.astype('float32')\n",
    "\n",
    "        ##record what is needed\n",
    "        self.true_model = true_model\n",
    "        self.trn_data_gen = trn_data_gen\n",
    "        self.val_data_gen = val_data_gen\n",
    "        self.true_NU = true_NU\n",
    "        self.true_rf_params = true_rf_params\n",
    "        self.rf_grid = rf_grid\n",
    "        self.Tval = Tval\n",
    "        self.V = V\n",
    "        self.Ttrn = Ttrn\n",
    "        self.chunk_size = chunk_size\n",
    "        self.D = D\n",
    "        self.nmaps = nmaps\n",
    "        self.G = G\n",
    "\n",
    "\n",
    "    ##test fwrf output dimensions\n",
    "    def test_fwrf_output_dimensions(self):\n",
    "        inp,outp = self.val_data_gen().next()\n",
    "        self.assertEqual(outp.shape,(self.chunk_size, self.V))\n",
    "        inp,outp = self.trn_data_gen().next()\n",
    "        self.assertEqual(outp.shape,(self.chunk_size, self.V))\n",
    "        self.assertEqual(self.D*self.nmaps, self.true_model.D)\n",
    "\n",
    "    ##loss is 0 for fwrf with true params\n",
    "    def test_fwrf_true_output(self):\n",
    "        inp,outp = self.val_data_gen().next()\n",
    "        learner = batch_model_learner(self.true_model.fwrf,self.true_model.input_var_dict,self.trn_data_gen,self.val_data_gen,self.V)\n",
    "        val_loss = learner.loss(outp, *inp.values())\n",
    "        np.testing.assert_array_almost_equal(val_loss, np.zeros(val_loss.shape), decimal = 7)\n",
    "    \n",
    "    ##test create proxy and test proxy shape\n",
    "    def test_create_proxy_net(self):\n",
    "        pxnet,_ = self.true_model._build_proxy_network((None,self.D*self.nmaps,self.G))\n",
    "        out_shape = pxnet.get_output_shape_for((self.Ttrn,self.true_model.D,self.G))\n",
    "        self.assertEqual(out_shape, (self.Ttrn,self.G,self.V))\n",
    "        \n",
    "    ##test construct model space tensor\n",
    "    def test_construct_model_space_tensor( self ):\n",
    "        _,mst_gen = self.true_model._build_rf_model_space_tensor(self.trn_data_gen,self.val_data_gen,self.rf_grid, consolidate=False)\n",
    "        self.assertEqual(mst_gen().next()[0]['mst'].shape, (self.chunk_size,self.true_model.D,self.G))\n",
    "        self.assertEqual(mst_gen().next()[1].shape, (self.chunk_size, self.V))\n",
    "        \n",
    "        _,mst_gen = self.true_model._build_rf_model_space_tensor(self.trn_data_gen,self.val_data_gen,self.rf_grid, consolidate=True)\n",
    "        self.assertEqual(mst_gen().next()[0]['mst'].shape, (self.Tval,self.true_model.D,self.G))\n",
    "        self.assertEqual(mst_gen().next()[1].shape, (self.Tval, self.V))\n",
    "        \n",
    "    \n",
    "    ##test coarse learning using rf grid\n",
    "    def test_fwrf_train_me( self ):\n",
    "        ##coarse\n",
    "        _=true_model.train_me(trn_data_gen, val_data_gen,coarse=True,rf_grid=rf_grid, learning_rate=10e-8,epochs=10, num_iters = 1,check_every=1,print_stuff=True,check_dims=False, normalize=True,consolidate=True)\n",
    "        ##fine\n",
    "        _=true_model.train_me(trn_data_gen, val_data_gen, fine=True, learning_rate=10e-8,epochs=7, num_iters = 1,check_every=1,print_stuff=True,check_dims=False)\n",
    "   \n",
    "        np.testing.assert_array_almost_equal(true_model.rf_layer.sig.get_value().astype('float32'),true_rf_params['sig'],decimal=2)\n",
    "        np.testing.assert_array_almost_equal(true_model.rf_layer.x0.get_value().astype('float32'),true_rf_params['x0'],decimal=2)\n",
    "        np.testing.assert_array_almost_equal(true_model.rf_layer.y0.get_value().astype('float32'),true_rf_params['y0'],decimal=2)\n",
    "    \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase( TestFwrf )\n",
    "unittest.TextTestRunner(verbosity=1).run( suite )                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##MAKE THIS A CLASS, WITH ABILITY TO LOOP OVER MANY TRN/VAL MODEL PAIRS.\n",
    "##\n",
    "##TODO: INPUTS AS SHARED VARIABLES?\n",
    "def decode_feature_maps(trn_fwrf_model, val_fwrf_model, trn_activity, val_activity, feature_map_dict, error_diff_thresh):\n",
    "    ##build loss\n",
    "    trn_activity_tensor = tnsr.matrix('trn_activity')\n",
    "    val_activity_tensor = tnsr.matrix('val_activity')\n",
    "\n",
    "    trn_diff = trn_activity_tensor-trn_fwrf_model.pred_expr  ##difference tensor: (T x V)\n",
    "    trn_loss_expr = (trn_diff*trn_diff).sum(axis=1) ##sum-sqaured-diffs tensor:: the sum is over VOXELS.\n",
    "    \n",
    "    ##Outputs are scalar\n",
    "    trn_loss_func = function([[trn_activity_tensor]+trn_fwrf_model.input_var_dict.values()], trn_loss_expr.sum())\n",
    "\n",
    "    val_diff = val_activity_tensor-val_fwrf_model.pred_expr  ##difference tensor: (T x V)\n",
    "    val_loss_expr = (val_diff*val_diff).sum(axis=1) ##sum-sqaured-diffs tensor: SUM OVER VOXELS\n",
    "    \n",
    "    ##Outputs should just be T.\n",
    "    val_loss_func = BLARF\n",
    "\n",
    "    ##build gradient w.r.t. input vars\n",
    "    grad_expr = tnsr.gradient(trn_loss_expr, wrt=trn_fwrf_model.input_var_dict.values())\n",
    "    \n",
    "    ##a list of feature map gradients.\n",
    "    ##each gradient in the list should be like T,D,S,S tensor4\n",
    "    grad_func = function([trn_activity_tensor]+trn_fwrf_model.input_var_dict.values(), grad_expr)\n",
    "    \n",
    "    val_loss_is = np.inf\n",
    "    val_loss_was = np.inf\n",
    "    \n",
    "    err_diff = np.abs(val_loss_is - val_loss_was)\n",
    "    \n",
    "    new_feature_map_values = SOME KIND OF COPY OF THE THE INIT. FEATURE MAPS. A LIST OF T,D,S,S tensors\n",
    "    \n",
    "    for step in range(number_of_steps):\n",
    "        \n",
    "        map_grads = grad_func(BLARF, new_feature_map_values) ##list of T,D,S,S tensors\n",
    "        ##update each feature map using \n",
    "        for fm in len(map_grads):\n",
    "            new_feature_map_values[fm] -= learning_rate*map_grads[fm]\n",
    "        \n",
    "        ##test against val set\n",
    "        val_loss = val_loss_fun(new_feature_map_values)\n",
    "        \n",
    "        ##save if improved. We'll stay with the batch model where we iterate for fixed number for all batches\n",
    "        \n",
    "    ##On exit, give the feature map, the val_loss, history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.inf - np.inf) < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####===================ORPHANAGE========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = 256 ##trials\n",
    "D = [61, 62, 63] ##features\n",
    "S = [11, 12, 13]\n",
    "nmaps = len(D)\n",
    "map_names = string.uppercase[:nmaps]\n",
    "V = 100 ##voxels\n",
    "deg_per_stim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MakeSpaceTest(unittest.TestCase):\n",
    "    def test_make_space(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class fwrfTest(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        \n",
    "        #testing of individual layers done with first feature map\n",
    "        #testing of rf_model_space and fwrf done with full feature map dict.\n",
    "        \n",
    "        #--feature maps\n",
    "        self.feature_map_dict = {key: val for key,val in zip(map_names, [np.random.rand(T,D[ii],S[ii],S[ii]) for ii in range(nmaps)])}\n",
    "        self.means = [np.mean(self.feature_map_dict[key],axis=0) for key in self.feature_map_dict.keys()]\n",
    "        self.stds = [np.std(self.feature_map_dict[key],axis=0) for key in self.feature_map_dict.keys()]\n",
    "        self.masks = [np.random.randint(0,2,size=(T,D[ii],S[ii],S[ii])) for ii in range(nmaps)]\n",
    "        \n",
    "        #--input\n",
    "        self.input_shape = (T,D,S[0],S[0])\n",
    "        #this tensor stores feature map\n",
    "        self.f_map_0 = tnsr.tensor4('f_map_0',dtype='float32')\n",
    "\n",
    "        #--layers\n",
    "        self.layers = {}\n",
    "        self.output_shapes = {}\n",
    "    \n",
    "    def test_input_layer_init(self):    \n",
    "        #the lasagne input layer\n",
    "        self.layers['layer_1'] = lasagne.layers.InputLayer(self.input_shape, input_var = self.f_map_0, name='layer_1')\n",
    "        #should not change output shape\n",
    "        self.output_shapes['layer_1'] = self.layers['layer_1'].get_output_shape_for(self.input_shape)\n",
    "        self.assertEqual(self.output_shapes['layer_1'], self.input_shape)\n",
    "    \n",
    "\n",
    "    def test_act_layer_init(self):\n",
    "        #--activation layber\n",
    "        self.layers['act_layer'] = compressive_nonlinearity_layer(self.layers['rf_layer'],name='act_layer')\n",
    "        self.output_shapes['act_layer'] = self.layers['act_layer'].get_output_shape_for(self.output_shapes['rf_layer'])\n",
    "        self.assertEqual(self.output_shapes['act_layer'], self.output_shapes['rf_layer']) ##should not change shape\n",
    "    \n",
    "    def test_norm_layer_init(self):\n",
    "        #--norm layer\n",
    "        self.layers['norm_layer'] = normalization_layer(self.layers['act_layer'], mean=self.means[0], stdev=self.stds[0], mask = self.masks[0], name='norm_layer')\n",
    "        self.output_shapes['norm_layer'] = self.layers['norm_layer'].get_output_shape_for(self.output_shapes['act_layer'])\n",
    "        self.assertEqual(self.output_shapes['norm_layer'], self.output_shapes['act_layer']) ##should not change shape\n",
    "    \n",
    "    def feature_layer_init(self):\n",
    "        #--feature layer\n",
    "        self.layers['feature_layer'] = feature_weights_layer(self.layers['norm_layer'])\n",
    "        self.output_shapes['feature_layer'] = self.layers['feature_layer'].get_output_shape_for(self.output_shapes['norm_layer'])\n",
    "        self.assertEqual(self.output_shapes['feature_layer'], (T,V))\n",
    "    \n",
    "    def tearDown(self):\n",
    "        for s in self.output_shapes:\n",
    "            print s\n",
    "    \n",
    "    \n",
    "    def test_rf_layer(self):\n",
    "        pass\n",
    "    \n",
    "    def test_norm_layer(self):\n",
    "        pass\n",
    "    \n",
    "    def test_modelspace(self):\n",
    "        pass\n",
    "    \n",
    "    def test_feature_weights_layer(self):\n",
    "        pass\n",
    "    \n",
    "    def test_fwrf_init(self):\n",
    "        pass\n",
    "    \n",
    "    def test_learn_lasagne_model(self):\n",
    "        pass\n",
    "    \n",
    "    def test_fwrf_train_me_coarse(self):\n",
    "        pass\n",
    "    \n",
    "    def test_fwrf_train_me_fine(self):\n",
    "        pass\n",
    "    \n",
    "    def test_fwrf_decode(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##rf layer unittest\n",
    "def setUp(self):\n",
    "        \n",
    "        #testing of individual layers done with first feature map\n",
    "        #testing of rf_model_space and fwrf done with full feature map dict.\n",
    "        \n",
    "        #--feature maps\n",
    "        self.feature_map_dict = {key: val for key,val in zip(map_names, [np.random.rand(T,D[ii],S[ii],S[ii]) for ii in range(nmaps)])}\n",
    "        self.means = [np.mean(self.feature_map_dict[key],axis=0) for key in self.feature_map_dict.keys()]\n",
    "        self.stds = [np.std(self.feature_map_dict[key],axis=0) for key in self.feature_map_dict.keys()]\n",
    "        self.masks = [np.random.randint(0,2,size=(T,D[ii],S[ii],S[ii])) for ii in range(nmaps)]\n",
    "        \n",
    "        #--input\n",
    "        self.input_shape = (T,D,S[0],S[0])\n",
    "        #this tensor stores feature map\n",
    "        self.f_map_0 = tnsr.tensor4('f_map_0',dtype='float32')\n",
    "\n",
    "        #--layers\n",
    "        self.layers = {}\n",
    "        self.output_shapes = {}\n",
    "    \n",
    "    def test_input_layer_init(self):    \n",
    "        #the lasagne input layer\n",
    "        self.layers['layer_1'] = lasagne.layers.InputLayer(self.input_shape, input_var = self.f_map_0, name='layer_1')\n",
    "        #should not change output shape\n",
    "        self.output_shapes['layer_1'] = self.layers['layer_1'].get_output_shape_for(self.input_shape)\n",
    "        self.assertEqual(self.output_shapes['layer_1'], self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-49159a4988b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msuite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munittest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTestLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadTestsFromTestCase\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfwrfTest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMakeSpaceTest\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0munittest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextTestRunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msuite\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tnaselar/anaconda/lib/python2.7/unittest/loader.pyc\u001b[0m in \u001b[0;36mloadTestsFromTestCase\u001b[1;34m(self, testCaseClass)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloadTestsFromTestCase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestCaseClass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;34m\"\"\"Return a suite of all tests cases contained in testCaseClass\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestCaseClass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTestSuite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             raise TypeError(\"Test cases should not be derived from TestSuite.\" \\\n\u001b[0;32m     52\u001b[0m                                 \" Maybe you meant to derive from TestCase?\")\n",
      "\u001b[1;31mTypeError\u001b[0m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        \n",
    "#     def _build_trn_pred(self):\n",
    "#         self.trn_pred_expr = lasagne.layers.get_output(self.fwrf)\n",
    "#         #self.trn_pred_func = function(self.input_var_dict.values(), self.trn_pred_expr)\n",
    "    \n",
    "#     def _build_val_pred(self):\n",
    "#         ##this will make predictions using the current normalization params (i.e., since the last call of trn_pred_func)\n",
    "#         self.val_pred_expr = lasagne.layers.get_output(self.fwrf, deterministic='True')\n",
    "#         ##FUNCTION\n",
    "#         self.predicted_activity = function(self.input_var_dict.values(), self.val_pred_expr)\n",
    "        \n",
    "#     def _build_trn_loss(self):\n",
    "#         ##training loss expression: same as above except for the last summing step. so:\n",
    "#         trn_diff = self.voxel_data_tnsr.T[:,:,np.newaxis]-self.trn_pred_expr  ##difference tensor: (V x T x 1) - (V x T x 1) = (V x T x 1)\n",
    "#         self.trn_loss_expr = (trn_diff*trn_diff).sum(axis=1).sum() ##sum-sqaured-diffs tensor: V x 1\n",
    "    \n",
    "#     def _build_val_loss(self): \n",
    "#         ##validation loss\n",
    "#         val_diff = self.voxel_data_tnsr.T[:,:,np.newaxis]-self.val_pred_expr  ##difference tensor: (V x T x 1) - (V x T x 1) = (V x T x 1)\n",
    "#         self.val_loss_expr = (val_diff*val_diff).sum(axis=1) ##sum-sqaured-diffs tensor: V x 1\n",
    "#         ##FUNCTION\n",
    "#         self.loss = function([self.voxel_data_tnsr]+self.input_var_dict.values(), self.val_loss_expr)\n",
    "\n",
    "        \n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to build an input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####input shape\n",
    "input_shape = (T,D,S,S)\n",
    "####this tensor stores feature map\n",
    "f_map_0 = tnsr.tensor4('f_map_0',dtype='float32')  ##(T,D,S,S)\n",
    "\n",
    "####the lasagne input layer\n",
    "layer_1 = lasagne.layers.InputLayer(input_shape, input_var = f_map_0, name='layer_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the rf layer\n",
    "Output will have model-space tensor dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = 48\n",
    "deg_per_stim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "rf_layer = receptive_field_layer(layer_1, G, make_space(S), deg_per_stim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### see initial stack of rfs -- they are all the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f77cf4f4510>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAEACAYAAABLUDivAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77ce389250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extent=[-deg_per_stim/2.,deg_per_stim/2.,deg_per_stim/2.,-deg_per_stim/2.]\n",
    "plt.imshow(rf_layer.make_rf_stack()[-1,:,:],cmap=plt.cm.Reds, interpolation='none', extent=extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### but now change them, and re-view them\n",
    "NOTE: everytyhing is specified in degrees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f77cf6cb550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAEACAYAAABLUDivAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77d05f6c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_layer.x0.set_value(-5*np.ones(G).astype('float32'))\n",
    "rf_layer.y0.set_value(5*np.ones(G).astype('float32'))\n",
    "rf_layer.sig.set_value(3*np.ones(G).astype('float32'))\n",
    "plt.imshow(rf_layer.make_rf_stack()[3,:,:],cmap=plt.cm.Reds, interpolation='none', extent=extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### compile output function, test shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output(rf_layer)\n",
    "pred_func = function([layer_1.input_var], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "(48, 256, 64)\n"
     ]
    }
   ],
   "source": [
    "outp = pred_func(np.zeros(input_shape).astype('float32'))\n",
    "print outp.dtype\n",
    "print outp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check out the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x0, y0, sig]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasagne.layers.get_all_params(rf_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the activation layer\n",
    "For the gabor model, we sometimes use the log(1+sqrt(x)) transforms. Here's that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_layer = compressive_nonlinearity_layer(rf_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x0, y0, sig]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasagne.layers.get_all_params(act_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_3 = lasagne.layers.get_output(act_layer)\n",
    "l_3_func = function([layer_1.input_var], l_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 256, 64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_3_func(np.zeros(input_shape).astype('float32')).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization layer\n",
    "    pass deterministic=False for training\n",
    "    pass deterministic=True for validation\n",
    "    in our case output shape = (G,T,D), so normalization must integrate over axis=1\n",
    "    this will give mean/stdev/gamma/beta params of shape (G,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_layer = lasagne.layers.BatchNormLayer(rf_layer, axes=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 64)\n",
      "(48, 64)\n",
      "(48, 64)\n",
      "(48, 64)\n"
     ]
    }
   ],
   "source": [
    "print norm_layer.mean.get_value().shape\n",
    "print norm_layer.inv_std.get_value().shape\n",
    "print norm_layer.gamma.get_value().shape\n",
    "print norm_layer.beta.get_value().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### so everytime you call the layer_4 output function the mean/stdev params are updated.\n",
    "but if you call with deterministic='True', the mean/stdev are not updated, they are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_pred_expr = lasagne.layers.get_output(norm_layer)\n",
    "norm_pred_func = function([layer_1.input_var], norm_pred_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.random(size=input_shape).astype('float32')\n",
    "_=norm_pred_func(x)\n",
    "before= norm_layer.mean.get_value()\n",
    "for _ in range(10):\n",
    "    x = np.random.random(size=input_shape).astype('float32')\n",
    "    um=norm_pred_func(x)\n",
    "after = norm_layer.mean.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f778f3a3c90>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCS1+v+nMkTzVrhJxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f778f13f690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(before,after)\n",
    "# lims = [np.min(after), np.max(after)]\n",
    "# plt.xlim(lims)\n",
    "# plt.ylim(lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...call layer_4 output function with deterministic='Trure' in a loop, and means are updated stay fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_pred_expr_fixed = lasagne.layers.get_output(norm_layer, deterministic = 'True')\n",
    "norm_pred_func_fixed = function([layer_1.input_var], norm_pred_expr_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f77cc20d910>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE3LwjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77cc32d290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "before= norm_layer.mean.get_value()\n",
    "for _ in range(10):\n",
    "    x = np.random.random(size=input_shape).astype('float32')\n",
    "    _=norm_pred_func_fixed(x)\n",
    "after = norm_layer.mean.get_value()\n",
    "plt.scatter(before,after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature weights layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_5 = feature_weights_layer(norm_layer,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_5_pred_expr = lasagne.layers.get_output(layer_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_5_pred_func = function([layer_1.input_var],layer_5_pred_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 256, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "layer_5_pred_func(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "Usually we'll have multiple features spaces, where each feature space is a stack of feature maps with the same \n",
    "resolution.  \n",
    "Because resolution can vary, we'll need construct separate rf layers for each feature space.  \n",
    "Then, we can merge the feature spaces, and apply whatever normalization, nonlinearity, etc. we want before apply the feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### construct feature space dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_map_names = ['a','b','c']\n",
    "feature_map_dict = {}\n",
    "feature_depths = [12, 45, 78]\n",
    "for fmap,fdep in zip(feature_map_names,feature_depths):\n",
    "    feature_map_dict[fmap] = np.random.random(size=(T,fdep,S,S)).astype('float32')\n",
    "\n",
    "num_rfs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##construct model space\n",
    "input_var_dict, model_space = make_fwrf_model_space(feature_map_dict, num_rfs, deg_per_stim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the model space part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 64, 125, 125)\n"
     ]
    }
   ],
   "source": [
    "print model_space.get_output_shape_for([input_shape])\n",
    "\n",
    "model_space_pred_expr = lasagne.layers.get_output(model_space)\n",
    "model_space_pred_func = function(input_var_dict.values(), model_space_pred_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[rf_a.x0, rf_a.y0, rf_a.sig]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasagne.layers.get_all_params(model_space,rf_param=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "important: note how we need to unpack the argument list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 135)\n",
      "135\n"
     ]
    }
   ],
   "source": [
    "print model_space_pred_func(*feature_map_dict.values()).shape\n",
    "print sum(feature_depths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two ways to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1**: Grid search over many rfs for each voxel. In this case we run the model space+act+norm network to construct a model-space tensor for the training and testing data. Then we construct a simple linear network where we just apply the feature weights to the mst. We do this to avoid having to re-apply the rf models to the feature maps each time we want to make a prediction. The gradient of loss is now w.r.t. to the feature weights only, but the training procedure needs to include a mechanism for selecting the best rf for each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##define the model space network\n",
    "model_space = compressive_nonlinearity_layer(model_space)\n",
    "model_space = lasagne.layers.BatchNormLayer(model_space, axes=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##define the prediction expression / function\n",
    "model_space_pred_expr = lasagne.layers.get_output(model_space, deterministic='False')\n",
    "model_space_pred_func = function(input_var_dict.values(), model_space_pred_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 135)\n"
     ]
    }
   ],
   "source": [
    "##construct the mst\n",
    "mst = model_space_pred_func(*feature_map_dict.values())\n",
    "print mst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##now construct full model: the input is the model space tensor\n",
    "mst_tnsr = tnsr.tensor3('mst')\n",
    "fwrf = lasagne.layers.InputLayer((mst.shape[0],None,mst.shape[-1]), input_var=mst_tnsr,name='fwrf')\n",
    "fwrf = feature_weights_layer(fwrf,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##construct prediction expr: (G,T,V)\n",
    "fwrf_pred_expr = lasagne.layers.get_output(fwrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##loss expression: we already have all this stuff from before\n",
    "voxel_data_tnsr = tnsr.matrix('voxel_data_tnsr')  ##voxel data tensor: T x V\n",
    "diff = voxel_data_tnsr-fwrf_pred_expr  ##difference tensor: (T x V) - (G x T x V) = (G x T x V)\n",
    "sq_diff = (diff*diff).sum(axis=1) ##sum-sqaured-diffs tensor: G x V\n",
    "\n",
    "\n",
    "fwrf_loss_expr = sq_diff.sum()  ##<<this sum is critical. theano knows not to differentiate w.r.t params if deriv. is always 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##update rule\n",
    "learning_rate = 0.1\n",
    "params = lasagne.layers.get_all_params(fwrf)\n",
    "fwrf_update = lasagne.updates.sgd(fwrf_loss_expr, params, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##training kernel: returns loss, updates params\n",
    "trn_kernel = function([mst_tnsr,voxel_data_tnsr], fwrf_loss_expr, updates=fwrf_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##validation kernel: returns predictions and loss, leaves params alone\n",
    "val_kernel = function([mst_tnsr, voxel_data_tnsr], [fwrf_pred_expr, fwrf_loss_expr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.03935089508e+14\n"
     ]
    }
   ],
   "source": [
    "neural_data = np.random.random(size=(T,V)).astype('float32')\n",
    "print trn_kernel(mst, neural_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 100)\n",
      "3.93386847789e+24\n"
     ]
    }
   ],
   "source": [
    "pred,loss = val_kernel(mst, neural_data)\n",
    "print pred.shape\n",
    "print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##training proc: pseudo-code\n",
    "# val_in, val_out = get_val_data(mst)\n",
    "# for trn_in, trn_out in batch_generator(mst):\n",
    "#     trn_loss = trn_kernel(trn_in, trn_out)\n",
    "#     _,val_loss = val_kernel(val_in,val_out) ##don't really care about val_pred here\n",
    "#     if val_loss is less_than_it_was:\n",
    "#         rf_idx = select_best_rf(fwrf)\n",
    "#         NU = select_best_weights(fwrf)\n",
    "#     if val_loss is too_big:\n",
    "#         break\n",
    "# new_fwrf = make_best_model(rf_idx, NU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2**: Gradient descent on rf parameters. In this case we optimize one rf per voxel using gradient descent. We build the full network and take gradients w.r.t. both the mean/size of each voxel's rf and the feature weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Construct the full model network\n",
    "##add activation\n",
    "fwrf = compressive_nonlinearity_layer(model_space)\n",
    "\n",
    "##add normalization\n",
    "fwrf = lasagne.layers.BatchNormLayer(fwrf, axes=(1,))\n",
    "\n",
    "##add feature layer\n",
    "num_outputs=1\n",
    "fwrf = feature_weights_layer(fwrf,num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##prediction expression with update of normalization params: ##V x T x 1\n",
    "trn_pred_expr = lasagne.layers.get_output(fwrf)\n",
    "# trn_pred_func = function(input_var_dict.values(), trn_pred_expr)\n",
    "\n",
    "##this will make predictions using the current normalization params (i.e., since the last call of trn_pred_func)\n",
    "val_pred_expr = lasagne.layers.get_output(fwrf, deterministic='True')\n",
    "# val_pred_func = function(input_var_dict.values(), val_pred_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##voxel data needed for loss\n",
    "voxel_data_tnsr = tnsr.matrix('voxel_data_tnsr')  ##voxel data tensor: (T x V)\n",
    "\n",
    "##training loss expression: same as above except for the last summing step. so:\n",
    "trn_diff = voxel_data_tnsr.T[:,:,np.newaxis]-trn_pred_expr  ##difference tensor: (V x T x 1) - (V x T x 1) = (V x T x 1)\n",
    "trn_loss = (trn_diff*trn_diff).sum(axis=1) ##sum-sqaured-diffs tensor: V x 1\n",
    "\n",
    "##validation loss\n",
    "val_diff = voxel_data_tnsr.T[:,:,np.newaxis]-val_pred_expr  ##difference tensor: (V x T x 1) - (V x T x 1) = (V x T x 1)\n",
    "val_loss = (val_diff*val_diff).sum(axis=1) ##sum-sqaured-diffs tensor: V x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[rf_a.x0, rf_a.y0, rf_a.sig, beta, gamma, feature_weights]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasagne.layers.get_all_params(fwrf,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##-----check the gradients\n",
    "fw_grad = tnsr.grad(trn_loss.sum(),fwrf.NU)\n",
    "rf_grad = tnsr.grad(trn_loss.sum(),lasagne.layers.get_all_params(fwrf,rf_param=True))\n",
    "\n",
    "\n",
    "##functionalize\n",
    "fw_grad_func = function([voxel_data_tnsr]+input_var_dict.values(), fw_grad)\n",
    "rf_grad_func = function([voxel_data_tnsr]+input_var_dict.values(), rf_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding_grad = tnsr.grad(trn_loss.sum(),lasagne.layers.get_all_params(fwrf,trainable=True))\n",
    "encoding_grad_func = function([voxel_data_tnsr]+input_var_dict.values(), encoding_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 135, 1)\n"
     ]
    }
   ],
   "source": [
    "print fw_grad_func(neural_data,*feature_map_dict.values()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neural_data = np.random.random(size=(T,num_rfs)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads= encoding_grad_func(neural_data,*feature_map_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 135, 1)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grad_func(neural_data,*feature_map_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##update rule\n",
    "learning_rate = 0.1\n",
    "params = lasagne.layers.get_all_params(fwrf,trainable=True)\n",
    "fwrf_update = lasagne.updates.sgd(trn_loss.sum(),params,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##training kernel\n",
    "trn_kernel = function(input_var_dict.values()+[voxel_data_tnsr], trn_loss, updates=fwrf_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##validation kernel\n",
    "val_kernel = function(input_var_dict.values()+[voxel_data_tnsr], [val_pred_expr,val_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##training proc: pseudo-code\n",
    "# val_in, val_out = get_val_data(mst)\n",
    "# for trn_in, trn_out in batch_generator(mst):\n",
    "#     trn_loss = trn_kernel(trn_in, trn_out)\n",
    "#     _,val_loss = val_kernel(val_in,val_out) ##don't really care about val_pred here\n",
    "#     if val_loss is less_than_it_was:\n",
    "#         NU = lasagne.layers.get_param_values(fwrf)[-1]  ##<<something like this\n",
    "#     if val_loss is too_big:\n",
    "#         break\n",
    "# new_fwrf = make_best_model(NU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "uses \"new_fwrf\" resulting from one of the two training methods.\n",
    "How to treat input layer?\n",
    "Link input to a shared variable? Probably a bad idea.\n",
    "Create a feature map layer that has a shared variable param? When building the \"new_fwrf\", make a decoding version with this feature map layer at the front?\n",
    "Or: just dont' use shared variable here...probably the best idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoding_pred_expr = lasagne.layers.get_output(fwrf, deterministic='True') ##V x T x 1\n",
    "target_activity = tnsr.matrix('target_activity')  #T x V\n",
    "decoding_loss = val_loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##apparently, the input_var can't be share variable, so we'll have to do updates explicitly\n",
    "decoding_grad = tnsr.grad(decoding_loss,wrt=input_var_dict.values())\n",
    "decoding_grad_func = function([voxel_data_tnsr]+input_var_dict.values(), decoding_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decodeable_pattern = np.random.random(size=(T,num_rfs)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_grad = decoding_grad_func(decodeable_pattern, *feature_map_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foo = shared(np.ones((2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fwrf_model_space'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_space.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = tnsr.tensor4('inp')\n",
    "rfs = tnsr.tensor3('rfs')\n",
    "outp = tnsr.tensordot(inp, rfs, axes= [[2,3],[1,2]])\n",
    "florg = function([inp, rfs], outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 64, 100)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((T,D,S,S),dtype='float32')\n",
    "r = np.zeros((V, S, S),dtype='float32')\n",
    "blarg = florg(x,r)\n",
    "print blarg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (256,64,125,125) (1,100,125,125) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1076-645ca8f7f1e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (256,64,125,125) (1,100,125,125) "
     ]
    }
   ],
   "source": [
    "x*r[np.newaxis,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featw = tnsr.matrix('featw')\n",
    "out2 = (outp*featw[np.newaxis,:,:]).sum(axis=1)\n",
    "final_out = function([outp,featw], out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 100)"
      ]
     },
     "execution_count": 1094,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.zeros((D,V), dtype='float32')\n",
    "final_out(blarg, w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
