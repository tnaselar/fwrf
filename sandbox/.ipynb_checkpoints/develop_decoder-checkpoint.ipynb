{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lasagne\n",
    "import matplotlib.pyplot as plt\n",
    "from theano import tensor as tnsr\n",
    "from theano import function\n",
    "from theano import map as tmap\n",
    "from theano import shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to lambda (<ipython-input-6-2fc370f4ae35>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-2fc370f4ae35>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    fnc = lambda x,ii: x[ii] = 1\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to lambda\n"
     ]
    }
   ],
   "source": [
    "fnc = lambda x,ii: x[ii] + 1\n",
    "x = shared(np.zeros((1,10), dtype='float32'))\n",
    "ii = shared(np.zeros((1,10), dtype='int'))\n",
    "f,u = tmap(fnc,[x,ii],non_sequences=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('update target must be a SharedVariable', 'x')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1448e54bd667>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mii\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mupx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtnsr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minc_subtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    320\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    323\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    441\u001b[0m                                          \u001b[0mrebuild_strict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrebuild_strict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                                          \u001b[0mcopy_inputs_over\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m                                          no_default_updates=no_default_updates)\n\u001b[0m\u001b[0;32m    444\u001b[0m     \u001b[1;31m# extracting the arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[0minput_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcloned_extended_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_stuff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_vars\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mrebuild_collect_shared\u001b[1;34m(outputs, inputs, replace, updates, rebuild_strict, copy_inputs_over, no_default_updates)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstore_into\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSharedVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             raise TypeError('update target must be a SharedVariable',\n\u001b[1;32m--> 184\u001b[1;33m                             store_into)\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstore_into\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mupdate_d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             raise ValueError('this shared variable already has an update '\n",
      "\u001b[1;31mTypeError\u001b[0m: ('update target must be a SharedVariable', 'x')"
     ]
    }
   ],
   "source": [
    "x = shared(np.zeros((1,10), dtype='float32'))\n",
    "ii = shared(np.zeros((1,10), dtype='int'))\n",
    "upx = {'x': tnsr.inc_subtensor(x[ii],1)}\n",
    "f = function([], [], updates=upx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.gof.opt): Optimization failure due to: local_gpu_advanced_incsubtensor1\n",
      "ERROR:theano.gof.opt:Optimization failure due to: local_gpu_advanced_incsubtensor1\n",
      "ERROR (theano.gof.opt): node: GpuFromHost(AdvancedIncSubtensor1{no_inplace,inc}.0)\n",
      "ERROR:theano.gof.opt:node: GpuFromHost(AdvancedIncSubtensor1{no_inplace,inc}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR:theano.gof.opt:TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/gof/opt.py\", line 1779, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1138, in local_gpu_advanced_incsubtensor1\n",
      "    as_cuda_ndarray_variable(y), *coords)]\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 46, in as_cuda_ndarray_variable\n",
      "    return gpu_from_host(tensor_x)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/gof/op.py\", line 602, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 139, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 95, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype int8 for variable None\n",
      "\n",
      "ERROR:theano.gof.opt:Traceback (most recent call last):\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/gof/opt.py\", line 1779, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1138, in local_gpu_advanced_incsubtensor1\n",
      "    as_cuda_ndarray_variable(y), *coords)]\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 46, in as_cuda_ndarray_variable\n",
      "    return gpu_from_host(tensor_x)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/gof/op.py\", line 602, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 139, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 95, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype int8 for variable None\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_gpu_advanced_incsubtensor1\n",
      "ERROR:theano.gof.opt:Optimization failure due to: local_gpu_advanced_incsubtensor1\n",
      "ERROR (theano.gof.opt): node: GpuFromHost(AdvancedIncSubtensor1{no_inplace,inc}.0)\n",
      "ERROR:theano.gof.opt:node: GpuFromHost(AdvancedIncSubtensor1{no_inplace,inc}.0)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR:theano.gof.opt:TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/gof/opt.py\", line 1779, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1138, in local_gpu_advanced_incsubtensor1\n",
      "    as_cuda_ndarray_variable(y), *coords)]\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 46, in as_cuda_ndarray_variable\n",
      "    return gpu_from_host(tensor_x)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/gof/op.py\", line 602, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 139, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 95, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype int8 for variable None\n",
      "\n",
      "ERROR:theano.gof.opt:Traceback (most recent call last):\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/gof/opt.py\", line 1779, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1138, in local_gpu_advanced_incsubtensor1\n",
      "    as_cuda_ndarray_variable(y), *coords)]\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 46, in as_cuda_ndarray_variable\n",
      "    return gpu_from_host(tensor_x)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/gof/op.py\", line 602, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 139, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 95, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype int8 for variable None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ff = function([], outputs=f, updates=u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ii.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ff = function([v],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ff(np.array([1,2,3],dtype='float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_feature_maps(trn_fwrf_model, val_fwrf_model, trn_activity, val_activity, feature_map_dict, error_diff_thresh):\n",
    "    ##build loss\n",
    "    trn_activity_tensor = tnsr.matrix('trn_activity')\n",
    "    val_activity_tensor = tnsr.matrix('val_activity')\n",
    "\n",
    "    trn_diff = trn_activity_tensor-trn_fwrf_model.pred_expr  ##difference tensor: (T x V)\n",
    "    trn_loss_expr = (trn_diff*trn_diff).sum(axis=1) ##sum-sqaured-diffs tensor:: the sum is over VOXELS.\n",
    "    \n",
    "    ##Outputs are scalar\n",
    "    trn_loss_func = function([[trn_activity_tensor]+trn_fwrf_model.input_var_dict.values()], trn_loss_expr.sum())\n",
    "\n",
    "    val_diff = val_activity_tensor-val_fwrf_model.pred_expr  ##difference tensor: (T x V)\n",
    "    val_loss_expr = (val_diff*val_diff).sum(axis=1) ##sum-sqaured-diffs tensor: SUM OVER VOXELS\n",
    "    \n",
    "    ##Outputs should just be T.\n",
    "    val_loss_func = BLARF\n",
    "\n",
    "    ##build gradient w.r.t. input vars\n",
    "    grad_expr = tnsr.gradient(trn_loss_expr, wrt=trn_fwrf_model.input_var_dict.values())\n",
    "    \n",
    "    ##a list of feature map gradients.\n",
    "    ##each gradient in the list should be like T,D,S,S tensor4\n",
    "    grad_func = function([trn_activity_tensor]+trn_fwrf_model.input_var_dict.values(), grad_expr)\n",
    "    \n",
    "    val_loss_is = np.inf\n",
    "    val_loss_was = np.inf\n",
    "    \n",
    "    err_diff = np.abs(val_loss_is - val_loss_was)\n",
    "    \n",
    "    new_feature_map_values = SOME KIND OF COPY OF THE THE INIT. FEATURE MAPS. A LIST OF T,D,S,S tensors\n",
    "    \n",
    "    for step in range(number_of_steps):\n",
    "        \n",
    "        map_grads = grad_func(BLARF, new_feature_map_values) ##list of T,D,S,S tensors\n",
    "        ##update each feature map using \n",
    "        for fm in len(map_grads):\n",
    "            new_feature_map_values[fm] -= learning_rate*map_grads[fm]\n",
    "        \n",
    "        ##test against val set\n",
    "        val_loss = val_loss_fun(new_feature_map_values)\n",
    "        \n",
    "        ##save if improved. We'll stay with the batch model where we iterate for fixed number for all batches\n",
    "        \n",
    "    ##On exit, give the feature map, the val_loss, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class batch_feature_map_decoder(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 trn_model, val_model,\n",
    "                 trn_model_input_tnsr_dict, trn_model_input_tnsr_dict,\n",
    "                 trn_data_generator, val_data_generator,\n",
    "                 epochs = 1,\n",
    "                 check_every=10,\n",
    "                 num_iters=10,\n",
    "                 learning_rate = 1.0,\n",
    "                 learn_these_maps = None,\n",
    "                 check_dims = True,\n",
    "                 print_stuff=False):\n",
    "\n",
    "        '''\n",
    "    batch_feature_map_decoder(   init_feature_maps,\n",
    "                                 trn_model, val_model,\n",
    "                                 trn_model_input_tnsr_dict, trn_model_input_tnsr_dict,\n",
    "                                 trn_data_generator, val_data_generator,\n",
    "                                 epochs = 1,\n",
    "                                 check_every=10,\n",
    "                                 num_iters=10,\n",
    "                                 learning_rate = 1.0,\n",
    "                                 learn_these_feature_maps = None,\n",
    "                                 check_dims = True,\n",
    "                                 print_stuff=False)\n",
    "\n",
    "        a class for gradient descent on independent feature maps, given encoding models and activity patterns.\n",
    "\n",
    "        inputs:\n",
    "                init_feature_maps   ~ dict map_name/tensor pairs. \n",
    "                                      tensors have shape (T,D,S,S), where T is temporal length of stimulus.\n",
    "                                      for stacks of T static images, optimization is performed independently\n",
    "                                      for each image.\n",
    "                                      for movies of T frames, we assume we have model weights that have a temporal\n",
    "                                      dimension, so optimization will be for entire movie (i.e., frames not indepen't.)\n",
    "                trn_model,val_model ~ lasagne models\n",
    "   trn_/val_model_input_tensor_dict ~ dicts of theano tensors that are inputs to trn/val_model, resp.\n",
    "                 trn_data           ~ tensor of voxel activity, shape=(num_stimuli, T, num_trn_voxels)\n",
    "                 val_data_generator ~ tensor of voxel activity, shape=(num_stimuli, T, num_val_voxels)\n",
    "                        check_every ~ int. how many gradients steps until validation loss is checked. default = 10\n",
    "                          num_iters ~ number of gradient steps per batch before tranistioning to next batch. default = 100\n",
    "                      learning_rate ~ size of gradient step. default = 1\n",
    "           learn_these_feature_maps ~ list of names of feature maps to learn. \n",
    "                                      default = None, meaning learn all feature maps\n",
    "                         check_dims ~ if True, run potentially slow sanity check on dimensions of inputs\n",
    "        outputs:\n",
    "        decoded_feature_maps ~ original l_model with params all trained up. this is a convenience, as params are learned in-place\n",
    "              final_val_loss ~ the final validation loss for each of the models\n",
    "                 trn_history ~ array showing training err history\n",
    "                 val_history ~ history showing number of voxels with decreased validat'n loss at each iteration\n",
    "\n",
    "        '''\n",
    "        \n",
    "        ##record all the inputs\n",
    "        if type(init_feature_maps) not list:\n",
    "            init_feature_maps = [init_feature_maps]\n",
    "        self.init_feature_maps = init_feature_maps\n",
    "        self.num_stimuli = len(init_feature_maps)\n",
    "        self.trn_model = trn_model\n",
    "        self.val_model = val_model\n",
    "        self.learn_these_feature_maps = learn_these_feature_maps\n",
    "        self.trn_model_input_tnsr_dict = trn_model_input_tnsr_dict\n",
    "        self.val_model_input_tnsr_dict = val_model_input_tnsr_dict\n",
    "        self.trn_data_generator = trn_data_generator\n",
    "        self.val_data_generator = val_data_generator\n",
    "        self.epochs = epochs\n",
    "        self.check_every=check_every\n",
    "        self.num_iters = num_iters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.print_stuff = print_stuff\n",
    "        self.check_dims = check_dims\n",
    "\n",
    "        ##inspect one batch of input to sanity check dimensions\n",
    "        if self.check_dims:\n",
    "            self.grab_example_batch()\n",
    "            ##check data dimensions\n",
    "            self.check_consistent()\n",
    "    \n",
    "        ##get learned feature maps: stores as a list of shared variables just like output of lasage.layers.get_all_params\n",
    "        self.get_learned_maps()\n",
    "\n",
    "\n",
    "\n",
    "        ##construct a gradient update and loss functions to be called iteratively by the learn method\n",
    "        self.decoding_kernel = self.construct_decoding_kernel()\n",
    "    \n",
    "    def grab_example_batch( self ):\n",
    "        self.trn_in, self.trn_out = next(self.trn_data_generator()) \n",
    "        self.val_in, self.val_out = next(self.val_data_generator())\n",
    "\n",
    "\n",
    "    def check_consistent( self ):\n",
    "        ##read first batches to get some dimensions. assumes first dimension of input is time, last dimension is voxel \n",
    "        num_trn_images,trn_batch_size = self.trn_out.shape[0],self.trn_out.shape[-1]\n",
    "        num_val_images,val_batch_size = self.val_out.shape[0],self.val_out.shape[-1]   \n",
    "        assert num_trn_images == num_val_images, \"number of trn/val images don't match\"\n",
    "        assert num_trn_voxels == self.num_stimuli,\"number stimuli in trn/val does not match specified number of stimuli\"\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_learned_maps( self ):\n",
    "        '''\n",
    "        return list of theano variables\n",
    "        '''\n",
    "        if self.learn_these_params is not None:            \n",
    "            self.learned_maps = [v for v in self.trn_feature_map_dict.values() if v.name in self.learn_these_feature_maps] ##<<unpack the dict.\n",
    "        else:\n",
    "            self.learned_maps = [v for v in self.trn_feature_map_dict.values()] ##<<unpack the dict.\n",
    "        print 'will solve for: %s' %(self.params)\n",
    "\n",
    "\n",
    "                \n",
    "    ##construct a gradient update and loss functions to be called iteratively by the learn method\n",
    "    def construct_decoding_kernel( self ):\n",
    "        \n",
    "        \n",
    "        trn_activity_tensor = tnsr.matrix('trn_activity')\n",
    "        val_activity_tensor = tnsr.matrix('val_activity')\n",
    "        \n",
    "        trn_pred_expr = lasagne.layers.get_output(self.trn_model)\n",
    "\n",
    "        trn_diff = trn_activity_tensor-trn_pred_expr  ##difference tensor: (T x V)\n",
    "        trn_loss_expr = (trn_diff*trn_diff).sum(axis=1) ##sum-sqaured-diffs tensor:: the sum is over VOXELS.\n",
    "\n",
    "        ##Outputs are scalar, note the extra sum() on the trn_loss_expr\n",
    "        trn_loss_func = function([[trn_activity_tensor]+trn_input_var_dict.values()], trn_loss_expr.sum())\n",
    "\n",
    "        val_diff = val_activity_tensor-val_fwrf_model.pred_expr  ##difference tensor: (T x V)\n",
    "        val_loss_expr = (val_diff*val_diff).sum(axis=1) ##sum-sqaured-diffs tensor: SUM OVER VOXELS\n",
    "\n",
    "        ##T distinct loss values.\n",
    "        val_loss_func = function([[val_activity_tensor]+val_input_var_dict.values()], val_loss_expr())\n",
    "\n",
    "        ##build gradient w.r.t. input vars\n",
    "        grad_expr = tnsr.gradient(trn_loss_expr, wrt=self.learned_maps)\n",
    "\n",
    "        ##a list of feature map gradients.\n",
    "        ##each gradient in the list should be like T,D,S,S tensor4\n",
    "        print 'compiling...'\n",
    "        grad_func = function([trn_activity_tensor]+self.trn_input_var_dict.values(), grad_expr)\n",
    "        \n",
    "        ##closure\n",
    "        def decode_kernel(activity_tensor, feature_map_dict):\n",
    "            args = [activity_tensor]+feature_map_dict.values()\n",
    "            map_grads = grad_func(*args)\n",
    "            ##update each feature map using names of theano variables ininput_var_dict\n",
    "            ##note: we can enumerate because dict.values() and dict.keys() has same order...I think...\n",
    "            ##note: probably a way we can vectorize this.\n",
    "            for ii,k in enumerate(self.trn_input_var_dict.keys()):\n",
    "                new_feature_map_values[k.name] -= learning_rate*map_grads[ii]\n",
    "        \n",
    "        return decode_kernel\n",
    "        \n",
    "        \n",
    " \n",
    "                \n",
    "    \n",
    "    ##if the last gradient step made things better for some voxels, update their parameters\n",
    "    def update_best_param_values(self, best_fmap_values, improved_maps):\n",
    "        for ii,p in enumerate(self.learned_maps):\n",
    "            vdim = self.voxel_dims[p.name] ##the voxel dimension\n",
    "            s = [slice(None),]*p.ndim      ##create a slicing object with right number of dims\n",
    "            s[vdim] = improved_voxels      ##assign improved voxel indices to the correct dim of the slice object\n",
    "            best_param_values[ii][s] = np.copy(p.get_value()[s])   ##keep a record of the best params.\n",
    "        return best_param_values        \n",
    "    \n",
    "    ##iteratively perform gradient descent\n",
    "    def learn(self):\n",
    "        \n",
    "        ##initialize best parameters to whatever they are\n",
    "        best_param_values = [np.copy(p.get_value()) for p in self.params]\n",
    "        \n",
    "        ##initalize validation loss to whatever you get from initial weigths\n",
    "        val_loss_was = 0\n",
    "        for val_in, val_out in self.val_data_generator():\n",
    "            val_loss_was += self.loss(val_out, *val_in.values())\n",
    "\n",
    "        ##initialize train loss to whatever, we only report the difference    \n",
    "        trn_loss_was = 0.0 ##we keep track of total across voxels as sanity check, since it *must* decrease\n",
    "  \n",
    "        val_history = []\n",
    "        trn_history = []\n",
    "        \n",
    "        ##descend and validate\n",
    "        epoch_count = 0\n",
    "        while epoch_count < self.epochs:\n",
    "            print '=======epoch: %d' %(epoch_count) \n",
    "            for trn_in, trn_out in self.trn_data_generator():\n",
    "                step_count = 0\n",
    "                while step_count < self.num_iters:\n",
    "                    \n",
    "                    ##update params, output training loss\n",
    "                    trn_loss_is = self.trn_kernel(trn_out, *trn_in.values())                    \n",
    "                    if step_count % self.check_every == 0:\n",
    "\n",
    "                        ##check for improvements\n",
    "                        val_loss_is = 0\n",
    "                        for val_in, val_out in self.val_data_generator():\n",
    "                            val_loss_is += self.loss(val_out, *val_in.values())\n",
    "                        improved = (val_loss_is < val_loss_was)\n",
    "\n",
    "                        ##update val loss\n",
    "                        val_loss_was[improved] = val_loss_is[improved]\n",
    "\n",
    "                        ##replace old params with better params\n",
    "                        best_param_values = self.update_best_param_values(best_param_values, improved)\n",
    "\n",
    "                        ##report on loss history\n",
    "                        val_history.append(improved.sum())\n",
    "                        trn_history.append(trn_loss_is)\n",
    "                        if self.print_stuff:\n",
    "                            print '====iter: %d' %(step_count)\n",
    "                            print 'number of improved models: %d' %(val_history[-1])\n",
    "                            print 'trn error: %0.6f' %(trn_history[-1])\n",
    "                    \n",
    "                    step_count += 1\n",
    "\n",
    "            epoch_count += 1\n",
    "\n",
    "        ##restore best values of learned params\n",
    "        set_named_model_params(self.l_model, **{k.name:v for k,v in zip(self.params, best_param_values)})\n",
    "       \n",
    "\n",
    "        return self.l_model, val_loss_was, val_history, trn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
